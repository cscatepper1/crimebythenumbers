[["index.html", "Crime by the Numbers: A Criminologists Guide to R Preface Why learn to program? What you will learn What you wont learn Simple vs easy How to read this book Citing this book How to contribute to this book Where to find data included in this book Where to find code included in this book", " Crime by the Numbers: A Criminologists Guide to R Jacob Kaplan 2022-08-24 Preface This book introduces the programming language R and is meant for undergrads or graduate students studying criminology. R is a programming language that is well-suited to the type of work frequently done in criminology - taking messy data and turning it into useful information. While R is a useful tool for many fields of study, this book focuses on the skills criminologists should know and uses crime data for the example data sets. For this book you should have the latest version of R installed and be running it through RStudio Desktop (the free version). Well get into detail on what R and RStudio are soon, but please have them both installed to be able to follow along with each chapter. While you must install both, you only ever need to open RStudio. While R is the actual programming language, RStudio is a program that makes it a lot easier to interact with R than opening up the R application itself.1 I highly recommend following along with the code for each lesson and then trying to use the lessons learned on a data set that you are interested in. Why learn to program? With the exception of some more advanced techniques like scraping data from websites or from PDFs, nearly everything we do here can be done through Excel, a software youre probably more familiar with. The basic steps for research projects are generally: Open up a data set - which frequently comes as an Excel file! Change some values - misspellings or too-specific categories for our purposes are very common in crime data Delete some values - such as states you wont be studying Make some graphs Calculate some values - such as number of crimes per year Sometimes do a statistical analysis depending on the type of project Write up what you find R can do all of this but why should you want (or have) to learn an entirely new skill just to do something you can already do? R is useful for two main reasons: scale and reproducibility. Scale If you do a one-off project in your career such as downloading some data and making a graph out of it, it makes sense to stick with software like Excel. The cost (in time and effort) of learning R is certainly not worth it for a single (or even several) project - even one perfectly suited for using R. R (and many programming languages more generally, such as Python) has its strength in doing something fairly simple many times. For example, it may be quicker to download one file yourself than it is to write the code in R to download that file. But when it comes to downloading hundreds of files, writing the R code becomes very quickly the better option than doing it by hand. For most tasks you do in research when dealing with data, you will end up doing them many times (including doing the same task in future projects). So R offers the trade-off of spending time upfront by learning the code with the benefit of that code being able to do work at a large scale with little extra work from you. Please keep in mind this trade-off - you need to front-load the costs of learning R for the rewards of making your life easier when dealing with data - when feeling discouraged about the small returns you get early in learning R. Reproducibility The second major benefit of using R over something like Excel is that R is reproducible. Every action you take is written down. This is useful when collaborating with others (including your future self) as they can look at your code and follow along what you did without you having to show them every click you made as you frequently would on Excel. Your collaborator can look at your code to help you figure out a bug in the code or add their own code to yours. In the research context specifically, you want to have code to give to people to ensure that your research was done correctly and there arent bugs in the code. Additionally, if you build a tool to, for example, interpret raw crime data from an agency and turn it into a map, being able to share the code so others can modify it for their own city saves these people a lot of time and effort. While not required (yet) in criminology, some academic journals (such as in economics) even require that you submit your data and code if your paper is accepted. If criminology follows in this trend, or if you submit to journals that require code submissions, youll need to be able to write code and not rely on software that doesnt track your steps (such as Excel and SPSS). What you will learn For many of the lessons we will be working through real research questions and working from start to finish as you would on your own project. This involves thinking about what you want to accomplish from the data you have and what steps you need to take to reach that goal. This involves more than just knowing what code to write - it includes figuring out what your data has, whether it can answer the question youre asking, and planning out (without writing any code yet) what you need to do when you start coding. For most lessons well be using actual crime data that is commonly used in research so youll become acquainted with a number of important data sets. Skills There is a large range of skills in criminology research - far too large to cover in a single book. Here we will attempt to teach fundamental skills to build a solid foundation for future work. Well be focusing on the following skills and trying to reinforce our skills with each lesson. Subsetting - Taking only certain rows or columns from a data set Graphing Regular expressions - Essentially Rs Find and Replace function for text Getting data from websites (webscraping) Getting data from PDFs (PDF scraping) Mapping Writing documents through R What you wont learn This book is not a statistics book so we will not be covering any statistical techniques. Though some data sets we handle are fairly large, this book does not discuss how to deal with Big Data. While the lessons you learn in this book can apply to larger data sets, Big Data (which I tend to define loosely as data that are too large for my computer to handle) requires special skills that are outside the realm of this book. If you do intend to deal with huge data sets I recommend you look at the R package data.table, which is an excellent resource for it. While we briefly cover mapping, this book will not cover working with geographic data in detail. For a comprehensive look at geographic data please see this book. This book also will not cover any qualitative data or analysis. While qualitative research is an important part of criminology, this book only focuses on working with quantitative data. Some parts of this book may apply to dealing with qualitative data, such as PDF scraping and regular expressions, but the examples I use in those chapters still deal with quantitative data. Simple vs easy In the course of this book we will cover things that are very simple. For example, well take a data set (think of it like an Excel file) with crime for nearly every police agencyg in the United States and keep only data from Colorado for a small number of years. Well then find out how many murders happened in Colorado each year. This is a fairly simple task - it can be expressed in two sentences. Youll find that most of what you do is simple like this - it is quick to talk about what you are doing and the concepts are not complicated. What it isnt is easy. To actually write the R code to do this takes knowing a number of interrelated concepts in R and several lines of code to implement each step. While this distinction may seem minor, I think it is important for newer programmers to understand that what they are doing may be simple to talk about but hard to implement. When you learn something new in R, or are first introduced to the language, you may feel like youre bashing your head through a brick wall. That is normal. It is easy to feel like a bad programmer because something that can be articulated in 10 seconds may take hours to do. So during times when you are working with R try to keep in mind that even though a project may be simple to articulate, it may be hard to code and that there is often very little correlation between the two. How to read this book This book is written so a person who has no programming experience can start with this chapter and by the end of the book be able to do a data project from start to finish. Each chapter introduces a new skill and builds on the skills introduced in previous chapters. So if you skip ahead you may miss important skills taught in the chapters you didnt read. For someone who has no - or minimal - programming experience, I recommend reading each chapter in order. If you have more programming experience and just want to learn how to do a specific thing, feel free to skip directly to that chapter. Citing this book If this book was useful in your research, please cite it. To cite this book, please use the below citation: Kaplan J (2021). Crime by the Numbers: A Criminologists Guide to R. https://crimebythenumbers.com/. BibTeX format: @Manual{crimebythenumbers, title = {Crime by the Numbers: A Criminologist&#39;s Guide to R}, author = {Jacob Kaplan}, year = {2021}, url = {https://crimebythenumbers.com/}, } How to contribute to this book If you have any questions, suggestions (such as a topic to cover), or find any issues, please make a post on the Issues page for this book on GitHub. On this page you can create a new issue (which is basically just a post on this forum) with a title and a longer description of your issue. Youll need a GitHub account to make a post. Posting there lets me track issues and respond to your message or alert you when the issue is closed (i.e. Ive finished or denied the request). Issues are also public so you can see if someone has already posted something similar. For more minor issues like typos or grammar mistakes, you can edit the book directly through its GitHub page. Thatll make an update for me to accept, which will change the book to include your edit. To do that, click the edit button at the top of the site - the button is highlighted in the below figure. You will need to make a GitHub account to make edits. When you click on that button youll be taken to a page that looks like a Word doc where you can make edits. Make any edits you want and then scroll to the bottom of the page. There you can write a short (please, no more than a sentence or two) description of what youve done and then submit the changes for me to review. Please only use the above two methods to contribute or make suggestions about the book. Dont email me. While its a bit more work for you to do it this way, since youll need to make a GitHub account if you dont already have one, it helps me. I wrote this book, in part, to help my career so having evidence that people read it and are contributing to it is important to me. Its a way to publicly measure the books impact. Where to find data included in this book To download the data used in this book please see here. Each of the files that are used in this book are available to download at that link. At the top of every chapter that uses one of these files Ill say exactly which file(s) you need to download. The best way to use this book is to follow along by downloading the data and running the code that I include in each chapter. Where to find code included in this book If youre reading this book through its website, you can easily copy the code by clicking on the Copy to clipboard option on the top right of every chunk of code. This button, shown in the image below, will copy all of the code in the chunk and you can then paste (through Control/Command+V) into R. Ive also made each chapter available to download as an R file that has every line of code used in each chapter available to you to run. To download the files, please go to the books GitHub page here. Ive saved each chapter twice - once where it only includes the code used (in the just_code folder) and once where it includes the code and all of the text in the chapter (in the code_and_text folder). So download whichever one you want to use. The code is identical in each. This is formally known as an integrated development environment or an IDE. "],["about-the-author.html", "About the author", " About the author Jacob Kaplan is the a researcher at the Princeton School of Public and International Affairs. He holds a PhD from the University of Pennsylvania. He is the author of several R packages that make it easier to work with data, including fastDummies and asciiSetupReader. His website allows easy analysis of crime-related data, and he has released over a dozen crime data sets that he has compiled, cleaned, and made available to the public. He is also the author of books on the two primary criminal justice data sets: the FBIs Uniform Crime Reporting (UCR) Program Data and the FBIs National Incident Based Reporting System (NIBRS) data. "],["a-soup-to-nuts-project-example.html", "1 A soup to nuts project example 1.1 Big picture data example 1.2 Little picture data example 1.3 Reusing and modifying code", " 1 A soup to nuts project example Before we get into exactly how to use R, well go over a brief example of a kind of data project that youd do in the real world. For this chapter well look at FBI homicide data that you can download here. The file is called shr_1976_2020.rds. 1.1 Big picture data example Below is a large chunk of R code along with some comments about what the code does. The purpose of this example is to show that with relatively little code (excluding blank lines and comments, there are only 35 lines of R code here) you can go from opening a data set to making a graph that answers your research question. I dont expect you to understand any of this code as it is fairly complex and involves many different concepts in programming. So if the code is scary - and for many early programmers seeing a bunch of code that you dont understand is scary and overwhelming - feel free to ignore the code itself. Well cover each of these skills in turn throughout the book so that by the end of the book you should be able to come back and understand the code (and modify it to meet your own needs). The important thing is that you can see exactly what R can do (and this is only a tiny example of Rs flexibility) and think about the process to get there (which well talk about below). At the time of this writing, the FBI had just released 2020 crime data, which showed about a 30% increase in murders relative to 2019. This had led to an explosion of (in my opinion highly premature) explanations of why exactly murder went up so much in 2020. A common explanation is that it is largely driven by gun violence among gang members who are killing each other in a cyclical pattern of murders followed by retaliatory murders. For our coding example, well examine that claim by seeing if gang violence did indeed increase, and whether it increased more than other types of murders. The end result is the graph below. It is, in my opinion, a fairly strong answer to our question. It shows the percent change in murders by the victim-offender relationship from 2019 to 2020. This is using FBI murder data, which technically does have a variable that says if the murder is gang related, but its a very flawed variable (i.e. vast undercount of gang-related murders) so I prefer to use stranger and acquaintance murders as a rough proxy. And we now have an easy to read graph that shows that while indeed stranger and acquaintance murders did go up a lot, nearly all relationship groups experienced far more murders in 2020 than in 2019. This suggests that there was a broad increase in murder in 2020, and it was not driven merely by an increase in one or a few groups. These graphs (though modified to a table instead of a graph) were included in a article I contributed to on the site FiveThirtyEight in discussing the murder increase in 2020. So this is an actual work product that is used in a major media publication - and is something that youll be able to do by the end of this book. For nearly all research you do youll follow the same process as in this example: load data into R, clean it somehow, and create a graph or a table or do a regression on it. While this can range from very simple to very complex depending on your exact situation (and how clean the data is that you start with), all research projects are essentially the same. Please look at the following large chunk of code. Well next go through each of the different pieces of this code to start understanding how they work. Throughout the course of this book well cover these steps in more detail - as most research programming work follows the same process - so here well talk more abstractly about what each does. The goal is for you to understand the basic steps necessary for using R to do research, and to understand how R can do it - but not having to understand what each line of code does just yet. library(dplyr) # Used to aggregate data library(ggplot2) # Used to make the graph # Warning: package &#39;ggplot2&#39; was built under R version 4.1.3 library(crimeutils) # Used to capitalize words in a column library(tidyr) # Used to reshape the data # Load in the data shr &lt;- readRDS(&quot;data/shr_1976_2020.rds&quot;) # See which agencies reported in 2019 and 2020 # An &quot;ori&quot; is a unique identifier code for agencies in FBI data agencies_2019 &lt;- shr$ori[shr$year == 2019] agencies_2020 &lt;- shr$ori[shr$year == 2020] # Get which agencies reported in both years so we have an # apples-to-apples comparison agencies_in_both &lt;- agencies_2019[agencies_2019 %in% agencies_2020] # Keep just data from 2019 and 2020 and where the agencies # is one of the agencies chosen above. Also keep only murder and # nonnegligent manslaughter (so excluding negligent manslaughter). shr_2019_2020 &lt;- shr[shr$year %in% 2019:2020, ] shr_2019_2020 &lt;- shr_2019_2020[shr_2019_2020$ori %in% agencies_in_both, ] shr_2019_2020 &lt;- shr_2019_2020[shr_2019_2020$homicide_type %in% &quot;murder and nonnegligent manslaughter&quot;, ] # Get the number of murders by victim-offender relationship in 2019 and 2020 # Then find the percent change in murders by this group from 2019 to 2020 # Sort data by smallest to largest percent change shr_difference &lt;- shr_2019_2020 %&gt;% group_by(year) %&gt;% count(victim_1_relation_to_offender_1) %&gt;% spread(year, n) %&gt;% mutate( difference = `2020` - `2019`, percent_change = difference / `2019` * 100, victim_1_relation_to_offender_1 = capitalize_words(victim_1_relation_to_offender_1) ) %&gt;% filter(`2019` &gt;= 50) %&gt;% arrange(percent_change) # This is only for the graph. By default graphs order alphabetically # but this makes sure it orders it based on the ordering we made above # (smallest to largest percent change) shr_difference$victim_1_relation_to_offender_1 &lt;- factor(shr_difference$victim_1_relation_to_offender_1, levels = shr_difference$victim_1_relation_to_offender_1 ) # Makes a barplot showing the percent change from 2019 to 2020 in number # of murders by victim group. Labels the x-axis and the y-axis, shifts # the graph so that relationship labels are on the y-axis for easy reading. # And finally uses the &quot;crim&quot; theme that changes the colors in the graph to # make it a little easier to see. ggplot(shr_difference, aes( x = victim_1_relation_to_offender_1, y = percent_change )) + geom_bar(stat = &quot;identity&quot;) + ylab(&quot;% Change, 2020 Vs. 2019&quot;) + xlab(&quot;Who Victim Is Relative to Murderer&quot;) + coord_flip() + theme_crim() 1.2 Little picture data example Well now look at each piece of the larger chunk of code above, and Ill explain what it does. There are five different steps that I take to create the graph from the data we use: Load the packages we use Load the data Clean the data Aggregate the data Make the graph 1.2.1 Loading packages In R well often use code written by other people that have tools that we want to use in our code. To use this code we need to tell R that we want to use that particular package - and packages are just a collection of other peoples code. A collection of code for a specific purpose (e.g. making a graph, doing a very particular cleaning task) is called a function. Each package is a collection of functions. For this example, were using packages that help us clean and aggregate data or to graph it, so we load it here. The general convention is to start your R file with each of the packages you want to use at the top of the file. library(dplyr) library(ggplot2) library(crimeutils) library(tidyr) 1.2.2 Loading data Next we need to load in our data. The data were using is a type of R data file called an .Rds file so we load it using the function readRDS(), which is one of the functions built into R so we dont actually need to use any package for it. For this example, were using data from the FBIs Supplementary Homicide Report which are an annual data set that has relatively detailed information on most (but not all, as not all agencies report data) murders in the United States. This includes the relationship between the victim and the offender (technically the suspected offender) in the murder, which is what well look at. When we read in the data to R we need to give it a name so R knows what it is called. Well call this data shr since that is the normal abbreviation for the Supplementary Homicide Report data. Normally in R we use lower cased letters when naming something, which is why were calling it shr rather than SHR. Each row of data is actually a murder incident, and there can be up to 11 victims per murder incident. So well be undercounting murders as in this example were only looking at the first victim in an incident. But, as its an example, this is fine as I dont want it to be too complicated and including more than just the first victim would greatly complicate our code. shr &lt;- readRDS(&quot;data/shr_1976_2020.rds&quot;) 1.2.3 Cleaning One of the annoying quirks of dealing with FBI data is that different agencies report each year. So comparing different years has an issue because youll be doing an apples-to-oranges competition as an agency may report one year but not another. So for this data the first thing we need to do is to make sure were only looking at agencies that reported data in both years. The first few lines check which agencies reported in 2019 and which agencies reported in 2020. We do this by looking at which ORIs (in the ori column) are present in each year (as agencies that did not report wont be in the data). An ORI is the FBI term for a unique ID for that agency. Then we make a vector, which has only the ORIs that are present in both years. We then subset the data to only data from 2019 and 2020 and where the agency reported in both years. Subsetting essentially means that we only keep the rows of data that meet those conditions. Another quirk of this data is that it includes homicides that are not murder - namely, negligent manslaughter. So the final subsetting condition we use is that it only includes murder and nonnegligent manslaughter. agencies_2019 &lt;- shr$ori[shr$year == 2019] agencies_2020 &lt;- shr$ori[shr$year == 2020] agencies_in_both &lt;- agencies_2019[agencies_2019 %in% agencies_2020] shr_2019_2020 &lt;- shr[shr$year %in% 2019:2020, ] shr_2019_2020 &lt;- shr_2019_2020[shr_2019_2020$ori %in% agencies_in_both, ] shr_2019_2020 &lt;- shr_2019_2020[shr_2019_2020$homicide_type %in% &quot;murder and nonnegligent manslaughter&quot;, ] 1.2.4 Aggregating Now we have only the rows of data that we want. Each row of data is a single murder incident, so we want to aggregate that data to the year-level and see how many murders there were for each victim-offender relationship group. The following chunk of code does that and then finds the percent difference. Since we can have large percent changes due to low base rates, we then remove any rows where there were fewer than 50 murders of that victim-offender relationship type in 2019. Finally, we arrange the data from smallest to largest difference. Well print out the data just to show you what it looks like. shr_difference &lt;- shr_2019_2020 %&gt;% group_by(year) %&gt;% count(victim_1_relation_to_offender_1) %&gt;% spread(year, n) %&gt;% mutate( difference = `2020` - `2019`, percent_change = difference / `2019` * 100, victim_1_relation_to_offender_1 = capitalize_words(victim_1_relation_to_offender_1) ) %&gt;% filter(`2019` &gt;= 50) %&gt;% arrange(percent_change) shr_difference # # A tibble: 16 x 5 # victim_1_relation_to_offender_1 `2019` `2020` difference percent_change # &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; # 1 Wife 330 294 -36 -10.9 # 2 Brother 93 86 -7 -7.53 # 3 Daughter 89 86 -3 -3.37 # 4 Son 157 160 3 1.91 # 5 Father 95 101 6 6.32 # 6 Boyfriend 164 180 16 9.76 # 7 Girlfriend 390 431 41 10.5 # 8 Mother 118 134 16 13.6 # 9 Acquaintance 1494 1729 235 15.7 # 10 Stranger 1549 1886 337 21.8 # 11 Neighbor 85 105 20 23.5 # 12 Other Family 209 260 51 24.4 # 13 Other - Known To Victim 757 955 198 26.2 # 14 Friend 272 358 86 31.6 # 15 Husband 58 79 21 36.2 # 16 Unknown 6216 8504 2288 36.8 1.2.5 Graphing Once we have our data cleaned and organized in the way we want, we are ready to graph it. By default when R graphs data it will organize it alphabetically. In our case we want it ordered by smallest to largest change in the number of murders between 2019 and 2020 by relationship type. So we first tell R to order it by the relationship type variable, which weve already sorted in the last section of code. Then we use the ggplot() function (which is covered extensively in Chapters 14 and 15) to make our graph. In our code we include the data set were using, which is the shr_difference data and the columns we want to graph. Then we tell it we want to create a bar chart and what we want the x-axis and y-axis labels to be. Finally, we have two lines that just affect how the graph looks. All of this is covered in the two graphing chapters, but is only several lines of code to go from cleaned data to a beautiful - and informative - graphic. shr_difference$victim_1_relation_to_offender_1 &lt;- factor(shr_difference$victim_1_relation_to_offender_1, levels = shr_difference$victim_1_relation_to_offender_1 ) ggplot(shr_difference, aes( x = victim_1_relation_to_offender_1, y = percent_change )) + geom_bar(stat = &quot;identity&quot;) + ylab(&quot;% Change, 2020 Vs. 2019&quot;) + xlab(&quot;Who Victim Is Relative to Murderer&quot;) + coord_flip() + theme_crim() 1.3 Reusing and modifying code One of the main benefits of programming is that once you write code to do one thing, its usually very easy to adapt it to do a similar thing. Below Ive copied some of the code we used above and changed only one thing: instead of looking at the column victim_1_relation_to_offender_1 were now looking at the column offender_1_weapon. Thats all I did, everything else is identical. Now after about 30 seconds of copying and changing the column name, we have a graph that shows weapon usage changes from 2019 to 2020 instead of victim-offender relationship. This is one of the key benefits of programming over something more click intensive like using Excel or SPSS.2 Theres certainly more upfront work than just clicking buttons, but once we have working code we can very quickly reuse it or modify it slightly. shr_difference &lt;- shr_2019_2020 %&gt;% group_by(year) %&gt;% count(offender_1_weapon) %&gt;% spread(year, n) %&gt;% mutate( difference = `2020` - `2019`, percent_change = difference / `2019` * 100, offender_1_weapon = capitalize_words(offender_1_weapon) ) %&gt;% filter(`2019` &gt;= 50) %&gt;% arrange(percent_change) shr_difference$offender_1_weapon &lt;- factor(shr_difference$offender_1_weapon, levels = shr_difference$offender_1_weapon ) ggplot(shr_difference, aes( x = offender_1_weapon, y = percent_change )) + geom_bar(stat = &quot;identity&quot;) + ylab(&quot;% Change, 2020 Vs. 2019&quot;) + xlab(&quot;Offender Weapon&quot;) + coord_flip() + theme_crim() Im aware that technically you can write SPSS code. However, every single person I know who has ever used SPSS does so by clicking buttons and is afraid of writing code. "],["intro-to-r.html", "2 Introduction to R and RStudio 2.1 Using RStudio 2.2 Assigning variables 2.3 What are functions (and packages)? 2.4 Reading data into R 2.5 First steps to exploring data", " 2 Introduction to R and RStudio In this chapter youll learn to open a data file in R. That file is ucr2017.rda, which youll need to download from the data repository available here. 2.1 Using RStudio In this lesson well start by looking at RStudio then write some brief code to load in some crime data and start exploring it. This lesson will cover code that you wont understand completely yet. That is fine, well cover everything in more detail as the lessons progress. RStudio is the interface we use to work with R. It has a number of features to make it easier for us to work with R. While not strictly necessary to use, most people who use R do so through RStudio. When using R you dont need to open up both R and RStudio on your computer. Just open RStudio and itll internally use R. Well spend some time right now looking at RStudio and the options you can change to make it easier to use (and to suit your personal preferences with appearance) as this will make all of the work that we do in this book easier. When you open up RStudio youll see four panels, each of which plays an important role in RStudio. Your RStudio may not look like the setup I have in the following image - that is fine, well learn how to change the appearance of RStudio soon. At the top right of the image (and this may be in a different location on your RStudio) is the Console panel. Here you can write code, hit enter/return, and R will run that code. If you write 2+2 it will return (in this case that just mean it will print an answer) 4. This is useful for doing something simple like using R as a calculator or quickly looking at data. In most cases during research this is where youd do something that you dont care to keep. This is because when you restart R it wont save anything written in the Console. To do reproducible research or to be able to collaborate with others you need a way to keep the code youve written. The way to keep the code youve written in a file that you can open later or share with someone else is by writing code in an R Script (if youre familiar with Stata, an R Script is just like a .do file). An R Script is essentially a text file (similar to a Word document) where you write code. To run code in an R Script just click on a line of code or highlight several lines and hit enter/return or click the Run button on the top right of the Source panel shown in the top left of the above image. Youll see the lines of code run in the Console and any output (if your code has an output) will be shown there too (making a plot will be shown in a different panel as well see soon). For code that you dont want to run, called comments, start the line with a pound sign # and that line will not be run (it will still print in the console if you run it but it wont do anything). These comments should explain the code you wrote (if its not otherwise obvious what the code does). It is good practice to do all of your code writing in an R Script - even if you delete some lines of code later - as it eliminates the possibility of losing code or forgetting what you wrote. Having all the code in front of you in a text file also makes it easier to understand the flow of code from start to finish for a task - an issue well discuss more in later lessons. While the Source and Console panels are the ones that are of most use, there are two other panels worth discussing. As these two panels let you interchange which tabs are available in them, well return to them shortly in the discussion of the options RStudio has to customize it. 2.1.1 Opening an R Script When you want to open up a new R Script you can click File on the very top left, then R Script. It will open up the script in a new tab inside of the Source panel. There are also a number of other file options available: R Presentation which can make PowerPoints; R Markdown, which can make Word Documents or PDFs that incorporate R code used to make tables or graphs (and which well cover in Chapter 7); and Shiny Web App to make websites using R. There is too much to cover for an introductory book such as this, but keep in mind the wide capabilities of R if you have another task to do. To open an R Script that is already saved to your computer, click Open File and navigate to the file that you want to open. 2.1.2 Setting the working directory Many research projects incorporate data that someone else (such as the FBI or a local police agency) has put together. In these cases, we need to load the data into R to be able to use it. In a little bit well load a data set into R and start working on it, but lets take a step back now and think about how to even load data. First, well need to get the data onto our computer somehow, probably by downloading it from an agencys website. Lets be specific - we dont download it to our computer, we download it to a specific folder on our computer (usually defaulted to the Downloads folder on a Windows machine). So lets say you wanted to load a file called data into R. If you have a file called data in both your Desktop and your Downloads folder, R wouldnt know which one you wanted. And unless your data was in the folder R searches by default (which may not be where the file is downloaded by default), R wont know which file to load. We need to tell R explicitly which folder has the data to load. We do this by setting the Working Directory (or the Folders where I want you, R, to look for my data in more simple terms). To set a working directory in R click the Session tab on the top menu, scroll to Set Working Directory, then click Choose Directory. This will open a window where you can navigate to the folder you want. After clicking Open in that window youll see a new line of code in the Console starting with setwd() and inside of the parentheses is the route your computer takes to get to the folder you selected. And now R knows which folder to look in for the data you want. It is good form to start your R Script with setwd() to make sure you can load the data. Copy the line of code that says setwd() (which stands for set working directory), including everything in the parentheses, to your R Script when you start working. 2.1.3 Changing RStudio Your RStudio looks different than my RStudio because I changed a number of settings to suit my preferences. To do so yourself click the Tools tab on the top menu and then click Global Options. This opens up a window with a number of different tabs to change how R behaves and how it looks. 2.1.3.1 General Under Workspace in the General tab make sure to uncheck the Restore .RData into workspace at startup and to set Save workspace to .RData on exit: to Never. What this does is make sure that every time you open RStudio it starts fresh with no objects (essentially data loaded into R or made in R) from previous sessions. This may be annoying at times, especially when it comes to loading large files, but the benefits far outweigh the costs. You want your code to run from start to finish without any errors. Something Ive seen many students do is write some code in the Console (or in their R Script but out of order of how it should be run) to fix an issue with the data. This means their data is how it should be, but when the R session restarts (such as if the computer restarts) they wont be able to get back to that point. Making sure your code handles everything from start to finish is well-worth the avoided headache of trying to remember what code you did to fix the issue previously. 2.1.3.2 Code The Code tab lets you specify how you want the code to be displayed. The important section for us is to make sure to check the Soft-wrap R source files check-box. If you write a very long line of code it gets too big to view all at once and you must scroll to the right to read it all. That can be annoying as you wont be able to see all the code at once. Setting Soft-wrap makes it so if a line is too long it will just be shown on multiple lines, which solves that issue. In practice it is best to avoid long lines of codes as it makes it hard to read, but that isnt always possible. 2.1.3.2.1 Saving Inside of the Code tab we also want to turn on an option to have RStudio automatically save the R script when we arent using it. This is like how Google Docs automatically saves your document every second or so. While we should be saving our file often (using the little floppy disk icon near the top of RStudio), having RStudio automatically save adds a level of security as it prevents losing a lot of progress if we forget to save and RStudio crashes or we close it. To set it to autosave, move to the Saving tab, and check the Automatically save when editor loses focus box. So if you click out of RStudio or stop typing, it will automatically save. You can also say how long to wait before saving with options ranging from 500 milliseconds to 10,000 milliseconds, which is the same as 0.5 seconds to 10 seconds. 2.1.3.3 Appearance The Appearance tab lets you change the background, color, and size of text. Change it to your preferences. 2.1.3.4 Pane Layout The final tab well look at is Pane Layout. This lets you move around the Source, Console, and the other two panels. There are a number of different tabs to select for the panels (unchecking one just moves it to the other panel, it doesnt remove it from RStudio), and well talk about three of them. The Environment tab shows every object you load into R or make in R. So if you load a file called data you can check the Environment tab. If it is there, you have loaded the file correctly. As well discuss more in Section 2.3, the Help tab will open up to show you a help page for a function you want more information on (well also discuss exactly what a function is below. But for now just think of a function as a shortcut to using code that someone else wrote). The Plots tab will display any plot you make. It also keeps all plots youve made (until restarting RStudio) so you can scroll through the plots. 2.1.4 Helpful cheat sheets RStudio also includes a number of links to helpful cheat sheets for a few important topics. To get to it click Help, then Cheatsheets, and click on whichever one you need. 2.2 Assigning variables When were using R for research the general process is to load data, change it somehow (such as deleting rows we dont want, aggregating from some small unit such as monthly crime to a higher unit such as yearly crime), and then analyze it. To do all this we need to be able to make sure each step we do actually changes the data. This seems simple but is actually a very common issue Ive noticed when working with new R programmers - they run code on the data (e.g. deleting certain rows) but forget to save the change to that data. Lets look at an example of this. First, we need to know how to create objects in R. I use object in a very vague sense to mean anything that is loaded into R and can be manipulated. To create something in R we assign something to an object name. This is a very technical sentence so lets look at an example and then step back and try to understand that sentence. a &lt;- 1 Above I am creating the object a by assigning it the value of 1. In R terms, a is assigned 1 or a gets 1. In non-technical terms: a equals 1. We can print out a to see if this is true. a # [1] 1 When we print out a, it returns 1 since that was what a was assigned to. We can assign a another value, and it will overwrite 1 with whatever value we choose. a &lt;- 33 a # [1] 33 Now a is 33. Or a equals 33. Or a was assigned 33. Or a gets 33. Or we assigned 33 to a. There are a lot of ways to explain what we did here, which is quite frustrating and confusing to new R programmers. I use the terms assignment and gets only because that is the convention in R, but if its easier for you to talk about something equaling something else (instead of being assigned to that value), please do so! The &lt;- is what does the assignment, or what makes the thing on the left equal to the thing on the right. You might be thinking that itd be easier to simply use the equal sign instead of the &lt;- - we are making things equal after all. And youd be right. Using = does the exact same thing as &lt;-. a &lt;- 13 a # [1] 13 We can use = instead of &lt;- and get the same results (with very few exceptions and none that are relevant in this book). The reason that people use &lt;- instead of = is largely a matter of convention. Its just the thing that R programmers do so new programmers tend to adopt it. If its easier for you to use = instead of &lt;-, feel free to do that. In this book Ill use &lt;- and talk about assigning values because that is the convention in R. And while thats not really a good reason to do anything, I think that its important that new R programmers at least know what the proper conventions are and be able to speak the language (so to speak) of R programmers. This is also important when searching for more help on a topic as you need to know the right term to be able to ask for help (from other R programmers and from Google) easily. So far weve just been assigning a a value, or overwriting that value with a new value. We can also assign something new to have the same value as a. Lets make the object example_123_value.demonstration get the value that a has - or in other words make example_123_value.demonstration be equal to a. example_123_value.demonstration &lt;- a example_123_value.demonstration # [1] 13 I use name example_123_value.demonstration just an example of what you can include in an object name - any character (lower or uppercase), any number (just cant start with a number), and some punctuation (e.g. underscores and periods). Spaces are not allowed. In practice youll want to call each object something specific so you know what it is, and ideally make the name as short as possible. For example, if you are using crime data from Houston youll want to call it something like houston_crime. The R convention is to only use lowercase characters and include only underscores as the punctuation, but you can name it whatever is most useful to you. As noted at the start of the section, a lot of new programmers will make a change to an object but forget to assign the result back into the object (or into a new object). This means that that object wont actually change. For example, lets say we want to multiply example_123_value.demonstration by 10. If we do example_123_value.demonstration * 10 then itll print out the result in the console, but not actually change example_123_value.demonstration. What we need to do is assign that result of the multiplication back into example_123_value.demonstration. Lots of new programmers forget to assign the results back into the object, which understandably leads to lots of confusion since the object is now not what they expect it to be. example_123_value.demonstration &lt;- example_123_value.demonstration * 10 example_123_value.demonstration # [1] 130 Ive been saying object a lot, without defining it. An object is a bit tricky to define, especially at this stage in the book. Throughout this book Ill be using object to describe something that has been assigned value, such as a and example_123_value.demonstration. This also includes outside data sets read into R, such as an Excel file loaded into R and even a set of R code that has been assigned to an object (which is called a function). Each object that you have created or loaded yourself can be found in the Environment tab. 2.3 What are functions (and packages)? When programming to do research youll often have to do the same thing multiple times. For example, many crime data sets are available as one file for each year of data. So if you are analyzing multiple years of data youll need to clean each file separately - and in most cases that involves using the exact same code for every file. This also includes doing things that other people have done. For example, most research leads to at least one graph being made. Since making graphs is so common, many people have spent a long time writing code to make it easy to make publication-ready graphs. Instead of doing all that work ourselves we can just use code that other people have written and made available to us. While we could do this by copying code, the easiest way to reuse code is to use functions. As noted in the previous section, a function is a bunch of code (it could range from a single line of code to hundreds of lines) that has been assigned to an object. Well dive into this topic in detail in Chapter 20 - including how to make your own functions - but using functions is such an important concept that well briefly introduce them here. Almost everything that you will do in R is through functions. For the most part thatll be using functions that other people have written that are available to use - and this includes functions that are built into R already and ones we have to download from other R programmers. Lets look at the function head() as an example. This is a function that is already built into R which means we dont need to do anything to use it. For functions that are written by other R programmers well need to download those functions and tell R we want to use it - and well show how in a bit. The way to identify a function is through the parentheses after the function name (the naming convention is the same as for objects as discussed in the previous section. We want a short, descriptive name that explains what the function does). If we see a word followed by parentheses, we can be confident that were looking at a function. The head() function prints out the first 6 rows of every column of a data.frame (which is essentially an Excel sheet, and something well cover in more detail in Chapter 3). head() is an extremely useful and common function in R, but just the name alone doesnt make it clear what it does or that we need to put a data object inside the parentheses. If you are having trouble understanding what a function does or how to use it, you can ask R for help and it will open up a page explaining what the function does, what options it has, and examples of how to use it. To do so we write help(function) or ?function in the console and it will open up that functions help page. For finding the help page of a function we do not include the parentheses part of the function: help(head) works while help(head()) does not. If we wrote help(head) to figure out what the head() function does, it will open up this page. Unfortunately, many help pages are not that useful. The following image shows the help page for head(), and it is not very friendly to a new R programmer. In cases where the help page is not useful, and youre looking at functions not covered in this book, I recommend looking online for help pages dedicated to that function or broader programming sites such as Stack Overflow, where people can ask questions about programming. For head(), all we need to do is tell the function what data were looking at. In programming terms, the input to the function (what we have to include in the parentheses) is the name of our data object. Well look at the very commonly used data called mtcars. mtcars is one of a small number of data files that are already in R when you open it. These are included in R just as examples of data to use when testing our code or teaching people to use R. Just type mtcars into the console and it will print out data to the console; theres nothing you need to do to load the data into R. mtcars has info about a number of cars with each row being a type of car and each column being information about the car such as the miles per gallon it gets and how many gears it has. Well use the head() function to print out just the first 6 rows of the mtcars data. head(mtcars) # mpg cyl disp hp drat wt qsec vs am gear carb # Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 # Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 # Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 # Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 # Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 # Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 Now we have the first 6 rows of every column from the mtcars data. This is a fairly simple function and is useful for quickly looking at our data. Many functions are more complicated than head() and involve multiple inputs rather than just the single input we had here. Some functions, for example, let you choose how you want the function to operate, as it can do so in multiple ways. Even in head() theres an optional input to choose how many rows you want it to return, with the default being 6. Since we didnt choose anything, the function stuck to the default and returned only 6 rows. Throughout this book well spend a lot of time introducing functions that other people have made and learning how to combine the functions together to be able to get our raw data (e.g. a CSV file downloaded from a police site) into a usable format for research (e.g. cleaned to include only the rows and columns we need to analyze and in the units we want). For functions that other people wrote, we need to tell R that we want to use these functions. We do so by having R download that persons package. A package is just the name for a collection of functions in an easily downloadable format. We can do all of the downloading through R, so we dont have to go searching for them. There are two ways to download a package in R: through writing R code or through a shortcut in RStudio. Downloading a package through R code uses - like pretty much everything else in R - a function. This function is install.packages(), where we put the name of the package we want in the (). This name also has to be in quotes since it is an object that is not currently in R. Lets install the package caesar, which is a simple package I made that creates a Caesar cipher from some text. We need to run the code install.packages(\"caesar\") and be sure to spell caesar right and put it in quotes. install.packages(&quot;caesar&quot;) The RStudio shortcut way is to go to the Packages tab and then click Install on the top left of this tab. This will open up a window as shown in the following image where you can enter the name of the package you want. Then click Install and RStudio will install it for you. Also in this tab is the Update button, which allows you to update packages that you have already installed. Since R programmers generally provide updates to their packages (usually bug fixes but occasionally new features and new functions), its important to update your packages every several months or so. Once we have downloaded the package, we need to tell R that we want to use that package. There are thousands of R packages and youll likely have hundreds downloaded before long (if a package relies on other packages to work itll download those too. So even if you install a single package it may also install other packages necessary for the package you want). Some packages have functions with the same name (but they do different things) so using all packages at once will cause issues since we wont know which functions were actually using. So we only want to use the packages we need for that task. We need a way to tell R that we want to use a package. We only need to do this once per session - that is, once before restarting RStudio. The way to do this is to use the function library(), where we put the package name in the parentheses. Since the package is something that has been installed to R, we dont need to have quotes around the name. library(caesar) Now we can run the caesar() function and make a Caesar cipher for that text (its just a coincidence that the function name is the same as the package name). caesar(&quot;example text&quot;) # [1] &quot;hAdpsohcwhAw&quot; 2.4 Reading data into R For many research projects youll have data produced by some outside group (e.g. FBI, local police agencies) and you want to take that data and put it inside R to work on it. We call that reading data into R. R is capable of reading a number of different formats of data, which we will discuss in more detail in Chapter 4. Here, we will talk about the standard R data file only. 2.4.1 Loading data As we learned in Section 2.1.2, we need to set our working directory to the folder where the data is. For my own setup, R is already defaulted to the folder with this data so I do not need to set a working directory. For those following along on your own computer, make sure to set your working directory now. The load() function lets us load data already in the R format. These files will end in the extension .rda or sometimes .Rda or .RData. Since we are telling R to load a specific file, we need to have that file name in quotes and include the file extension .rda. With .rda data, the object inside the .rda file already has a name so we dont need to assign a name to the data. With other forms of data such as .csv files, we will need to do that as well see in Chapter 4. In this example (and elsewhere in this book when I load in data), I have all of the data in a folder called data in my working directory, which is why I have data/ before the data name. You do not need this as you should have all of your data directly in your working directory. load(&quot;data/ucr2017.rda&quot;) 2.5 First steps to exploring data The object we loaded is called ucr2017. Well explore this data more thoroughly in Chapter 11, but for now lets use four simple (and important) functions to get a sense of what the data holds. To use each of these functions, we need to write the name of the data set (without quotes since we dont need quotes for an object already made in R) inside the (). head() summary() plot() View() Note that the first three functions are lowercase while View() is capitalized. That is simply because older functions in R were often capitalized while newer ones use all lowercase letters. R is case sensitive so using view() will not work. The head() function prints the first 6 rows of each column of the data to the console. This is useful to get a quick glance at the data but has some important drawbacks. When using data with a large number of columns it can be quickly overwhelming by printing too much. There may also be differences in the first 6 rows with other rows. For example, if the rows are ordered chronologically (as is the case with most crime data) the first 6 rows will be the most recent. If data collection methods or the quality of collection changed over time, these 6 rows wont be representative of the data. head(ucr2017) # ori year agency_name state population actual_murder actual_rape_total actual_robbery_total # 1 AK00101 2017 anchorage alaska 296188 27 391 778 # 2 AK00102 2017 fairbanks alaska 32937 10 24 40 # 3 AK00103 2017 juneau alaska 32344 1 50 46 # 4 AK00104 2017 ketchikan alaska 8230 1 19 0 # 5 AK00105 2017 kodiak alaska 6198 0 15 4 # 6 AK00106 2017 nome alaska 3829 0 7 0 # actual_assault_aggravated # 1 2368 # 2 131 # 3 206 # 4 14 # 5 41 # 6 52 The summary() function gives a six-number summary of each numeric or Date column in the data. For other types of data, such as character types (which are just columns with words rather than numbers or dates), itll say what type of data it is. Well cover different types of data in Chapter 3. The six values it returns for numeric and Date columns are The minimum value The value at the 1st quartile The median value The mean value The value at the 3rd quartile The max value In cases where there are NAs, it will say how many NAs there are. An NA value is a missing value. Think of it like an empty cell in an Excel file. NA values will cause issues when doing math, such as finding the mean of a column, as R doesnt know how to handle a NA value in these situations, though summary() automatically excludes NAs when doing the math operations. summary(ucr2017) # ori year agency_name state population actual_murder # Length:15764 Min. :2017 Length:15764 Length:15764 Min. : 0 Min. : 0.000 # Class :character 1st Qu.:2017 Class :character Class :character 1st Qu.: 914 1st Qu.: 0.000 # Mode :character Median :2017 Mode :character Mode :character Median : 4460 Median : 0.000 # Mean :2017 Mean : 19872 Mean : 1.069 # 3rd Qu.:2017 3rd Qu.: 15390 3rd Qu.: 0.000 # Max. :2017 Max. :8616333 Max. :653.000 # actual_rape_total actual_robbery_total actual_assault_aggravated # Min. : -2.000 Min. : -1.00 Min. : -1.00 # 1st Qu.: 0.000 1st Qu.: 0.00 1st Qu.: 1.00 # Median : 1.000 Median : 0.00 Median : 5.00 # Mean : 8.262 Mean : 19.85 Mean : 49.98 # 3rd Qu.: 5.000 3rd Qu.: 4.00 3rd Qu.: 21.00 # Max. :2455.000 Max. :13995.00 Max. :29771.00 The plot() function allows us to graph our data. For criminology research we generally want to make scatterplots to show the relationship between two numeric variables, time-series graphs to see how a variable (or variables) change over time, or barplots comparing categorical variables. Here, well make a scatterplot seeing the relationship between a citys number of murders and their number of aggravated assaults (assault with a weapon or that causes serious bodily injury). To do so we must specify which column is displayed on the x-axis and which one is displayed on the y-axis. In Section 10.3.1 well talk explicitly about how to select specific columns from our data. For now, all you need to know is to select a column in which you write the data set name followed by a dollar sign $, followed by the column name. Do not include any quotations or spaces (technically spaces can be included but make it a bit harder to read and are against conventional style when writing R code so well exclude them). Inside of plot() we say that x = ucr2017$actual_murder so that column goes on the x-axis and y = ucr2017$actual_assault_aggravated so aggravated assault goes on the y-axis. And thats all it takes to make a simple graph. plot(x = ucr2017$actual_murder, y = ucr2017$actual_assault_aggravated) Finally, View() opens essentially an Excel file of the data set you put inside the (). This allows you to look at the data as if it were in Excel (though you cant edit the data at all here) and is a good way to start to understand the data. View(ucr2017) "],["data-types.html", "3 Data types and structures 3.1 Data types 3.2 Numeric, character, and logical (boolean) 3.3 Data structures", " 3 Data types and structures 3.1 Data types When you read a sentence like two plus two you know the answer is four. R doesnt know that. This is because R takes things very literally. It will read two as a word, not as a number. For R to understand numbers you need to specify that youre talking about numbers, and not just words. Lets look at an example, making two variables which each have the value of 2. a &lt;- &quot;2&quot; b &lt;- &quot;2&quot; We now have a and b that are equal to 2 (in quotes!). Lets try to add them. a + b # Error in a + b: non-numeric argument to binary operator We get an error that is a technical way of saying that we did math on something that isnt a number. Thats because we made a and b get 2 with quotes around it, which R interpreted as a word, not as a number. If we change a and b to 2 (without quotes), then R will know that the 2 is a number, and will do math on it. a &lt;- 2 b &lt;- 2 a + b # [1] 4 This may seem like a pretty simple concept but is fundamental to how R works, and can trip up new and experienced programmers alike. R trusts you. It only knows what you tell it. If you tell it that something is a word (by including quotes), it will treat it as a word, even if it looks to you like a number. So we must be very precise about what code we write, as R wont (for the most part) fix our mistakes - though it will give us an error if we try to do something it doesnt like, like add two words. 3.2 Numeric, character, and logical (boolean) There are three main data types that are important to know for using R to do research: numeric, character, and logical. A numeric type is a number, and this includes both integers like 2 and decimals like 2.5. You can tell something is numeric if it is a number and there are no quotes around it. 2 is a number, 2 is not. For real data this will likely be something like the age of an individual or the number of crimes in a city. We want it as numeric type because we can do math on numbers. For example, we can find the average age of victims of crimes, or the median number of crimes in a city each week. This wont work unless R knows that these values are numbers. A character is just a word or a set of words. If it is in quotes its a character. Other programming languages generally call this a string instead of a character, but they mean the same thing. Pretty much anything that youd write in English class fits in here. Finally, a logical data type is just a true or false value, though in R it must be written all in capital letters: TRUE or FALSE. This is also referred to as a Boolean value. Booleans or logical data are useful when comparing two things. For example, we can see if 2 is equal to 3. 2 == 3 # [1] FALSE Its not, so R returned FALSE (the == just compares the thing on the left to the thing on the right). This is very useful when we want to keep only certain rows in our data. For example, if we had data on multiple years of crime and we only wanted to keep a single year (lets say 2020), we could tell R to keep only rows where the year equals 2020 - where it is TRUE that that rows year column is equal to what year we want. Well cover this in great detail in Chapter 10. While you could try to figure out what type of data something is just by looking at it, R has a number of functions to check for you. Well look at a few general functions that tell you the type of data something is, and then ones that check if the data is a specific type. First, the is() function tells you all of the types of data something is - and a value can actually have multiple types. While it cant be both, for example, numeric and character, it can have other data types that well look at in the next section. First, lets look at what is() returns (prints out to the console) for a few simple examples. is(2) # [1] &quot;numeric&quot; &quot;vector&quot; Checking what 2 is tells us that it is both a numeric type and a vector type. is(&quot;2&quot;) # [1] &quot;character&quot; &quot;vector&quot; &quot;data.frameRowLabels&quot; &quot;SuperClassMethod&quot; Checking 2 (in quotes), gives us four different types of data for this value: character, vector, data.frameRowLabels, and SuperClassMethod. You can ignore the last two types, we just are interested in that it is a character type and, like the type of 2, is a vector. is(TRUE) # [1] &quot;logical&quot; &quot;vector&quot; Finally, checking what TRUE is returns both logical and vector. We expected logical since TRUE is a logical type. Again, we see that it is also a vector type. TRUE has to be both in capital letters and not be in quotes. If we write it in quotes then R will think it is a character, and if we have it lowercase and without quotes R will think that it is an object (such as something we make using &lt;- and not a Boolean). is(&quot;TRUE&quot;) # [1] &quot;character&quot; &quot;vector&quot; &quot;data.frameRowLabels&quot; &quot;SuperClassMethod&quot; is(true) # Error in is(true): object &#39;true&#39; not found All three of the values we checked say that they are a vector type. Well cover vectors in the next section, but for now lets see one other function that tells us the type of data something is. If we use class() instead of is() well get just the first value returned in the types of data that we input. class(2) # [1] &quot;numeric&quot; class(&quot;2&quot;) # [1] &quot;character&quot; class(TRUE) # [1] &quot;logical&quot; In a lot of cases well want to check if some data is a specific type. For example, we might want to check that the year column of a data set is numeric, rather than say character. We do this with three functions, each of which checks that the data input (the data put in the parentheses of the function) is that type of data or not. These functions are: is.numeric(), is.character(), and is.logical(). Running any of these functions will actually return a logical value, either TRUE or FALSE telling us if the value inputted is that type. is.numeric(2) # [1] TRUE is.character(&quot;2&quot;) # [1] TRUE is.character(2) # [1] FALSE is.logical(TRUE) # [1] TRUE So far weve just been checking the value of a single thing: a single number, a single character/string, or a single logical/Boolean value. In practice almost everything we do will be on a column of a data set. These functions still work in the exact same way. We input the column (using the data$column syntax discussed in Chapter 2 to specify which data set we want and specifically which column in that data set) and the function will behave just like it did above. Thats because each column can only be a single type of data; if the column is numeric, all values will be numeric; if the column is character, all values in that column are character; if the column is logical, every value in that column is also logical. Lets use the UCR data from 2017 that was introduced in Chapter 2. Remember that the data must be in your working directory to load it. And here I have data/ before the data name because the data is in a folder called data in my working directory. For more on working directories, please see Section 2.1.2. load(&quot;data/ucr2017.rda&quot;) We need to know the column names before using them, so we can use the names() function to get a list of all of the column names (the colnames() function does the same thing). names(ucr2017) # [1] &quot;ori&quot; &quot;year&quot; &quot;agency_name&quot; &quot;state&quot; # [5] &quot;population&quot; &quot;actual_murder&quot; &quot;actual_rape_total&quot; &quot;actual_robbery_total&quot; # [9] &quot;actual_assault_aggravated&quot; Now we can check the types of some of the columns. Lets check the year column as an example. A year is a number so we may expect it to be numeric, but theres technically nothing stopping that data from being character type. It cant be logical type because then instead of a year value itd just be TRUE or FALSE, which is certainly not what a year is. is(ucr2017$year) # [1] &quot;numeric&quot; &quot;vector&quot; And we can use is.numeric() as another way to see if this column is numeric. is.numeric(ucr2017$year) # [1] TRUE 3.3 Data structures Well look in detail about two important data structures - vectors and data.frames - and then talk briefly about two other structures that are not that important in this book, but are nonetheless good to know that they exist. So far weve just been looking at either a single value, such as a &lt;- 1 or more complicated structures such as the ucr2017 data set, which is called a data.frame - Rs version of an Excel file. Data structures each operate a little differently from each other so its good to understand what they are and how they work. Well cover much more of how they work in Chapter 10, which covers how to subset data - which is just how to keep only certain values (such as specific rows or columns) in the data. 3.3.1 Vectors (collections of things) The first data structure well discuss is a vector. A vector is a collection of same type (numeric, character, logical, Date) values in a single object. When we made a in Chapter 2, we assigned it only a single value, such as a &lt;- 1. Usually well want to have a group of values - such as a set of years or a group of crime types - rather than just a single value. We can do this by using the same assignment method as a &lt;- 1 but put all of the values we want to assign to a into the function c() and separate each value by a comma. The c() function combines each value together into a single vector. Now, technically a single value, such as our object called a which now equals 1, is still a vector. In this case itd be a vector of length 1, since there is only one value in it. But when we generally talk about vectors there are multiple elements in it. Heres an example of making the object a be a vector with three values: 1, 2, and 3 (in that order). a &lt;- c(1, 2, 3) It is absolutely crucial to have the c() function, otherwise wed get an error from R. a &lt;- (1, 2, 3) # Error: &lt;text&gt;:1:8: unexpected &#39;,&#39; # 1: a &lt;- (1, # ^ It is likewise crucial to have a comma separating every single separate value. a &lt;- c(1 2 3) # Error: &lt;text&gt;:1:10: unexpected numeric constant # 1: a &lt;- c(1 2 # ^ The terminology for talking about values in a vector is that each value is called an element, and we identify them by the number they are, in order from start to finish. So here we have 1, 2, and 3, and we can say that the first element is 1, the second element is 2, the third element is 3. If we assigned a to b (b &lt;- a) we dont need to use the c() again. a is already a vector so if we assign its value to something, that carries over the vector. The c() is only necessary when first creating the vector. Note that vectors take values that are the same type, so all values included must be the same type, such as a number or a string. If they arent the same type, R will automatically convert it to the same type. c(&quot;cat&quot;, &quot;dog&quot;, 2) # [1] &quot;cat&quot; &quot;dog&quot; &quot;2&quot; Above we made a vector with the values cat, dog and 2 (without quotes) and it added quotes to the 2. Since everything must be the same type, R automatically converted the 2 to a string of 2. 3.3.2 Data.frames Nearly everything you do in this book and in research will be through data.frames. A data.frame is basically Rs version of an Excel file. More precisely, a data.frame is a collection of equal-length vectors. Each column in a data.frame is actually a vector. They must all be equal length so every column has the same number of rows. You cant have, for example, a data.frame with 10 rows of data for the city column and only 8 rows for the year column. It must be 10 for each. Since vectors can only be a single type, each row in a particular column in a data.frame must be the same type, though different columns can be different types. This is how we can have, for example, our ucr2017 data.frame, which has both numeric and character type columns. In this book Ill refer to data.frames by keeping it all lower case and with a dot between the words. This is just because the function to make one is data.frame(), and writing it this way is the normal convention. But writing it as a data frame is also fine. In nearly all cases well be using data that is loaded into R and is already in the structure of a data.frame (usually these will be Excel files or R data files like an .rda or .rds file). If we wanted to create our own data.frame we would use the data.frame(), function and the input would be vectors, which will become our columns. Lets make a simple one. If the vector is already created then R would automatically take the name of that vector object as the column name, otherwise we could name it ourselves example &lt;- data.frame( column_1 = c(1, 3, 5, 7, 9), column2 = c( &quot;hello&quot;, &quot;darkness&quot;, &quot;my&quot;, &quot;old&quot;, &quot;friend&quot; ) ) example # column_1 column2 # 1 1 hello # 2 3 darkness # 3 5 my # 4 7 old # 5 9 friend Now we have a new data.frame called example, which has two columns and five rows. We named the columns ourselves, and in this case we dont need to put the column name in quotes, though doing so would give the same result. Here were saying that the column column_1 is equal to the vector c(1, 3, 5, 7, 9) and column_2 is equal to the vector c(\"hello\", \"darkness\", \"my\", \"old\", \"friend\"). Were essentially creating an object inside of the data.frame() function but in this case we need to use the equal sign and not the &lt;- because R doesnt allow the use of &lt;- inside of a function. If we forget to name the columns, and our vectors arent already created with their own name, R will create a name based on the values in that vector. As shown below, this looks really bad so make sure to always name your columns. example &lt;- data.frame( c(1, 3, 5, 7, 9), c( &quot;hello&quot;, &quot;darkness&quot;, &quot;my&quot;, &quot;old&quot;, &quot;friend&quot; ) ) example # c.1..3..5..7..9. c..hello....darkness....my....old....friend.. # 1 1 hello # 2 3 darkness # 3 5 my # 4 7 old # 5 9 friend If the vectors are already made then we wont have an issue. R will default to the vector name, but we can override that if we want. column_1 &lt;- c(1, 3, 5, 7, 9) column2 &lt;- c(&quot;hello&quot;, &quot;darkness&quot;, &quot;my&quot;, &quot;old&quot;, &quot;friend&quot;) example &lt;- data.frame(column_1, overridden_name = column2 ) example # column_1 overridden_name # 1 1 hello # 2 3 darkness # 3 5 my # 4 7 old # 5 9 friend As with other objects, we can use the is() function to see what type it is. If we use is() on our example object, itll tell us that it is a data.frame. is(example) # [1] &quot;data.frame&quot; &quot;list&quot; &quot;oldClass&quot; &quot;vector&quot; We also often will want to know how many columns and rows a data.frame has. For finding the number of rows we use the function nrow(), and for finding the number of columns well use the ncol() function.3 In each the n part of the function just stands for number. So nrow() is number of rows. For each we put our data.frame object in the parentheses (without quotes since it is something already loaded in R), and it will return the number of rows/columns. nrow(example) # [1] 5 ncol(example) # [1] 2 Alternatively, we could have looked in the Environment tab which shows us the number of rows and columns of each data.frame that is loaded to R. For example, ucr2017 says it has 15764 obs. of 9 variables. This just means there are 15,764 rows and 9 variables. A variable in this context is just another way to say a column. However, youll occasionally want to find the exact number of rows and columns, and as youll often delete certain rows and columns from your data this can change throughout your code. So being able to use nrow() and ncol() is easier than repeatedly checking the Environment tab. You may encounter something called a data.table or a tibble. These are two popular variations of data.frames that operate much the same way as data.frames but with some different features. Well use tibbles in this book so will discuss their features when we use them. 3.3.3 Other data structures There are two other data structures that Ill mention only so you have heard of them and can look up more information on them if youd like. However, these are not that important to know about for the purpose of this book. Some of these structures may come up in rare cases when youre programming, so its important to know that they exist. The first data structure is a list. A list is essentially a vector but where different values can be different types. Lists are actually very powerful data structures and ones that youll encounter a lot when using R, but are almost entirely on the backend of R so not things youll actually deal with much. For example, all data.frames are actually lists. And more specifically, they are a list of vectors. Lists can come in handy because they can store different types of data structures. A single list can, for example, have a number, a vector, a matrix (discussed below), and an entire data.frame inside. Lists can even have other lists inside of them. Lets look at an example of this. list_example &lt;- list( &quot;hello&quot;, 1:5, 6:10, list(c(33, 66, 99)), head(mtcars) ) head(list_example) # [[1]] # [1] &quot;hello&quot; # # [[2]] # [1] 1 2 3 4 5 # # [[3]] # [1] 6 7 8 9 10 # # [[4]] # [[4]][[1]] # [1] 33 66 99 # # # [[5]] # mpg cyl disp hp drat wt qsec vs am gear carb # Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 # Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 # Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 # Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 # Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 # Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 The list that I called list_example contains six different elements in it: a character, two numeric vectors, a list of a numeric vector, and the first six rows of the mtcars data.frame. Lists can be useful when storing many different objects at once, but as they are not used too often for research-related programming Ill say no more of them. The other type of data structure is a matrix. A matrix is a two-dimensional object where every value is the same type. Think of a data.frame but each column has to be the same type. Below is an example of a matrix with values 1 through 50 and with five columns and five rows. Every value here is a number. matrix(1:50, nrow = 5, ncol = 5) # [,1] [,2] [,3] [,4] [,5] # [1,] 1 6 11 16 21 # [2,] 2 7 12 17 22 # [3,] 3 8 13 18 23 # [4,] 4 9 14 19 24 # [5,] 5 10 15 20 25 If I change it to have the first value be 1 (in quotes so it is a character) and the others be the numbers 2 through 50, the matrix will automatically convert everything to a character type. So it will remain having everything be the same type, but now everything is a character. matrix(c(&quot;1&quot;, 2:50), nrow = 5, ncol = 5) # [,1] [,2] [,3] [,4] [,5] # [1,] &quot;1&quot; &quot;6&quot; &quot;11&quot; &quot;16&quot; &quot;21&quot; # [2,] &quot;2&quot; &quot;7&quot; &quot;12&quot; &quot;17&quot; &quot;22&quot; # [3,] &quot;3&quot; &quot;8&quot; &quot;13&quot; &quot;18&quot; &quot;23&quot; # [4,] &quot;4&quot; &quot;9&quot; &quot;14&quot; &quot;19&quot; &quot;24&quot; # [5,] &quot;5&quot; &quot;10&quot; &quot;15&quot; &quot;20&quot; &quot;25&quot; We could also use the dim(), function which tells the dimensions of the data.frame. The dimensions are the rows and columns in the data.frame so dim() tell us the results of both nrow() and ncol() at the same time. This function returns a vector showing first the number of rows and then the number of columns. But I find it easier to simply ask for the number of rows or columns separately, and to not deal with the result, which has two values. "],["reading-and-writing-data.html", "4 Reading and writing data 4.1 Reading data into R 4.2 Writing data", " 4 Reading and writing data For this chapter youll need the following files, which are available for download here: fatal-police-shootings-data.csv, fatal-police-shootings-data.dta, fatal-police-shootings-data.sas, fatal-police-shootings-data.sav, sqf-2019.xlsx, sf_neighborhoods_suicide.rda, and shr_1976_2020.rds. So far in these lessons weve used data from a number of sources, but which came as .rda or .rds files, which are the standard R data formats. Many data sets, particularly older government data, will not come as .rda or .rds files but rather as Excel, Stata, SAS, SPSS, or fixed-width ASCII files. In this brief lesson, well cover how to read these formats into R as well as how to save data into these formats. Since many criminologists do not use R, it is important to be able to save the data in the language they use to be able to collaborate with them. In this lesson well load and save multiple files into R as examples of how R can handle data that is used in many different software programs. When loading data into R remember that your data must be in your current working directory or R wont be able to read it. For a refresher on working directories please see Section 2.1.2. In these examples I have my data in a folder called data that is in my working directory, which is why I use data/ when naming the file. You do not need to include data/ when loading in data on your computer. 4.1 Reading data into R 4.1.1 R 4.1.1.1 .rda and .rdata files As weve seen earlier, to read in data with a .rda or .rdata extension you use the function load() with the file name (including the extension) in quotation marks inside of the parentheses. This loads the data into R and calls the object the name it was when it was saved. Therefore we do not need to give it a name ourselves. Below, were loading the sf_neighborhoods_suicide.rda file, and it creates an object in R (which we can look at in the Environment tab) called sf_neighborhoods_suicide. It has the same name only because when I originally saved the file I saved it using the same name as it was called in R. But in practice I could have called it whatever I wanted. So it being the same name is convenient, as it is clear what the data is, but not necessary. load(&quot;data/sf_neighborhoods_suicide.rda&quot;) 4.1.1.2 .rds files For each of the other types of data well need to assign a name to the data were reading in. Whereas weve done x &lt;- 2 to say x gets the value of 2, now wed do x &lt;- DATA where DATA is the way to load in the data, and x will get the entire data set that is read in. This includes the other kind of R data file, the .rds file. Here, we must explicitly name the data - there is no name by default like in a .rda or a .rdata file. We can load .rds files into R using the readRDS(), which is built into R so we dont need any package to use it. Like in load(), we just put the name of the file (in quotes) in the parentheses. Here were naming it rds_example, but we can name it whatever we like. rds_example &lt;- readRDS(&quot;data/shr_1976_2020.rds&quot;) 4.1.2 Excel To read in Excel files that end in .csv, we can use the function read_csv() from the package readr (the function read.csv() is included in R by default so it doesnt require any packages but is far slower than read_csv() so we will not use it). install.packages(&quot;readr&quot;) library(readr) The input in the () is the file name ending in .csv. As it is telling R to read a file that is stored on your computer, the whole name must be in quotes. Unlike loading an .rda file using load(), there is no name for the object that gets read in so we must assign the data a name. We can use the name shootings as its relatively descriptive for what this data is and it is easy for us to write. shootings &lt;- read_csv(&quot;data/fatal-police-shootings-data.csv&quot;) read_csv() also reads in data to an object called a tibble, which is very similar to a data.frame but has some differences in displaying the data. If we run head() on the data it doesnt show all columns. This is useful to avoid accidentally printing out a massive amounts of columns. head(shootings) # # A tibble: 6 x 14 # id name date manner_o~1 armed age gender race city state signs~2 threa~3 flee body_~4 # &lt;dbl&gt; &lt;chr&gt; &lt;date&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt; &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt; # 1 3 Tim Elliot 2015-01-02 shot gun 53 M A Shel~ WA TRUE attack Not ~ FALSE # 2 4 Lewis Lee Lembke 2015-01-02 shot gun 47 M W Aloha OR FALSE attack Not ~ FALSE # 3 5 John Paul Quintero 2015-01-03 shot and ~ unar~ 23 M H Wich~ KS FALSE other Not ~ FALSE # 4 8 Matthew Hoffman 2015-01-04 shot toy ~ 32 M W San ~ CA TRUE attack Not ~ FALSE # 5 9 Michael Rodriguez 2015-01-04 shot nail~ 39 M H Evans CO FALSE attack Not ~ FALSE # 6 11 Kenneth Joe Brown 2015-01-04 shot gun 18 M W Guth~ OK FALSE attack Not ~ FALSE # # ... with abbreviated variable names 1: manner_of_death, 2: signs_of_mental_illness, 3: threat_level, # # 4: body_camera We can convert it to a data.frame using the function as.data.frame() though that isnt strictly necessary since tibbles and data.frames operate so similarly. shootings &lt;- as.data.frame(shootings) To read in Excel files that end in .xls or .xlsx, we need to use the readxl package and use the read_excel() function. Well read in data on stop, question, and frisks in New York City. install.packages(&quot;readxl&quot;) library(readxl) # Warning: package &#39;readxl&#39; was built under R version 4.1.3 sqf &lt;- read_excel(&quot;data/sqf-2019.xlsx&quot;) 4.1.3 Stata For the next three files, well use the package haven. install.packages(&quot;haven&quot;) library(haven) # Warning: package &#39;haven&#39; was built under R version 4.1.3 haven follows the same syntax for each data type and is the same as with read_csv() - for each data type we simply include the file name (in quotes, with the extension) and designate a name to be assigned the data. Like with read_csv(), the functions to read data through haven all start with read_ and end with the extension youre reading in. read_dta() - Stata file, extension .dta read_sas() - SAS file, extension .sas read_sav() - SPSS file, extension .sav To read the data as a .dta format we can copy the code above that read in the .csv file but change .csv to .dta and change the function from read_csv() to read_dta(). shootings &lt;- read_dta(&quot;data/fatal-police-shootings-data.dta&quot;) Since we called this new data shootings, R overwrote that object (without warning us!). This is useful because we often want to subset or aggregate data and call it by the same name to avoid making too many objects to keep track of, but watch out for accidentally overwriting an object without noticing! 4.1.4 SAS shootings &lt;- read_sas(&quot;data/fatal-police-shootings-data.sas&quot;) 4.1.5 SPSS shootings &lt;- read_sav(&quot;data/fatal-police-shootings-data.sav&quot;) 4.1.6 Fixed-width ASCII The final type of data source well talk about is a fixed-width ASCII. An ASCII file is just a text file and the fixed-width part means that each row has the exact same number of characters. This is a very old file format system that hopefully youll never encounter but is one that some government agencies - including the FBI for their annual data releases (though some individuals and organizations re-release the data in better formats like R and Stata files) - still use, so it is good to know to how handle. A fixed-width ASCII file is essentially an Excel file but with all of the columns smushed together. It also tries to reduce its file size by replacing long strings of text with short ones. For example, instead of including a state name itll usually have a number - as a number has fewer characters than an entire name, the file is therefore smaller. Each fixed-width ASCII file also comes with what is called a setup file, which is some code that tells the program youre reading the data into how to separate columns and when to replace the (in our example) numbers that indicate the state with the actual state name. In nearly all cases where you have a fixed-width ASCII, youll also be able to download the setup file from the same source, so I wont cover how to make a setup file yourself. To read fixed-width ASCII files into R well use the asciiSetupReader package, which I created myself for this very purpose. For more information on this package including details on all of the different options in the function, please see the packages site here. install.packages(&quot;asciiSetupReader&quot;) Well use the read_ascii_setup() function, which takes two mandatory inputs in the parentheses: the name of the data file (which will have a file name ending in .txt or .dat) and the name of the setup file (which will have a file name ending is .sps or .sas). Each of these file names must be in your current working directory, and you must put the names in quotes. The data file in this example is the 2020 FBI Supplementary Homicide Report data (their murder data set), which is called 2020_SHR_NATIONAL_MASTER_FILE.txt and the setup file is called ucr_shr.sps. We can name the object we read shr. library(asciiSetupReader) shr &lt;- read_ascii_setup( &quot;data/2020_SHR_NATIONAL_MASTER_FILE.txt&quot;, &quot;data/ucr_shr.sps&quot; ) 4.2 Writing data When were done with a project (or an important part of a project) or when we need to send data to someone, we need to save the data weve worked on in a suitable format. For each format we are saving the data in, we will follow the same syntax of function_name(data, \"file_name\") As usual we start with the function name. Then inside the parentheses we have the name of the object we are saving (as it refers to an object in R, we do not use quotations) and then the file name, in quotes, ending with the extension you want. For saving an .rda or .rdata file we use the save() function. For saving a .rds file we use the saveRDS() function. Otherwise we follow the syntax of write_ ending with the file extension. write_csv() - Excel file, extension .csv write_dta() - Stata file, extension .dta write_sas() - SAS file, extension .sas write_sav() - SPSS file, extension .sav As with reading the data, write_csv() comes from the readr package while the other formats are from the haven package. Though the readxl package lets you read .xls and .xlsx files, it does not currently have functions that let you save a file to that type. There are other packages that let you save .xls and .xlsx file but in the interest of keeping the packages we learn to a minimum, I wont include those here. In nearly all cases youll want to save your data as an .rds, .csv, or a .dta file. Fixed-width ASCII files are so primitive that while we may need to load them into R, we should never save data in this format. 4.2.1 R 4.2.1.1 .rda and .rdata For saving an .rda file we must set the parameter file to be the name were saving. For the other types of data they use the parameter path rather than file but it is not necessary to call them explicitly. A parameter in a function is just an option for how the function works. Only for save() do we need to write file = explicitly in the function. save(shootings, file = &quot;data/shootings.rda&quot;) 4.2.1.2 .rds saveRDS(shootings, &quot;data/shootings.rds&quot;) 4.2.2 Excel write_csv(shootings, &quot;data/shootings.csv&quot;) 4.2.3 Stata write_dta(shootings, &quot;data/shootings.dta&quot;) 4.2.4 SAS write_sas(shootings, &quot;data/shootings.sas&quot;) 4.2.5 SPSS write_sav(shootings, &quot;data/shootings.sav&quot;) "],["mise-en-place.html", "5 Mise en place 5.1 Starting with a pencil and paper 5.2 R Projects 5.3 Modular R scripts 5.4 Modular code", " 5 Mise en place If youre familiar with cooking you might have heard the phrase mise en place, which is French for everything in its place. In cooking this concept means that you get everything - ingredients, pots, pans, bowls, utensils, etc. - needed to cook that item ready before you begin cooking. This saves time as you have everything you need in front of you and can just cook from start to finish without stopping to find something. This is also a useful idea in programming, especially when youre programming to conduct research. In this chapter, well cover how to get mise en place for your programming projects. First, well discuss how to think about the project and write out each step that we need to take to complete the project, and each output that we want (such as a graph or table). Well write this out by hand and in plain English (or whichever language you are most comfortable in), before writing any code. Finally, well go over what is, in my opinion, the best way to organize your folders, data, and code. This method is particularly suited for research projects, but please feel free to modify my methods to suit your own needs and preferences. 5.1 Starting with a pencil and paper This may seem counter intuitive, but the best way to start any programming project - and in particular, research project - is to use a pencil and paper. On this paper you should outline every step (broadly speaking, not literally every line of code) that youll take for the project. This is a useful process at the start of a project to step back from the code and think about the overarching goal of the project - and what you need to do to get there. For example, lets think about doing research using data from the US Border Patrol data (Well actually work on this data in Chapter 22). The US Border Patrol releases data as PDFs, which have a table showing the annual number of apprehensions they make. We want to see if a policy change affected apprehensions at the border. On the data side, thatd require scraping and cleaning the PDFs. On the analysis side, wed probably want to do a time-series graph showing apprehensions over time, and run a regression to see if the policy had a significant effect. So here we have four broad categories of work (scraping, cleaning, graphing, running a regression) for a fairly simple policy evaluation. Within each category you can make a number of subcategories of steps you need to do. For example, in the scraping category you might want to add the following subcategories: download the PDFs, see how each table relates to each other, figure out which parts of the tables are actually relevant, etc. We can probably break down these subcategories even further if we want. You essentially want to build a roadmap to follow - you can, of course, deviate from this roadmap if necessary - as you work on the project. This is useful for two reasons. First, writing out what you need to do will often clarify exactly what you need to do. Knowing that youll want a time-series graph, for example, will mean that you need to have your data aggregated into a certain time unit. Knowing this before-hand will save you time as youll have a tangible goal to work towards and dont have to keep stopping during your work to figure out what to do next. And second, from my experience helping people at Penn with R, people - especially new programmers (and myself when I first started learning R, my first programming language) - can get overwhelmed with programming. One major problem they had is they couldnt articulate what they needed to do since they werent familiar enough with R to know the right words. They knew the end goal, and what they had at the start, but couldnt articulate the path from start to finish. Writing out each step in plain language allowed them to know the path - it is simpler to know what steps you need to do to complete a project in plain language than to actually write the code (though this still requires experience to tell you a lot of the minor intermediate steps). Having a game plan helps people avoid being overwhelmed since they could do one step at a time (and feel accomplished at each step). 5.1.1 Tables and graphs One of the biggest challenges I had early in my PhD was figuring out what data was supposed to look like. I mean that literally. My first research project was analyzing if monthly crime in school buildings changed after a new policy was instituted that increased building security. The data I had available was incident-level so one row for every crime at the school, and I needed to convert it to the building-month level. For some reason I just couldnt think of the proper way for my data to appear in the final data set, which prevented me from figuring out what I needed to do. One solution to this - and useful even if you dont have this problem - is to draw out the graphs and tables you want before starting the code. Like writing out the steps for the code, drawing the graph will help you understand exactly how your data needs to look - and thus what code you need to write - for these graphs. Below are two images from a recent project of mine with the tables and graphs that I wanted sketched out. Note that in the image showing my graphs I have crossed out the first graph. These sketches are just preliminary tools to help your work, you arent chained to them. Like any tool, if it is no longer relevant or useful, find something new. For regression result tables especially, sketching these out helps you think about what variables you will need to have to run the regression. For example, you may want to have control variables for demographics in your geographic unit (say, from the US Census). If we continue our example of using the US Border Patrol data, this means that youll also need to grab, clean, and merge Census data to your other data sets. Sketching out the resulting tables and graphs is a good tool to figure out steps that youll need to do for the project but may have not thought of. 5.2 R Projects Weve talked about projects in an abstract sense - that they are research papers or specific data exploration jobs. RStudio provides, a bit confusingly, something called an R Project, which is merely a helpful way to organize folders for a specific project (paper, data exploration, etc.) that you do. When you do a project, I recommend keeping everything for that project in a single folder on your computer. Below is an image showing all of the folders I use for my various R work. As you can see from the file names, each folder is for a separate project, and there is not overlap between them - each project is independent. First, Ill explain how to set up an R Project through RStudio, and why you would want to do it. There are two main reasons to want to use an R Project. First, throughout this book I had you set your working directory so that R knew where to look for a particular file. In R Projects, by default the working directory is in that projects folder. So if you had a file example.csv in your project folder, you wouldnt need to set a working directory since R would already be looking in that folder. This may be a minor time-saving method if youre working alone since youd only need to set the working directory once when not using an R Project. But consider if youre collaborating with three people and youve shared your code. When using an R Project, it just runs. Your collaborators wont need to change the working directory to their own directory. Second, it provides easy access to using the version control software Git, which well talk about in detail in Chapter 9. To make an R Project, start by clicking the File button on the top-left corner of RStudio and then click New Project. This will open up a window that has three options: New Directory, Existing Directory, and Version Control. New Directory says that the project we are making is going to be in a brand-new folder that were (R will do this automatically) going to create. This is the one youll click on in the majority of cases. Existing Directory is for making a folder in an existing folder, which doesnt have too many useful cases. The Version Control is taking a project that someone else has created and downloading it to your computer. Well cover this more in Chapter 9. Once youve clicked New Directory, itll change the window to ask you what type of project you want. The following two figures show all the different types of projects R can make (installing some R packages, such as bookdown, can add more types of projects to this list). R is very versatile and has project types ranging from the standard R Project to books and websites. We just want a standard project so click the New Project button at the top. Now itll have a window that says Create New Project up top. In the Directory name: section you write the name of your R Project. This will be the name of your folder so you want it descriptive enough to understand (and for collaborators to understand) what it is for, without being overly long. Once you have a name you can click the Browse button on the right and go to the folder on your computer where you want to put this folder (ideally, youll put it in a folder that is backed up by something like DropBox). Make sure the Create a git repository checkbox is selected, and well explain why in Chapter 9. Click Create Project and R will make the project folder on your computer and open that project in RStudio. Below are images of a brand new R Project that I made called example that I put in my Desktop folder. The folder is now empty except for two files - .gitignore (which we wont talk about here) and example which is type R Project (and the full name would be example.Rproj). This is a very important file. Note that its name is the same as the R Project name that I made, and the same as the folder name on my computer. This file is essentially a shortcut that you click to open that R Project. It doesnt do anything more than open the R Project, but this is the way youll access the project every time you want to use it. Double-click this and the R Project will open. This RStudio session looks nearly identical to other sessions that weve used - and it is nearly identical. A few key differences can be found in the top-left corner where it says example - RStudio, indicating that were in the example R Project. And then directly below the Console tab it says C:/Users/user/Desktop/example/. This is the working directory of this project. I didnt set it; R just knew where it was. If you move this folder to a new folder (say, the Downloads folder) or if someone else downloads it to their computer, R will automatically change the working directory to the right one. You no longer have to worry about it. 5.2.1 Folders Now that we have the R Project made, we need to start adding some R code and data files to the project so we can get started working. But first, lets talk about proper ways to organize the folder. Ive added a few new folders to the new example R Project as the basic layout of my work process. This is for a research-oriented project so it may not apply in your particular case. Organizing your folders (and as well see below, your code) is important so please play around with different ways to organize and find a way that works well for you. Ive added five folders to the R Project folder: analysis, articles, data, drafts, and R (note that I moved it to the Downloads folder, and if I opened the project RStudio would know where the new working directory was). I tend to do my analysis using Stata (primarily because most of my co-authors use Stata instead of R so this is a way we can both work on the analysis) so in the analysis folder Id keep all of the .do (Stata) files to run the regressions. In articles, I put PDFs of every article I read that I use (or planned to use while reading it) for the paper Im working on in this project. Its good to keep this organized to share with co-authors or just for easy reference after youve read it. It certainly takes time to find good sources for a lot of research, so you dont want to have to search again because youve forgotten which article you had a particular reference from or that was important to your study. While I recommend writing your papers in R Markdown (see Chapter 7), you will need to create drafts of the paper to send to others (e.g. your collaborators or journals). The drafts folder is a good place to keep these versions - some journals require that you submit a Word Document with track-changes for a revise and resubmit so you will need to leave R Markdown occasionally to comply with these rules. The final two important folders are R and data. In the R folder - as you may have guessed - belong the various R scripts that you write during the project. In Section 5.3, well talk in detail as to how to organize these scripts. Inside the data folder I made two subfolders: raw and clean. The raw folder is where youll store the data exactly as you got it (for cases where the data is acquired through webscraping, this isnt necessary). This folder will have, for example, the PDFs that you intend to scrape, or .csv files with crime data in them. It is important to keep this data always unchanged (change it only in R and save the output to a new file) so you can replicate your results from the original data. In the clean folder is that final data output from your work to clean and manipulate (e.g. subset, aggregate) the data. It isnt strictly necessary to even output a final data set - you could just rerun your code from the original data each time, and this is fine if your code is very quick to run - but it is important both for safekeeping and to be able to share with others. If you collaborate with people, youll want to be able to send them the data so they can examine it without having to run all of your code themselves. 5.3 Modular R scripts If you are like many people who start programming, all of your code will be in a single R script. This is fine when youre first getting familiar with R and dont want to go searching for code in places when youre still uncomfortable with the language. As you become more familiar with R - and as your projects get more complex - youll want to start making multiple R scripts in a single project. When youre writing a paper you dont just write one extremely long sentence. You break up ideas into paragraphs and divide groups of paragraphs into larger sections. This is useful in a paper to organize your thoughts and to make it readable for others. Its also useful when working since you know, for example, Section 1 is done, but I still need to finish Section 2 and the last part of Section 3. This way you dont confront working on the entire paper at once. Youll want to follow these lessons in the code you write, with each section of code being its own R script and within a script split up code into particular paragraphs. The end goal should be to have modular R scripts, with each script being independent (or relatively so) and the combination of these parts has all the code for your particular project. This is a bit of an abstract concept so lets use a real example from one of my recent projects. Above is a folder for the code used to analyze data for a paper examining perceptions of outdoor lighting. There are five R scripts in the folder - clean.R, census.R, tables.R, graphs.R, and utils.R - and these are the only ones used for this project.4 Each of these files (utils.R is an exception) has a particular role to play in the analysis of the data. The first file, clean.R is just code that cleans up the survey data and makes it ready to be analyzed and graphed. The census.R file has code that cleans Census data that my co-author and I use to compare our survey sample to the general public. As this is a separate data set than the survey data, I have it in its own R script. tables.R and graphs.R are the code to make descriptive statistics tables and figures for the paper, respectively. I chose these files because they are doing fairly separate tasks, all with the goal of turning raw data into a research paper. This is an example of how I approach making R scripts, not necessarily the best way to do so. Even here, other decisions could be made. For example, I could have put the code from census.R into clean.R since theyre both about cleaning data. While you should try to make separate R scripts for broadly different tasks (regardless of how much code that task requires), you should experiment with how you prefer to separate these scripts, and balance between having one (or a few) super scripts that comprise everything with having too many scripts that do too little - this balance requires experience and experimentation so keep at it! 5.4 Modular code In addition to having separate scripts for each major part of your project, you will want to organize each individual script into relatively modular parts. Whereas each script is like a book chapter, the code inside the script should be like paragraphs, separated into distinct chunks. For example, lets say you have some raw data and want to subset it, change some values (e.g. renaming F to Female, M to Male), and then aggregate to a larger geographic area. This is a three-step process - subset, change, aggregate - so youll want to have three different parts of your R script dedicated to this. Now, if this is a simple process (and it will always depend on the data and what you want to do with it), you may want to have each step in its own Section (as well discuss next). If its relatively simple and takes only a few lines per step, youll likely just want to have a line break between steps and identify your choices in comments. Its hard to give precise rules on how to do this as it really does depend on personal preference - I think having more comments and line breaks early in your R career is best as youre still learning R, and it is good practice to comment your code. You can always alter this balance to suit your preferences as you gain more experience. The goal of making modular code is to avoid having a large amount of code without breaks or comments - thatd be like reading a run-on sentence. Well talk about comments more in Section 6.2.1, but here you should explain your choices (e.g. Subset to only violent crime and property crime) to inform collaborators (other people and yourself in the future who will likely forget what or why you did something), but without writing too much. Generally the rule of thumb is to have comments for why you did something, not explaining what you did. I think a mix of what and why is helpful as its quicker than looking at the code, especially if your code is complex. Like a lot of your work, however, this depends on the project and your audience - if youre working with someone new to R, having more comments explaining what you did is helpful. 5.4.1 Section labels When you have major parts of an R script, you should have something to indicate that this is a distinct section from other parts. RStudio has a handy tool to help make that distinction by creating sections in your R script. Press the keys Control+Shift+R (Command+Shift+R in a Mac), and it will open up a window where you can set a section label. Write the name of the section you want and click OK, and itll add that to where your cursor was in the R Script. You can also do this by simply adding four dashes on the end of a comment. Sections are more than just commented parts of a Script. Note that in the following screenshot, there is both the Section label in the R Script and that same label in a new section of the Source tab on the right. You can get to this section by clicking on the button on the very top right, the one that looks like a bunch of misaligned lines. In here, it shows all the Sections that exist and clicking the Section name will move to the start of that Section in your R Script. If you have a long script (which is generally unadvised but sometimes cant be helped), this is an easy way to find a particular part of your code. 5.4.2 Helper R scripts As part of making code organized, I find it helpful to make an R script in each (or most) of my projects to hold helper functions or objects - and I call this utils.R (utils stands for utility as these are helpful pieces of code for the project). This file should be for code that will be used in multiple R Scripts, so you want them in a single place rather than copying them over in each script where you need them. In utils.R, I keep functions that are either auxiliary (such as code to check data by printing out a set of outputs) or code that is used infrequently (such as loading several files and merging them together at the start of an R script) where I dont want them in the main file. I also include useful objects such as a vector of values that I will use to subset. For example, if I wanted to subset all violent crimes from a data set, I would need to know what crimes in that data are considered violent, put them as strings in a vector, and subset to only rows that match those strings. I could make an object with this vector, such as violent_crimes &lt;- c(\"murder\", \"rape\", \"robbery\", \"assault\"). If you want to run utils.R (or any .R file) in a different R script, you can use the source() function, which makes R run the entire script inputted in the parentheses. Just put the file name (in quotes) in the parentheses and it will run. For example, if we want to run utils.R, wed write source(\"utils.R\"). If that file was in the data folder of our R Project, wed write source(\"data/utils.R\") so R knew to look in the data folder for the file. It isnt necessary to make helper functions like these, but I find them helpful. I recommend that you try them out when you do R projects, but if you dont find them useful please feel free to stop using them. The analysis was done in Stata so there are separate files for that. "],["collaboration.html", "6 Collaboration 6.1 Code review 6.2 Documentation", " 6 Collaboration 6.1 Code review When you collaborate with other people, you will probably each be working on a separate (though related) part of the project and then will combine each part when you are done. Combining your code could be through emailing each other R scripts - and having one person combine everything - or something more formalized such as using Git, which we discuss in Chapter 9. However you decide to do this, it is important to use a process to review your collaborators code (and have them review yours) to check for mistakes.5 This is a similar process to having a colleague read a paper draft before submitting it. Code review is a useful technique for reducing the number of mistakes as it is a check on the work before using the code for real. Code review generally involves having one person who writes the code send it to another person who checks the code for any potential mistakes or issues. This check involves ensuring that the code meets the specified style (this is discussed further in Section 6.1.1) and that there are no bugs (which are errors in the code). For the person having their code reviewed, having comments explaining the what and why of the code (discussed more in Section 6.2.1) will help the reviewer quickly go through the code. The code should also be relatively short, comprised of a specific R script (or related scripts) and no more than a few hundred lines of code. This is because as code gets more interrelated and complex, it is harder for someone unfamiliar with the code to understand it and see any issues. That means that a reviewer for long code is more likely to miss issues and take longer to review. Reviewing shorter code, even if that means reviewing more often, is often far more efficient for both the reviewer and reviewee and catches more issues. In cases where you have unit tests (which are discussed in Chapter 8) written for the code, these tests are an automated form of code review as they too check for mistakes. To save peoples time, you should avoid sending the code for review until it passes all unit tests. However, if youre stuck and cant get certain tests to pass, working with someone else to solve the problem is often faster than doing so yourself because then you have an outside perspective who may see something that you missed. For code review to be most efficient, I recommend developing some rules with your collaborators to specify how and when code review is done. For example, you should determine who reviews certain peoples code (ideally with senior people reviewing junior peoples code) and how often it is done. I believe that doing code reviews relatively frequently (i.e. after a working draft of some code is ready) is useful as you can catch issues early and not waste anyones (especially the person writing the code) time. However, having hard time limits is probably ill-advised as sometimes writing certain code takes far longer than expected and reviewing an unfinished (and potentially far from finished) bunch of code is not efficient for anyone. When someone is reading a draft of your research paper, they are generally looking for whether it is correct (i.e. your methods are right, the lit review is thorough, etc.) and how well it flows. Code review is the same. While the primary goal is finding errors, an important aspect is to ensure that it is readable (i.e. proper spacing, how names are written) and consistent across everyones code in that group. More formally, ensuring that everyones code is readable and consistent is having people follow a style guideline. 6.1.1 Style guidelines An important part of reviewing peoples code is ensuring that everyone is following the same style guidelines when it comes to writing code. Style guidelines are the grammar rules of writing code. They dictate (or encourage) certain style choices, such as whether object names are lowercase, whether they include punctuation, and even when to put long code on a new line. This is equivalent to making sure that people writing in plain language put punctuation and capitalization in the expected place. While you can read !SomEThiNg WrITen. LiKE thIs, it is easier to understand when it follows adopted and accepted rules. The important thing here is to be consistent. Consistency makes code much easier to read and helps make code written by multiple people more interchangeable. This book follows the tidyverse style guide, which is one that many R programmers follow, but the exact style you choose is relatively unimportant (choosing more common styles helps when your code may be used by people out of your organization). Feel free to adopt an already-made style guide, make any modifications to suit your preferences, or to create an entirely new one yourself. As long as people follow the same format, youll be able to spend more time on the code, and less time trying to understand it. 6.2 Documentation An important, though occasionally tedious, part of writing code is documenting your work. Well talk about documentation in two ways, through comments, which focus on specific parts of code, and vignettes, which document the project more broadly. 6.2.1 Comments In Section 2.1 we introduced comments, which are essentially notes about the code that you include in an R script (by starting a line with the pound key #) that isnt run. They are just comments to yourself or anyone else reading the code to explain what that code does and why it is there. As is often repeated in explaining the benefit of comments, the main collaborator you will have is yourself in the future.6 You dont need to comment on every single line of code - and doing so would just make it hard to read - but you should comment on important things or chunks of code (i.e. several lines of code that all are for the same purpose). If you write a function, youll want at least a brief comment explaining what it does and what the inputs and parameters do. Writing comments is not as fun as writing code. Stopping to write a comment on something that seems obvious at the time (after-all, you figured out how to do something you wanted to do and likely were focusing on) interrupts the flow of writing code and slows down your work. And when you have looming deadlines and multiple projects that youre working on, spending the time writing good comments may seem like a bad use of time as the payoff is only in the future. However, the benefits far outweigh the cost. This is true for two reasons. First, when youre collaborating with others, it is much quicker to have text explaining the code than to walk through the code with them (or to have them try to figure it out themselves).7 As you work with more people, comments become increasingly important. Writing good comments is also time-efficient when considering that in many cases when you do research you will have to return to the project in the future. This is best shown when considering a research project that leads to a journal article. For many papers, even if you are fantastically productive and can work nonstop at it without forgetting any decisions, at a certain point youll need to finish and submit it to a journal. Journal reviews can often take three to six months so at that point youll likely have forgotten many of the (seemingly obvious) decisions you made in the course of the project.8 Having comments explaining why you made a certain decision (such as including or excluding certain crime types from your analysis) can be a huge time-saver when addressing reviewer concerns - you will know why each decision was made and wont have to try to figure out the why. This is particularly important when you have to defend a decision in which there is no obvious choice and you want to know your thought process at the time you wrote the code and were immersed in the issues of the data. A lot of data decisions are reasonable at the time based on the quirks of the data but can appear to make no sense if you arent familiar with the data - comments can remind you of the quirkiness and how you handled it. 6.2.2 Vignettes Vignettes are essentially a document that explains how to do something with the code you have written. This is common when someone has written an R package and they want to explain in detail important functions from the package. You can think of chapters of this book as vignettes covering particular topics - PDF scraping, webscraping, regular expressions, etc. To make a vignette, you can make an R Markdown file (for more information on R Markdown please see Chapter 7) detailing that topic. Since the text you write is included in the document, these files are basically normal R scripts with extensive comments written in plain language. Often, these comments are more formal than what youd write in an R script as they are written as complete sentences or paragraphs and walk through comprehensive ideas rather than focus on discrete chunks of code. One increasingly prominent method of using R for research is to do everything in an R Markdown file. This allows you to explain your approach - including context on why you did something - and each step you took in plain language in the text of the R Markdown file while still including the code directly in the file. Whether you include the code in the output (e.g. a PDF or Word Document), or just the result of the code (e.g. a graph or table), depends on your audience and how far along you are in the project. If this is for a presentation to update collaborators, for example, it is useful to include the code as they may notice an issue or give advice based on the code. Including code can also teach your audience something new (Ive certainly learned a lot by watching people present using code I wasnt familiar with). If the document is for an audience unfamiliar with R (or programming more generally), or where time to present is limited, you probably wont want to include code. Whether you do your work in an R script or in an R Markdown file is up to you. If you intend to write up a report anyway, having everything written up in the R Markdown file as you write your code can save you time as youre merging the code and the writing process. However, this loses some nice features in R such as unit tests, which we will discuss in detail in Chapter 8. It also depends on how complex your project is. If you have code that is hundreds of lines long and spans multiple R scripts, putting it all into a single R Markdown file is unfeasible. In this case itd be better to run the code in the R scripts and use the R Markdown file just to present results. If your collaborator does not know R, they should read this book. I recently worked on a follow-up paper to one I had done a year ago. For some reason, past me decided to name some functions based on the authors of a paper that created that particular method, and didnt leave comments explaining what the code did or why. Past me caused a lot of problems for current me. Please comment your code! This is one of the main reasons I wrote this book. After a few years of helping Penn students with the same questions, I decided to write out guides to those topics. If youre like me and on your seventh rejection for a particular paper, three to six months may be optimistic. "],["r-markdown.html", "7 R Markdown 7.1 Code 7.2 Inline Code 7.3 Tables 7.4 Footnotes 7.5 Citation 7.6 Spell check 7.7 Making the output file", " 7 R Markdown When conducting research your end product is usually a Word Document or a PDF which reports on the research youve done, often including several graphs or tables. In many cases people do the data work in R, producing the graphs or numbers for the table, and then write up the results in Word or LaTeX. While this is a good system, there are significant drawbacks, mainly that if you change the graph or table you need to change it in R and change it in the report. If you only do this rarely it isnt much of a problem. However, doing so many times can increase both the amount of work and the likelihood of an error occurring from forgetting to change something or changing it incorrectly. We can avoid this issue by using R Markdown, Rs way of writing a document and incorporating R code within. This chapter will only briefly introduce R Markdown, for a comprehensive guide please see this excellent book. For a cheat sheet on R Markdown see here. What R Markdown does is let you type exactly as you would in Microsoft Word and insert the code to make the table or graph in the places you want it. If you change the code, the document will have the up-to-date result already, reducing your workload. There is some additional formatting you have to do when using R Markdown but it is minimal and is well-worth the return on the effort. This book, for example, was made entirely using R Markdown. I include this chapter early in the book - and likely before you are really comfortable with using R - since some new R programmers do like to do all of their work using this method. In my experience this is relatively rare, but I still wanted to make the info available for those that do. For new programmers I recommend reading this chapter so you understand R Markdown, but still use normal R scripts when writing code - dont use R Markdown for everything. Focus on learning how to write good code before adding the complexity of writing full documents using R Markdown. To open up an R Markdown file click File from the top menu, then New File, and then R Markdown From here itll open up a window where you select the title, author, and type of output. You can always change all three of these selections right in the R Markdown file after making your selection here. Selecting PDF may require you to download additional software to get it to output - some operating systems may already have the software installed. For a nice guide to making PDFs with R Markdown, see here. When you click OK, it will open a new R Markdown file that is already populated with example text and code. You can delete this entirely or modify it as needed. When you output that file as a PDF it will look like the image below. R converted the file into a PDF, running the code and using the formatting specified. In an R Script a # means that the line is a comment. In an R Markdown file, the # signifies that the line is a section header. There are 6 possible headers, made by combining the # together - a # is the largest header while ###### is the smallest header. As with comments, they must be at the beginning of a line. The word Knit was surrounded by two asterisks * in the R Markdown file and became bold in the PDF because that is how R Markdown sets bolding - to make something italics using a single asterisks like this. If youre interested in more advanced formatting please see the book or cheat sheet linked earlier. Other than the section headers, most of what you do in R Markdown is exactly the same as in Word. You can write text as you would normally and it will look exactly as you write it. 7.1 Code The reason R Markdown is so useful is because you can include code output in the file. In R Markdown we write code in what is called a code chunk. These are simply areas in the document which R knows it should evaluate as R code. You can see three of them in the example - at lines 8-9 setting a default for the code, lines 18-20 to run the summary() function on the cars data (a data set built into R), and lines 26-28 (and cut off in the screenshot) to make a plot of the data set pressure (another data set built into R). To make a chunk click Insert near the top right, then R. It will then make an empty code chunk where your cursor is. Notice the three ` at the top and bottom of the chunk. Dont touch these! They tell R that anything in it is a code chunk (i.e. that R should run the code). Inside the squiggly brackets {} are instructions about how the code is outputted. Here you can specify, among other things if the code will be outputted or just the output itself, captions for tables or graphs, and formatting for the output. Include all of these options after the r in the squiggly brackets. Multiple options must be separated by a comma (just like options in normal R functions). If you do not have the R Markdown file in the same folder as your data, youll need to set the working directory in a chunk before reading the data (you do so exactly like you would in an R Script). However, once a working directory is set, or the data is read in, it applies for all following chunks. You will also need to run any packages (using library()) to use them in a chunk. It is good form to set your working directory, load any data, and load any packages you need in the first chunk to make it easier to keep track of what youre using. 7.1.1 Hiding code in the output When youre making a report for a general audience you generally only want to show the output (e.g. a graph or table), not the code that you used. At early stages in writing the report or when youre collaborating with someone who wants to see your code, it is useful to include the code in the R Markdown output. If you look at the second code chunk in the screenshot (lines 18-20) it includes the function summary(cars) as the code and the options {r cars} (the cars simply names the code chunk cars for if you want to reference the chunk - or its output if a table or graph - later, but does not change the code chunks behavior). In the output it shows both the code it used and the output of the code. This is because by default a code chunk shows both. To set it to only show the output, we need to set the parameter echo to FALSE inside of the {}. In the third code chunk (lines 26-28), that parameter is set to false as it is {r pressure, echo=FALSE}. In the output it only shows the graph, not the code that was used. 7.2 Inline Code You can also include R code directly in the text of your document and it will return the output of that code. To use it, you need to setup an inline code chunk using the tick mark followed by the lowercase letter R, the code you want to use, and then end it using another tick mark. This is called using inline code. When you have a table or visualization to output, this isnt the proper method, it is best for small pieces of text to add to your document. This is most useful for when you want to include some descriptive info, such as the number of respondents to a survey or the mean of some variable, in the text of your document. Inline code will only present the output of the code and doesnt show the code itself. Below is an example of inline code - see the image below that for what it looks like with the code. The data set mtcars has 32 rows and 11 columns. The mean of the mpg column is 20.090625. 7.3 Tables There are a number of packages that make nice tables in R Markdown. We will use the knitr package for this example. The easiest way to make a table in R Markdown is to make a data.frame with all the data (and column names) you want and then show that data.frame (there are also packages that can make tables from regression output though that wont be covered in this book). For this example we will subset (which well cover in Chapter 10) the mtcars data (which is included in R) to just the first 5 rows and columns. The kable function from the knitr package will then make a nice looking table. With kable you can add the caption directly in the kable() function. The option echo in our code chunk is not set to FALSE here so you can see the code. library(knitr) # Warning: package &#39;knitr&#39; was built under R version 4.1.3 mtcars_small &lt;- mtcars[1:5, 1:5] kable(mtcars_small, caption = &quot;This is an example table caption&quot;) Table 7.1: This is an example table caption mpg cyl disp hp drat Mazda RX4 21.0 6 160 110 3.90 Mazda RX4 Wag 21.0 6 160 110 3.90 Datsun 710 22.8 4 108 93 3.85 Hornet 4 Drive 21.4 6 258 110 3.08 Hornet Sportabout 18.7 8 360 175 3.15 For another package to make very nice looking tables, see this guide to the kableExtra package. 7.4 Footnotes In your writing, youll often have sentences that you want to include but are auxiliary to your main point (or, frequently, to include links to specific resources such as a website where you got data from). In these cases youll want to include that info as a footnote, which is a section at the bottom of the page for this kind of information. To create a footnote in R Markdown, you use the carrot ^ followed immediately by square brackets []. Put the text inside of the [] and itll print that at the bottom of the page. Code for a footnote will look like this: ^[This sentence will be printed as a footnote.]. In cases where you have a very long footnote it may extend to the next page and will be again at the bottom of the page. Look down at the bottom of this page to see the footnote (in a PDF or Word Doc, the footnote will be on the page you create it on, however since websites are just one long page without breaks, this footnote is at the very bottom of this entire page).9 When you use a footnote, youll usually put it immediately after the punctuation of the sentence it should be after. Note that footnotes are numbered so you can identify them. Theres a blue superscript 1 where we make the first footnote. If we make another footnote, itll be numbered sequentially, such that the next one is 2, the next is 3, etc. If youre familiar with LaTeX you can use LaTeX code such as \\footnote{} where the text goes inside the {}. But note that citations (which well learn in Section 7.5) wont work properly in the footnote if made this way. You can use LaTeX code - and use LaTeX packages - in R Markdown if youd like and itll operate (in most cases) like normal LaTeX. 7.5 Citation In academic research you will need to cite the papers that you are referencing. R Markdown has a built-in way to cite papers, though its a bit of a process to get everything setup. Youll need the citation data in BibTeX format and well walk through the steps from finding an article that you want to cite to citing it in your R Markdown file. First, a brief overview of what kinds of citations you can use. There are two types of citations you can use, in-text and parenthetical. Youll use in-text citations when you want to have the author names be in the text, and parenthetical citations when you want everything to be in parentheses. Note, there may be other ways to get the citations in the right format; Im just showing you one way to do so. For this example, well use the article Using NIBRS data to analyze violent crime by Brian Reaves that was published in 1993. Well walk through the process from finding the article on Google Scholar to citing it in your paper. First, from Google Scholar well search for the article title. This returns all articles that meet your search criteria. Since were searching for a specific article title, we only get one result. The result shows some basic info about the article - title, date, name, abstract. Below the abstract are some important things. First, and circled in blue in the above photo, is a link that looks like quotation marks. This is what well click on to get to the BibTeX citation. While not necessary for citation, the next two links may come in handy during your research. Cited by 31 means that 31 published (in some format that Google can locate, not necessarily peer-reviewed) articles have cited this article. If you click the link itll open up a Google Scholar page with all of these articles. This is a good way to find relevant literature. Clicking Related articles does the same thing but with articles that Google Scholar deems similar, not necessarily articles linking to the one youre looking up. But back to the quotes link circled in blue. Click this and itll make a popup, shown below, of ways to cite this article is various formats. Well have R Markdown automatically generate the citation in the format we want so we dont need to worry about this. Instead, click the BibTeX link at the bottom left. When you click it, itll open up a new page with that articles citation in BibTeX form, as shown below. This basically is just a way to tell a computer how to cite it properly. Each part of the citation - author, year, title, etc. - is its own piece. Take a close look at the section immediately after the first squiggly bracket, reaves1993using. This is how youll identify the article in R Markdown so R knows which article to cite. Its essentially the citations name. Its created automatically by combining the author name (first author if there are more than one author, publication year, and part of the title). You can change it to whatever you want it to be called. Note at the end of the publisher section are the characters ~. This looks like a mistake made by Google Scholar so well need to delete that so it isnt included in a paper we use this citation in. When using Google Scholar, youll occasionally find issues like this which youll need to fix manually - a bigger issue is apostrophes or other punctuation may copy over from Google Scholar weirdly (meaning that it copies as a character that your computer, and thus R Markdown, doesnt understand) and needs to be rewritten so R Markdown will run. You can rewrite it by just deleting the punctuation and typing it using your keyboard. This isnt always an issue so dont worry about it unless you get an error with the citations when outputting your document. Below is the citation included in my .bib file, and the start of another citation also included in the file. A .bib file is basically a text file that programs can read to get citation info. Youll have all of your citations (in the BibTeX format) in this one file. To make a .bib file you can open up a text document, such as through the Notepad app in Windows, and paste the BibTeX that youve copied from Google Scholar. Save this file as a .bib extension (by renaming it filename.bib) and youll have a usable .bib file. Note that I have the word NIBRS surrounded by squiggly brackets {}. That is because by default R Markdown (and other citation generators such as Overleaf) will only capitalize the first letter of the title or the first letter following a colon. Since NIBRS is an abbreviation and should be capitalized, I put it in the {} to force it to remain capitalized. This is often a problem with abbreviations or country names (such as United States) in the paper title. Since all citations you use for a project should be in a single .bib file, you can see the start of another article citation below the Reaves citation. To use citations from your .bib file, add bibliography: references_file_name.bib to the head of your R Markdown file. If your .bib file isnt in the R Markdown files working directory, as my example below is not, youll need to include the path in the file name. Now that we have the citation in BibTeX format, have put it in our .bib file, and have told R Markdown where to look for that file, we are ready to finally cite that article. To use a citation we simply put the @ sign in front of the citation name (in our case reaves1993using) so we would write @reaves1993using. This will give us an in-text citation, with the author name in the text and the year in parentheses. Adding a - right in front of the @ will cause the citation to show just the year, not the authors name. Youll usually want to use this if youve already named the author earlier in the sentence. Generally we will want parenthetical citations, with both the authors and the year in parentheses. To do this, we put the citation inside of square brackets like this [@reaves1993using]. If were citing multiple articles, we separate each citation using a semicolon [@reaves1993using; @jain2000recruitment]. Heres what the results look like when citing that Reaves article, see the image below for what this looks like just as code. (Reaves 1993) Reaves (1993) -Reaves (1993) (1993) (Reaves 1993; Jain, Singh, and Agocs 2000) If you use a citation that isnt in your .bib file, R Markdown will show a question mark, indicating that you made some mistake. (wrongCitation?) When you use citations, R will automatically put the reference section at the very end of the document. Two LaTeX commands may be useful here. \\clearpage makes a new page so your reference section isnt on the same page as the conclusion. \\singlespace makes the reference section single spaced if your document is set to be double spaced. Put these commands at the very end of your document so they only apply to the reference page. You dont need to do anything other than write them (for easier reading, make them on separate lines) at the end of the R Markdown file. If you want to make the references go in another part of the paper (e.g. after tables and figures), just put this code at the place in the paper where you want to reference section to go: &lt;div id=\"refs\"&gt;&lt;/div&gt;. 7.6 Spell check R Markdown does have a built-in spell checker (the ABC above a check mark symbol to the left of the Knit button) but it isnt that great. I recommend that you export to Word (or open up the PDF in Word if you prefer using PDFs) and using Words superior spell checker. 7.7 Making the output file To create the Word or PDF output click Knit and it will create the output in the format set in the very top. To change this format click the white down-arrow directly to the right of Knit and it will drop-down a menu with output options. Click the option you want and it will output it in that format and change that to the new default. Sometimes it takes a while for it to output, so be patient. References "],["tests.html", "8 Testing your code 8.1 Why test your code? 8.2 Unit tests 8.3 Test-driven development (TDD)", " 8 Testing your code This chapter covers how to write code that tests other code. Its especially useful when you write complex functions but is also useful for work such as PDF scraping or webscraping where you know the right answer (by looking at the PDF or webpage yourself) and want to be sure your scraping code did the scrape correctly. However, in most cases when programming for research you wont formally test your code - though you should be checking if everything makes sense and rereading your code to look out for errors (such as typos or using the wrong data). If youve never programmed before, I recommend that you skip this chapter entirely (or read it but dont feel pressure to understand everything) and return to it after youve finished the rest of the book. 8.1 Why test your code? As you write code, you will inevitably make mistakes. There are two main types of mistakes with coding - those that prevent code from working (i.e. give you an error message and dont run the code) and those that run the code but give you the wrong result. Of these, the first is probably more frustrating as R tends to give fairly unhelpful error messages and youll feel you hit a roadblock since R just isnt working right. However, the second issue - code is wrong but doesnt tell you its wrong! - is far more dangerous. This is especially true for research projects. Lets use examining whether a policy affected murder as an example. In the example data set below, we have two years of data for both murder and theft, and well say that the policy changed at the start of the second year. If we want to see if murder changed from 2000 to 2001, we could (overly simply) see if the number of murders in 2001 was different from the number in 2000. And since the data also has theft, wed want to subset to murder first. example_data &lt;- data.frame( year = c(2000, 2000, 2001, 2001), crime_type = c(&quot;murder&quot;, &quot;theft&quot;, &quot;murder&quot;, &quot;theft&quot;), crime_count = c(100, 100, 200, 50) ) example_data # year crime_type crime_count # 1 2000 murder 100 # 2 2000 theft 100 # 3 2001 murder 200 # 4 2001 theft 50 To see if murder changed, we can subset to the rows where the crime is murder, and then print out the year and crime_count columns to see if there is a change. So our code will be example_data[example_data$crime_type == \"murder\", c(\"year\", \"crime_count\")]. Below Ive accidentally only put one = instead of two, this will give us an error and not give any other results. Helpfully, the error message tells us that theres an error with the = sign, though not what that exact error is. example_data[example_data$crime_type = &quot;murder&quot;, c(&quot;year&quot;, &quot;crime_count&quot;)] # Error: &lt;text&gt;:1:38: unexpected &#39;=&#39; # 1: example_data[example_data$crime_type = # ^ Now Ive made a different mistake. Here, instead of ==, Ive written != which is the opposite of what we want - itll return all rows that do not equal murder. Now it looks like the policy cut murder in half when in actuality the policy doubled murders! Since we dont print out the type of crime in the output, we wouldnt catch this from the output alone. example_data[example_data$crime_type != &quot;murder&quot;, c(&quot;year&quot;, &quot;crime_count&quot;)] # year crime_count # 2 2000 100 # 4 2001 50 You may think this is a silly example that is unrealistic. And it is to a degree, its just one line of code that were using to evaluate an entire policy. Now think about how you would actually evaluate a policy using data that youre familiar with. Now the code is going to be much more complex. Your code may be hundreds of lines long, deal with multiple data sets that must be joined together, and involve a number of relative subjective (though must be defensible) decisions as to how to deal with your data (e.g. what crimes constitute violent crime, what time unit to analyze), and some of the code may be written by other people who you are collaborating with. The increased complexity with a real analysis increases the likelihood that errors will occur - and even small issues such as an incorrect subset can have large impacts on your results. So, how do we properly test our code? There are two main methods that Ill refer to as informally testing and formally testing. The formal method will be using something called unit tests that well discuss in the next part of this chapter. Informal methods are what youve likely been doing already. Essentially, just looking at your data and trying to see if it looks right. This includes stuff like printing summary statistics (using summary()) of important variables and making simple graphs to look at the data. If something is wrong, exploring the data is a fairly good way to discover it. For example, if you are looking at arson data from the FBI, you may find (as this is actually in the data) some cities with millions of car arsons in a month. This is clearly wrong so you know theres an issue - in this case, an issue with not subsetting out obvious outliers. Knowledge about the topic and the data are also important in this approach. If you are familiar with a given topic and your results are similar to that of past studies, thats a good sign that you did things right.10 You can also take this kind of approach when testing functions - which ideally are the way you write code. For example, if you have a function that takes a number and returns that number + 2, you can test it by checking a few cases. If you input 2, you expect 4. If you input -2, you expect 0. Do this a few times and you can be more confident that the function works properly. Now imagine a function thats more complex - one that calls a different function and uses the result of that function. If you change the underlying function, youll need to check both that function and the function that calls it. As you have more intertwined pieces in your code, this gets more and more complex. It also takes a lot more time as youll have a lot of code that only checks a function and will have to run it line by line to see if theres an issue. At this point, relying on informal methods becomes unfeasible and youll want to use unit tests, a formal way to test your code. Note, however, that this is far better suited for checking functions than for checking data, though it is possible to some degree. Well discuss formally testing data in Section 8.2.3.1. 8.2 Unit tests A unit test is simply a conditional statement where you have some input, usually a function with some parameters set, and state what you expect the result to be. You are saying I expect that if I do X, I will get Y. And if you get a result other than what you expected, R will tell you. In R, you can make a number of unit tests and have R run them all at once and inform you of which ones failed. Each unit test is just a function in R that is specifically for checking whether other functions - or other code or data - are correct. They operate just like a normal function. To use unit tests, well use the R package testthat which has a number of functions that make unit testing easier, and well use some keyboard shortcuts in RStudio that also improve the ease of testing. Please note that these shortcuts will only work if youre working in an R Package, a normal R Project wont work. An R Package is a special type of R Project, which you can make by following the steps in Section 5.2 and choosing R Package instead of New Project in the Project Type panel. An R Package is essentially an R Project with the goal of creating a package in R, though theres no requirement that we actually make a package. We can treat it as a normal R Project but use the added testing tools. If you dont have testthat installed, do so using install.packages(\"testthat\"). For more information on the package, please see the packages website. install.packages(&quot;testthat&quot;) library(testthat) # Warning: package &#39;testthat&#39; was built under R version 4.1.3 # # Attaching package: &#39;testthat&#39; # The following object is masked from &#39;package:tidyr&#39;: # # matches # The following object is masked from &#39;package:dplyr&#39;: # # matches # The following objects are masked from &#39;package:readr&#39;: # # edition_get, local_edition In testthat, every function follows the same expect_ format where a type of conditional statement follows the _. For example, expect_equal() checks if two values are equal, expect_named() checks if the name of a data set is correct, and expect_silent() makes sure that the code thats run doesnt return any warnings, messages, or errors. To use this technique for our above example of the function that adds 2 to an inputted number - which well call add_2() - we can use some expect_equal() functions. If we input 2, we expect 4. So wed write expect_that(add_2(2), 4). add_2 &lt;- function(number) { return(number + 2) } expect_equal(add_2(2), 4) Above is the code that makes the add_2() function and one unit test checking it. It doesnt output anything. That is good. When a test passes, there is no information; when it fails, the function will output a message that it failed. Below is another test, this one intentionally wrong to show what happens when a test fails. expect_equal(add_2(2), 5) # Error: add_2(2) not equal to 5. # 1/1 mismatches # [1] 4 - 5 == -1 It gives an error, telling us that the result of the add_2(2) function does not equal 5. Helpfully, it also shows us how much of a difference there is between what we expected and what we got. Note that it says 1/1 mismatches. That says that all of the expected values - we only expect one value here - are incorrect. If we expect more than one result, such as if we expect a function to return a vector, it will check each value and say exactly which ones (in the order we have the resulting vector) are incorrect. This is helpful when diagnosing exactly which part failed. There are a few different ways to run the unit tests. First, you can run them like a normal R script by running each line directly. This is fairly inefficient and loses some of the benefits built into RStudio for testing. Though in this case you do not need to be using an R Package, you can just use a normal R Project or just use normal R and the code will work fine. The next way is to use the test_dir() function from the testthat package where you enter the folder directory in the parentheses and it runs every test file in that folder. Its easier to simply use the test_path() function which inserts the correct folder, assuming that you didnt move folders around after use_test() (which well discuss below) created them. So youd write test_dir(test_path()) and it would run all of your tests. This also prints out a nice summary of the results for all of your tests combined, showing the number of tests that passed, that failed (i.e. didnt pass), that returned warnings, and that were skipped (you can force R to skip some tests which is useful when you change one part of the code and know those tests will fail but still want to test other parts). You will also need to run all the functions or load all of the data that the tests check, otherwise youll get an error since R doesnt implicitly know that the functions/data exist. You can run this the normal way by highlighting and running the function (or the code to load data) in your R script or use the shortcut Control+Shift+L (Command+Shift+L on a Mac, the L stands for Load) which will run every R file in your project (and every line of each file). The final way is to use the keyboard shortcut Control+Shift+T (Command+Shift+T on a Mac, the T stands for Test) which will load all of the files in your folder and then run all of the tests. Its a quicker way of doing the above method. However, this shortcut only works when using an R Package, not a normal R Project. 8.2.1 Modular test scripts Before getting into exactly how to write a unit test correctly, well talk about organizing each testing file. As with your normal R script, you can have separate testing scripts (a testing script is a normal R script which people use specifically for testing code but doesnt actually function any different) for each major part of the code that youre testing.11 As with the R scripts for your code, this is simply a way to organize your work, and doesnt affect the testing. Below is an image showing the files I use to test the US Border Patrol scrapers. I have one file per PDF that I scraped. Note where the folder depicted above is located. Its in a folder called testthat in the tests folder in the main project folder that I called borderpatrol. Well use a helpful function from the usethis package to organize our test files and generate them automatically. If you havent installed this package already, do so using install.packages(\"usethis\") and then load it with library(usethis). 12 You can use the function use_test() from the usethis package to create a test file inside your R Project. This will automatically create the necessary file and folders (if not created already) so you dont have to do any more work. Run this function by putting the name of the test file you want to create (in quotes) in the parentheses. It will open the test file in the Source panel (shown in the top left). In the example shown below, I wrote use_test(\"test\") to make a new file called test. In the Source panel, the file is called test-test.R, which is just because usethis will automatically add test- to the name of any test file name you make. use_test() will also generate an example of a test, which you can modify (or delete entirely) to suit your own needs. The first file in the testthat folder is called setup.R, which is a file that will automatically run first when you run a test script through R or using RStudios keyboard shortcut. This file is where you run some code that is used during the tests. In my setup.R file I made several vectors, which I use during the tests to subset the data. You wont always need to have a setup.R file, but its useful when you want to run the same code beforehand for multiple different test scripts. 8.2.2 How to write unit tests Well start by looking at the default test example made when using use_test() to understand the organization of a test file before getting into an example of actual tests. In the image below, there are really two pieces. First, we have the actual test on line 2 - expect_equal(2 * 2, 4). This is saying, I expect 2 * 2 to equal 4, and R will check if that is true. All of your tests will be in this format, just for a specific result from a specific input. Now lets look at the code surrounding that line - test_that(\"multiplication works\", {}) where the expect_equal() line goes inside the {}. The test_that code is basically a form of organization within a test file to group similar tests together. In this case it is grouping all of the tests that check if multiplication works, though we only have one test written. Below Ive added three new tests to this multiplication works testing group. To run this code, I can either run each expect_equal() individually (remember to run library(testthat) beforehand or it wont run) or run the entire test_that() group at once. You can do this by either highlighting it all and running it or selecting either the top or bottom line (which has the squiggly brackets) and running that line - the entire thing will run. The benefit of this is that when you run all the tests you write (and youll often have many test groups and more individual tests than shown here), if a test in a group fails, it will tell you exactly which group failed (based on the name of the group which you specify - here, multiplication works). Note that the final test in this example is incorrect, and in the Console panel on the right it says that Test failed: multiplication works to tell you where the test failed. The test groups arent necessary, but they make it easier to organize your tests. As an example of actual tests, well go over the tests that I wrote when I first scraped the US Border Patrol data that we will scrape in Chapter 22. This test file is organized almost identically to the example one shown above. At the start I have some code that loads the data that I will test - this isnt in the setup file since the code is for this specific test script (though it could be in the setup.R file and the results would be the same). While most tests check the result of functions, here I am checking the data that is outputted by the function, and not rerunning the function for each test. I do this because the function that scrapes the PDF is relatively slow to run and I have many tests, but putting the function that gets the data in the test directly will give the exact same results. Then there are several test_that() groups with some expect_equal() tests inside each. Since these tests are checking if the code is scraping the PDFs correctly, I determine the expected result by looking at the PDFs and writing down what the values should be (be careful, this must be done by hand but that can mean you mistype - so double-check your work!). Well use the test on lines 21-22 as an example. Here I am asking if the values in the cocaine_pounds column, for rows where the sector is coastal border are equal to the values c(6843, 1701, 3169, 1288, 6884, 20, 709, 5962, 989). If they are, then the scraping was correct (at least for this part of the PDF) and the code worked. In this case I checked every value that meets the two conditions, but thats just because there were relatively few values. If I had many values that meet those conditions (i.e. many rows of data in that column), I would just check a small number of them. 8.2.3 What to test Now that weve gone over how to make unit tests, lets talk about what to test. When testing functions, you generally want to test every possible parameter in the function, and a variety of inputs. In particular, try to think of ways that the function could be used incorrectly and write a test to catch that. For example, our add_2() function will fail if a string (e.g. 2) was inputted instead of a number, so youll want to add a test for that. Youll also want to make sure that inputting something other than simply a single number, such as a vector of numbers, works as expected. Basically, you want to be thorough and cover all of your bases. Writing unit tests is one of the most time-efficient things you can do since it helps you avoid making costly (in time and in getting wrong results) mistakes. But dont spend too much time writing tests. If youve tested that add_2(2) equals 4, no need to test that add_2(3) equals five since youre essentially testing the exact same thing. And consider your audience (even if that is only you). If you know that add_2() will only be used by people who know better than to input a string, theres no need to test for that. In general, I think its always better to have more tests than fewer, but consider whether writing that test is a good use of your time. This is something that youll learn with experience so its better to have too many tests when youre first using R than too few. 8.2.3.1 Tests for research projects When you use R for a research project, youll usually take data that someone else collected, or scrape it yourself, do some work to clean this data (e.g. subset or aggregate the data, standardize values) and then run a regression on it. In these cases there are relatively few opportunities to use unit tests to check your code. Indeed, the best checks are often content knowledge about the data and examining the results of your analysis to see if it makes sense and fits prior literature. While testing is most commonly used for functions, you can use it to test data. Writing tests for research data is best if your code is scraping the data (webscrape or PDF scrape) and you want to verify that it is correct, or if you expect the data to change and want to ensure that it is still correct (while exact values will change, you can check broad categories such as whether certain groups are included). For example, if you know that you only want to look at a certain state, you can write a test that expects the only state in the data to be the one youre analyzing. This way, if you add more data, such as a new release of that data set, the test will catch if theres any other state that you may have forgotten to remove after adding the new data. If youre sure that you will only use a particular data set that never changes, youre better off just writing code in your main R script (or a specific script for checking the data) to do these checks rather than dedicated tests. 8.2.3.2 Tests for data collection Our example in this chapter was tests for a data collection process - in our case, PDF scraping - so weve already seen how to test code for gathering data. Well still talk briefly here about what kind of tests - and how many tests - you will want for this type of code. In normal tests, you dont want to test the exact same thing multiple times (for example, if you test that 2 + 2 = 4, you dont need to test that 2 + 3 = 5). This is different when it comes to testing code that collects data from a source, such as through PDF scraping or webscraping. When testing data collection code, you want to be far more thorough, retesting something in multiple ways. This is because small differences in the data you are scraping may affect the code at different parts of the scrape. For example, imagine a PDF with ten pages and a single table on each page. On the first page there five columns but on the next nine pages there are six columns. If you test only data from the first page youll miss all of the pages where there are six columns instead of five - and where your code to scrape it is probably wrong. Ive often experienced PDF or webscraping where some parts of the data are just weird and cause the code to scrape it incorrectly - but often not tell me that theres an issue. So to catch this youll need far more tests than normal. I prefer to choose a few random pages (more if the PDF/website is longer) and test random rows and columns since thatll give a good coverage of the results. In addition, I look at the PDF or website and try to see if theres anything atypical about a certain part; if there is, I test that specifically. Its easy to over-test (and thats better than under-testing) this kind of work, but there are rapidly diminishing returns. So test comprehensively but not at the cost of having too little time to work on code - again, this is something that requires experience and doesnt have a hard rule on what constitutes too much (or too little) testing. 8.3 Test-driven development (TDD) Well finish this chapter by talking about test-driven development (TDD), a philosophy in programming where you write the tests first and then write the code that meets these tests after. This is really an extension to the discussion in Chapter 5 of planning out your project before you start. In Chapter 5 we talked about writing out every step of the project and hand sketching all the figures or tables that you intended to have. With TDD, you write tests for all of the functions you intend to write (and any variations of parameters or inputs for these functions) or data you intend to gather/clean. Test-driven development is a useful tool to make you really think about the functions that you need to write, and how they interact with each other. This is an excellent way to identify potential issues (Ive often realized while writing tests that the approach I was going to do wouldnt work) before you start on the code. However, for this same reason, it is a fairly advanced topic since you need to know exactly (or, mostly) what you need to do, and the likely problems that each approach will face. For that reason, I recommend holding off on using TDD until youre fairly experienced with R or programming in general. However, make sure that you dont look less closely just because the results are the way you expect. Past results may be wrong, or you can have a new finding, so make sure to avoid complacency just because you like the results For more info on having separate R scripts for each major section of your code, please refer to Section 5.3 The usethis package is an extremely helpful package that automates a lot of work that you would do primarily for R package development so if you go down that route I recommend exploring the package more through its website https://usethis.r-lib.org/index.html). "],["git.html", "9 Git 9.1 What is Git, and why do I need it? 9.2 Git basics 9.3 Using Git 9.4 Setting up Git on an already-made R Project 9.5 Using Git through RStudio 9.6 When to commit 9.7 Other resources", " 9 Git This chapter covers Git, which is a way to have version control for your code - like a programming version of Dropbox, but with a few added features. This is relatively advanced material and isnt necessary for using R. However, when youre dealing with complex projects or with multiple collaborators it is helpful to use. Given that this material is relatively advanced, feel free to skim or skip this chapter entirely, and come back to it when you think you need it - which will likely be after you finish the rest of the book. 9.1 What is Git, and why do I need it? As you write R code you will - I hope! - save your R script from time to time to avoid losing any code youve written if you close R or shut down your computer. This is important as itll save everything youve done locally, but if your computer crashes youll want your work to be backed up elsewhere. While you should have something like Dropbox or Google Drive that keeps backups of your work, here well talk about Git, which is a version control software that gives you much more control (but requires more work) of the saved work than from something like Dropbox.13 Before getting into exactly how to use Git, well talk first about what it is and how itll help your work. Git is also a very powerful and complex tool so this guide is going to be touching just a small - but useful to most researchers and R programmers - part of it. With backup software such as Dropbox, itll save your work very frequently - so frequently in fact that I sometimes turn off Dropbox when I write R since it keeps interrupting me by saving at the moment Im typing, which stops the typing. The following image is the Dropbox page for some R code that Ive been working on to scrape Covid data. Notice the timestamps - 4/5 of them are within one minute, showing how often Dropbox is saving changes. This is useful if I need the most recent update - or to share the most recent version with a collaborator. Heres the big issue - and the one that Git solves - I have four versions within a minute of each other: whats the difference between them? Dropbox is saving automatically and doesnt indicate how theyre different (clicking on the file shows the complete file, not differences relative to some previous version), which means if I mess up some code a while ago, I cant easily see which version is the one that works. With Git you can wait until youve made enough changes to decide that these changes merit a new version of your work. If youve ever used the track changes feature on a Word Document, the concept is similar. When you have this setting in a Word Document every time you (or anyone else) makes changes in that document, those changes, who made them, and when they occurred, is tracked. This makes it easy to see exactly what part of the file was changed and to undo that change if necessary. Below is an example of this feature on one of my drafts on Overleaf (basically a way to collaborate using LaTeX, which is similar to R Markdown). You can see each change that my co-author made in the draft in the purple changes in the main part of the photo. The parts that were rewritten or added are highlighted in purple while the parts that were deleted are crossed out. What is shown in purple isnt all of the history of changes for this paper. If you look at the part on the right, highlighted in green, it shows what files were edited, by whom, and at what time. If you dont like a change - or in Rs case more commonly, broke some code by accident - you can go back in the history of changes and return to an older version. The way that R - and many other programming languages (and technically you can use this for any file or folder) does this version control is through Git. 9.2 Git basics There are four main processes you need to know for a basic understanding of Git: checkout, add and commit, push, and pull. This chapter will explain how to use Git through buttons on RStudio so you dont necessarily need to know these commands in Git, but its useful to know enough to talk about them and ask questions if needed. Well use the example of getting a book from the library to walk through using Git. The steps for this are simple, we go to the library, pick a book we want, check it out from the librarian, read it, and eventually return it. Using Git adds one wrinkle to this: we will want to write in the book and see what other people write too. Of course, when the book is checked out, no one else could write in our version, and no one can see what we write. So anything we write has to be done before we return the book to the library, then we check out the book again to see what other people have written. When we want another book, we simply redo these steps. Library Steps Git steps Git code Go to library Find book and check out book Clone (usually will just be done once per project). Git clone path to repo, can be GitHub link Read or write in book This is done in R, not in Git No Git code, this is going to be whatever code we write in R. Also includes any outputs such as making a graph that is saved, R Markdown outputs like a PDF, or even new R files. Return book Add and commit Push Git add . Git commit m message indicating what we wrote Git push Check out book again (to see what other people have written in it) Pull Git pull Another way to think about commit vs push is that of writing an email. When you write an email, youre essentially editing a blank document by adding the words of the email. When you save (but dont send) the email, you are making a commit (essentially committing or promising to make a change). When you send the email you are making a push (taking something that you have written and changed and sending it to the main repository). While emails let you correspond directly between two or more people, how Git works is like sending the email to a central server (or a post office) and anyone who wants to read it has to go there. And when someone reads it and responds, their email also goes to this central server. You have to go there to get their response (called a pull in Git terms), which is essentially an addition to your initial email. 9.3 Using Git While you can use Git like writing R code (though the syntax is not that similar to R), RStudio has built-in buttons that work instead of writing code yourself. Well go through these buttons and not discuss any Git code beyond the small amount needed to link your project to GitHub, a website that is like Dropbox for code. 9.3.1 Setting up Git To install Git on your computer install Git for Windows for Windows computers and Xcode for Mac computers. If youre on a Linux operating system, see here for how to install Git. For more help I recommend this chapter of Happy Git and GitHub for the useR, which covers installing Git. Youll now need to tell Git some identifying information about yourself so that whenever you make a commit, Git will know who you are. We will use a function from the usethis package to do this. The only information we need is your name (or nickname, just something so collaborators know that it was you who did a certain commit) and email address (below youll set up an account on GitHub - use the same email address there as here). Well use the function use_git_config, which has two parameters - user.name and user.email, which take strings with your name and email, respectively. library(usethis) # Warning: package &#39;usethis&#39; was built under R version 4.1.3 use_git_config( user.name = &quot;Your name&quot;, user.email = &quot;email_address@gmail.com&quot; ) Once you have Git installed, youll need to enable it through RStudio. To do this, go to Tools and click Global Options. Then go to the Git/SVN tab and check the Enable version control interface for RStudio projects checkbox. The final step here is to click the first Browse button and navigate to where you installed Git on your computer. Select the Git file (on a Windows computer this will be within the larger Git folder) and then hit OK to close the popup. 9.3.2 Setting up GitHub Well be using GitHub to host our Git commits. To use GitHub, please make an account on their website. There are several types of accounts at various monthly costs, but you only need the free version. This gives you an unlimited number of public and private repositories (sometimes shorthanded to repos) - these are basically R Projects (you can use any language when it comes to using Git and GitHub, not just R). A public repository is one that anyone can look at on GitHub, download the code/files, and make any changes they want (though if they want to make changes to your repository they need to make a change request that requires your approval; it is not automatic). This is good for projects where you want others to collaborate on or to showcase your work. A private repository is the same thing, but only people you approve can view, download, and work on your repository. This is good for when you dont want the code to be public (e.g. code for an employer or dealing with sensitive data, such as peoples personal information). I tend to keep my research work private until the paper is published and my data work public since I want people to notice it and find bugs.14 Once youve made an account on GitHub, youll need to create a repository there to connect to your R Project. You can do this through the GitHub home page as shown in the following image. This page is my own homepage and shows several of my current repositories on the left (note the ones with a golden lock to the left, these are the private repositories which are only accessible to people I permit), a list of updates on other peoples repositories that I chose to get updates from, and some suggested repositories that GitHub thinks Id be interested in on the right. To create a new repository, click the green New button on the left side above the list of current repositories. After you click the green New button, youll go to a page when you set a name for your repository (this can be different from the name of your R Project, though I prefer to use the same name so I know exactly what project the repository is for), provide a short description, and choose if the repository should be public or private. You can also optionally add a README file, which is a longer form of description for what the code is and its purpose (basically a short manual for the project - often explaining how, not why, it works), and add a .gitignore file or set a license (which tells people who look at the project what theyre allowed to do with it. For more on code licenses please see this excellent site.) The .gitignore file is essentially a list of files or folders than you do not want to upload to GitHub. These last three choices are all optional. and if you dont do it now, you can do it anytime through R. Once youve made your choices, click the green Create Repository button This will open up a new page with a bunch of code that youll enter in R that connects your Git commits to this repository on GitHub. Well get to this in a bit - for now, lets focus on those three buttons in the top right. These are for accessing or following other peoples public repositories (you can technically click on them in your own repository, but there isnt much benefit to that apart from the first button). The first button sets your notification settings for the repository. To change the notification setting, click Unwatch and then select what you want to be notified for. By default it is set to notify you of all conversations that occur. The main conversation will be when someone posts a message in the Issues tab where they tell you about an issue (or sometimes make a request for a new feature or just ask a question) about the code in this repo. With your own repositories, youll want to be notified of all conversations so you dont miss anything. You can use this option on other peoples repositories, and it will alert you of changes or conversations in that repo. This is useful when you want to know about updates (i.e. new features) on repositories that youre interested in (for example, I follow the testthat repo so I know of any new versions of that package that may have useful features). Stars are simply a way to favorite a repository, and you can see a list of all repositories that you have starred by clicking the profile button on the top right and going to Your stars. The final option is Fork which creates a new repository on your account that is a copy of the repository that you forked. You will occasionally want to fork other peoples repositories - there isnt much benefit of forking your own as thats essentially just making a duplicate of your own work - and modify them to suit your needs. This is useful for two reasons. First, if you want to collaborate with someone - even if just to submit a fix to a bug you found (or a typo in this book!) - you can fork their repository, make the changes on your own R Project, commit the changes, and request that the original account accept your changes into the repository that you forked (called a pull request). This sounds very complicated to make what could be a simple change (and it is) so why bother? As you get more familiar with R and how R handles Git, this process wont take too much extra time so its not that much of an additional burden. But the main advantage is that Git establishes much more structure than would exist otherwise, and helps protect the original creators time. Consider that you found a bug in some of my code and sent me an email detailing that issue. This is probably the best-case scenario for you - it is quick to send emails. For me, that adds time to try to figure out what and where the bug is (describing it better would just take more time for you to write and me to read) and then to fix the bug. Even if you included the fix in the email, it would take me time to test it. When using Git and GitHub, this process is far easier for the person receiving the changes (and while it is extra work because you must follow Git procedures, it can be somewhat easier as you wont need to explain as much). If you submit a bug fix to me through GitHub, I will immediately know what it changes as Git highlights all differences between my version and your fixed version, and I can set it to automatically run tests (see Chapter 8 for more on this) to make sure everything works. There are no longer any questions of what was changed, where the code was changed, or whether it passes all the unit tests (GitHub will run all unit tests and tell you if they pass). Everything is largely automated so accepting changes is a breeze. As you program and collaborate more, youll increasingly be on the side of receiving changes to your code, so the balance between extra work as a submitter and easier time as a receiver of changes gets better. In Section 5.2 we walked through making an R Project and selected the Create a Git repository box without explaining what that does. Clicking this box sets the R Project up to use Git so you dont need to do any other steps from the R side (but youll need some steps to connect with GitHub). In the below section we discuss a simple way to connect your R Project to Git if you didnt check this box. If you plan on always checking the box - and have no unchecked R Projects that you want to use with Git, feel free to skip the following section. 9.4 Setting up Git on an already-made R Project If you didnt tell RStudio to set up Git in your R Project, its quite simple to do so through RStudio. First, go to Tool -&gt; Project Options. Then click the Git/SVN button that is second to the bottom to open up the Git options. This will open up a page that says Version control system, which will be set to (None). Click this and set it to Git. It will then ask if you want to set up Git for the current R Project. Say Yes. You need to restart RStudio for Git to work now, so click Yes. Now if you look at the Environment panel you can see a new tab called Git. Well do all of the Git work in RStudio through this tab. You are now ready to use Git for this project. 9.5 Using Git through RStudio Now we have an R Project with Git ready, and a repo on GitHub to store the project files. We need a way to connect the R Project to the specific GitHub repo - for this, well return to that screen on GitHub with all of the weird code that starts with the word git. We need to enter that code into R to connect the two. To do this, we need to use the Git Shell, which is basically like the Console panel but for Git. You can get to this by going to the Git tab, click on the More button, then click Shell. This opens up a popup almost identical to the Console panel. Here we can write the code (or copy it from GitHub) and hit enter/return to run the line. This is the only time we will be using actual Git code in this chapter (there is some benefit to learning the Git code rather than relying on the buttons in RStudio as it is much faster when dealing with large files or simply a large number of files to use the code rather than through RStudio - though Im not sure why this is). We will use the first chunk of code thats shown on GitHub - the one that starts with the bold text or create a new repository on the command line. You can copy and paste all of the code (starting with the echo line and ending with the Git push -u origin master line) to the shell and hit enter or you can do it one line at a time. Refresh your GitHub page and youll see that instead of code on the screen, it shows the files that you uploaded. In this case, I didnt make any files so it is largely blank, just a relatively empty README file. If this was a real project, youd see all of the same files (except those you chose not to commit) as in your R Project folder. Your R Project is now connected to the GitHub repo so you can do the rest of the Git work on this project entirely through RStudio and will not need to touch the Git Shell again. The below image shows my Git tab while working on this chapter and from an update to the Subsetting chapter. It has a list of all of the files that I changed since my last commit (if you havent committed at all yet, this is just all of the files in your project folder) and is color coded based on what I did to them. The blue M means that I have modified an already existing (i.e. one that has already been committed through Git) file, and the yellow ? means that these are new files. If there was a red D next to any of the files, that would mean that I deleted a file that had previously been committed. There are a lot of buttons here (Diff, Commit, Pull, etc.) but you can ignore them and just click the Commit button when ready to make a commit. Doing so will open up a new window that has all the functionality of these various buttons in an easier (in my opinion) format. This window (shown in the following image) is where you can review the changes and write up a brief note about what you did. The window is a bit overwhelming so well take it in pieces. First lets start by examining how the list of files in the top-left is related to the big box on the bottom with text highlighted in red and green. The list of files is identical to that in the Git tab - its just a list of files that have changed (including new files and deleted files) since the last commit. When you click one, itll show you the changes made to this file relative to the most recent version on Git (note that while this will show changes on R files and some other types of files, not all are available to be viewed - though that wont affect Git working at all - so it may just show a blank part of the window instead). The section that was removed is highlighted in red, and the replacement is highlighted in green. Unfortunately, it shows changes on entire lines so if you only change a small part of a line, you will have to read closely to see the difference. You can look through this to figure out exactly what you changed - both which files were changed and what was changed in each file. Now lets walk through the process of actually committing and pushing your changes to GitHub. In real terms, this is basically uploading a new version of the files to GitHub, with brief documentation of what changed. At this point all we need to do is tell RStudio which files we want to commit, write a brief message explaining the changes, and submit it. First, we select which files to commit by clicking the checkbox to the very left on the top left panel. In the image above, they are all unchecked as I havent selected any yet. You can click each files box or click the Stage button near the top once you have a file (or files) highlighted to stage it. Once its staged the checkbox will now have a check in it. Staging a file just means that you want to commit this file. If you want to commit all of the files, you can do Control+A (or Command+A for Mac users) to select all of the files and then click Stage. Now youre ready to document the overall changes that youre committing, not the changes for each individual file. You do so in the Commit message box on the right. Again, here it is blank but you would write a short description of the changes. There is no hard rule that it must be short, but the general convention is that each commit is relatively small and thus the description of the message can be short. You generally want no less than a short sentence and no more than a paragraph, though of course this depends on your unique circumstances. As you first start out, I think over-describing your work is best as you get a feel to what to do. Now click the Commit button. It will make a popup window showing all the changes that it made. The create mode  stuff is saying that these files are new files that Git hasnt seen before. You can close this popup. You have now completed your first commit using Git through RStudio. The files arent on GitHub just yet though. Now right above the list of files is text that says Your branch is ahead of origin/master by 1 commit. This means that your version of the project is ahead of (since you made changes to the project that you just committed) the version on GitHub. To send it to GitHub you just need to click the Push button on the top right. In our email example, this is like clicking send after writing your draft and saving (committing) it. When you click Push itll open up a popup, which you can close once its done. 9.6 When to commit There is no hard rule for when to make a commit, but the general convention is to make one whenever youve finished a unique part of the work. For example, if you have some data that you need to clean, graph, and run a regression on, youd likely commit after each part is done. One of the benefits of using Git is that you will have a record of each version of the code that you commit - so you want to balance between having too many records that are very similar to each other (similar to saving a new version of a paper draft every time you add a sentence) and too few so you lose a lot of work if you need to go back (similar to saving a new version of the paper only every 10 pages of writing). 9.7 Other resources For an excellent overview of using Git and GitHub with R, please see this chapter of Hadley Wickham and Jenny Bryans book R Packages. For a short and very accessible book on this topic, please see Jenny Bryan and Jim Hesters excellent Happy Git and GitHub for the useR. This came in handy for me as somehow one of my dissertation papers written in R Markdown became empty a couple of months before my defense, and I couldnt undo that change. My Dropbox backup was older than my Git backup so having Git was a real time-saver. You may disagree with my decision to keep research code private until publication - and for good reason. Doing this has the benefit of preventing people from scooping my (and my collaborators) work, but also makes it more likely to lead to bugs as there are fewer people looking at the code. "],["subsetting-intro.html", "10 Subsetting: Making big things small 10.1 Select specific values 10.2 Logical values and operations 10.3 Subsetting a data.frame", " 10 Subsetting: Making big things small For this chapter youll need the following file, which is available for download here: offenses_known_yearly_1960_2020.rds. Subsetting data is a way to take a large data set and reduce it to a smaller one that is better suited for answering a specific question. This is useful when you have a lot of data in the data set that isnt relevant to your research - for example, if you are studying crime in Colorado and have every state in your data, youd subset it to keep only the Colorado data. Reducing it to a smaller data set makes it easier to manage, both in understanding your data and avoiding have a huge file that could slow down R. 10.1 Select specific values animals &lt;- c(&quot;cat&quot;, &quot;dog&quot;, &quot;gorilla&quot;, &quot;buffalo&quot;, &quot;lion&quot;, &quot;snake&quot;) animals # [1] &quot;cat&quot; &quot;dog&quot; &quot;gorilla&quot; &quot;buffalo&quot; &quot;lion&quot; &quot;snake&quot; Here we have made a vector object called animals with a number of different animals in it. In R, we will use square brackets [] to select specific values in that object, something called indexing. Put a number (or numbers) in the square bracket, and it will return the value at that index. The index is just the place number where each value is. cat is the first value in animals so it is at the first index, dog is the second value so it is the second index or index 2. snake is our last value and is the 6th value in animals so it is index 6.15 The syntax (how the code is written) goes object[index] First, we have the object and then we put the square bracket []. We need both the object and the [] for subsetting to work. Lets say we wanted to choose just the snake from our animals object. In normal language we say I want the 6th value from animals. We say where were looking and which value we want. animals[6] # [1] &quot;snake&quot; Now lets get the third value. animals[3] # [1] &quot;gorilla&quot; If we want multiple values, we can enter multiple numbers. If you have multiple values, you need to make a vector using c() and put the numbers inside the parentheses separated by a comma. If we wanted values 1-3, we could use c(1, 2, 3), with each number separated by a comma. animals[c(1, 2, 3)] # [1] &quot;cat&quot; &quot;dog&quot; &quot;gorilla&quot; When making a vector of sequential integers, instead of writing them all out manually we can use first_number:last_number like so 1:3 # [1] 1 2 3 To use it in subsetting we can treat 1:3 as if we wrote c(1, 2, 3). animals[1:3] # [1] &quot;cat&quot; &quot;dog&quot; &quot;gorilla&quot; The order we enter the numbers determines the order of the values it returns. Lets get the third index, the fourth index, and the first index, in that order. animals[c(3, 4, 1)] # [1] &quot;gorilla&quot; &quot;buffalo&quot; &quot;cat&quot; Putting a negative number inside the [] will return all values except for that index, essentially deleting it. Lets remove cat from animals. Since it is the 1st item in animals, we can remove it like this animals[-1] # [1] &quot;dog&quot; &quot;gorilla&quot; &quot;buffalo&quot; &quot;lion&quot; &quot;snake&quot; Now lets remove multiple values, the first 3. animals[-c(1, 2, 3)] # [1] &quot;buffalo&quot; &quot;lion&quot; &quot;snake&quot; When using the first_number:last_number notation, we need to put it in parentheses if we want to turn it negative. If we dont, it will just think that the first value is a negative number, and give every integer from that first value to the last value. -1:3 # [1] -1 0 1 2 3 Putting it in parentheses will create the integers first and then turn them all negative. animals[-(1:3)] # [1] &quot;buffalo&quot; &quot;lion&quot; &quot;snake&quot; Earlier I said we can remove values with using a negative number and that index will be removed from the object. For example, animals[-1] prints every value in animals except for the first value. animals[-1] # [1] &quot;dog&quot; &quot;gorilla&quot; &quot;buffalo&quot; &quot;lion&quot; &quot;snake&quot; However, it doesnt actually remove anything from animals. Lets print animals and see which values it returns. animals # [1] &quot;cat&quot; &quot;dog&quot; &quot;gorilla&quot; &quot;buffalo&quot; &quot;lion&quot; &quot;snake&quot; Now the first value, cats, is back. Why? To make changes in R you need to tell R very explicitly that you are making the change. If you dont save the result of your code (by assigning an object to it), R will run that code and simply print the results in the Console panel without making any changes. This is an important point that a lot of students struggle with. R doesnt know when you want to save (in this context I am referring to creating or updating an object that is entirely in R, not saving a file to your computer) a value or update an object. If x is an object with a value of 2, and you write x + 2, it would print out 4 because 2 + 2 = 4. But that wont change the value of x. x will remain as 2 until you explicitly tell R to change its value. If you want to update x you need to run x &lt;- somevalue or x = somevalue, where somevalue is whatever you want to change x to. So to return to our animals example, if we wanted to delete the first value and keep it removed, wed need to write animals &lt;- animals[-1]. Which is essentially making a new object, also called animals (to avoid having many, slightly different objects that are hard to keep track of well reuse the name) with the same values as the original animals except this time excluding the first value, cats. 10.2 Logical values and operations We also frequently want to conditionally select certain values. Earlier we selected values by indexing specific numbers, but that requires us to know exactly which values we want. We can conditionally select values by having some conditional statement (e.g. this value is lower than the number 100) and keeping only values where that condition is true. First, we will discuss conditionals abstractly and then we will use a real example using data from the FBI to make a data set tailored to answer a specific question. We can use these TRUE and FALSE (in R true and false must be spelled all in capital letters and without quotes. For the book section on logical values, please see Section 3.1) values to index, and it will return every element which we say is TRUE. animals[c(TRUE, TRUE, FALSE, FALSE, FALSE, FALSE)] # [1] &quot;cat&quot; &quot;dog&quot; This is the basis of conditional subsetting. If we have a large data set and only want a small chunk based on some condition (e.g. data for certain states, data for a certain time period, data with at least a certain population) we need to make a conditional statement that returns TRUE if it matches what we want and FALSE if it doesnt. There are a number of different ways to make conditional statements. First lets go through some special characters involved and then show examples of each one. For each case you are asking: does the thing on the left of the conditional statement return TRUE or FALSE compared to the thing on the right. == Equals (compared to a single value) %in% Equals (one value match out of multiple comparisons) != Does not equal &lt; Less than &gt; Greater than &lt;= Less than or equal to &gt;= Greater than or equal to Since many conditionals involve numbers (especially in criminology), lets make a new object called numbers with the numbers 1-10. numbers &lt;- 1:10 10.2.1 Matching a single value The conditional == asks if the thing on the left equals the thing on the right. Note that it uses two equal signs. If we used only one equal sign it would assign the thing on the left the value of the thing on the right (as if we did &lt;-). 2 == 2 # [1] TRUE This gives TRUE as we know that 2 does equal 2. If we change either value, it would give us FALSE. 2 == 3 # [1] FALSE And it works when we have multiple numbers on the left side, such as our object called numbers. This returns TRUE only for the value in numbers that is 2. For all other values it returns FALSE. numbers == 2 # [1] FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE This also works with characters such as the animals in the object we made earlier. gorilla is the third animal in our object, so if we check animals == \"gorilla\" we expect the third value to be TRUE and all others to be FALSE. Make sure that the match is spelled correctly (including capitalization) and is in quotes. animals == &quot;gorilla&quot; # [1] FALSE FALSE TRUE FALSE FALSE FALSE The == only works when there is one thing on the right-hand side. In criminology we often want to know if there is a match for multiple things - is the crime one of the following crimes, did the crime happen in one of these months, is the victim a member of these demographic groups? So we need a way to check if a value is one of many values. 10.2.2 Matching multiple values The R operator %in% asks each value on the left whether or not it is a member of the set on the right. It asks, is the single value on the left-hand side (even when there are multiple values such as our animals object, it goes through them one at a time) a match with any of the values on the right-hand side? It only has to match with one of the right-hand side values to be a match. 2 %in% c(1, 2, 3) # [1] TRUE For our animals object, if we check if they are in the vector c(\"cat\", \"dog\", \"gorilla\"), now all three of those animals will return TRUE. animals %in% c(&quot;cat&quot;, &quot;dog&quot;, &quot;gorilla&quot;) # [1] TRUE TRUE TRUE FALSE FALSE FALSE 10.2.3 Does not match Sometimes it is easier to ask what is not a match. For example, if you wanted to get every month except January, instead of writing the other 11 months, you just ask for any month that does not equal January. We can use !=, which means not equal. When we wanted an exact match, we used ==, if we want a not match, we can use != (this time it is only a single equals sign). 2 != 3 # [1] TRUE &quot;cat&quot; != &quot;gorilla&quot; # [1] TRUE Note that for matching multiple values with %in%, we cannot write !%in% but have to put the ! before the values on the left. !animals %in% c(&quot;cat&quot;, &quot;dog&quot;, &quot;gorilla&quot;) # [1] FALSE FALSE FALSE TRUE TRUE TRUE 10.2.4 Greater than or less than We can use R to compare values using greater than or less than symbols. We can also express greater than or equal to or less than or equal to. 6 &gt; 5 # [1] TRUE 6 &lt; 5 # [1] FALSE 6 &gt;= 5 # [1] TRUE 5 &lt;= 5 # [1] TRUE When used on our object numbers it will return 10 values (since numbers is 10 elements long) with a TRUE if the condition is true for the element and FALSE otherwise. Lets run numbers &gt; 3. We expect the first 3 values to be FALSE as 1, 2, and 3 are not larger than 3. numbers &gt; 3 # [1] FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE 10.2.5 Combining conditional statements - or, and In many cases when you are subsetting you will want to subset based on more than one condition. These conditional statements can be tricky for new R users since you need to remember both what conditions you need and the R code to write it. For a simple introduction to combining conditional statements, well first start with the dog food instructions for my new puppy Peanut. Here, the instructions indicate how much food to feed your dog each day. Then instructions are broken down into dog age and expected size (in pounds or kilograms), and the intersection of these tells you how much food to feed your dog. Even once you figure out how much to feed the dog, theres another conditional statement to figure out whether you feed them twice a day or three times a day. This food chart is basically a conditional statement matrix where you match the conditions on the left side with those on the top to figure out how much to feed your dog.16 So if we wanted to figure out how much to feed a dog that is three months old and will be 4.4 pounds, wed use the first row on the left (which says 4.4 pounds/2.2 kilograms) and the second column (which says three months old). When the dog gets to be four-months-old wed keep the same row but now move one column to the right. In normal English youd say that the dog is four months old and their expected size is 4.4 pounds (2 kg). The language when talking about (and writing code for) a conditional statement in programming is a bit more formal where every condition is spoken as a yes or no question. Here we ask is the dog four months old and is the expected weight 4.4 pounds? If both are true, then we give the dog the amount of food shown for those conditions. If only one is true, then the whole thing is wrong - we wouldnt want to underfeed or overfeed our dog. In this example, a four month old dog can eat between 5/8th of a cup of food and 2 cups depending on their expected size. So having only one condition be true isnt enough. Can you see any issue with this conditional statement matrix? It doesnt cover the all possible choices for age and weight combinations. In fact, it is really quite narrow in what it does cover. For example, it covers two- and three-months, but not any age in between. We can assume that a dog that is 2.5 months old would eat the average of two and three month meal amounts, but wouldnt know for sure. When making your own statements please consider what conditions you are checking for - and, importantly, what youre leaving out. For a real data example, lets say you have crime data from every state between 1960 and 2020. Your research question is did Colorados marijuana legalization affect crime in the state? In that case you want only data from Colorado. Since legalization began in January 2014, you wouldnt need every year, only years some period of time before and after legalization to be able to measure its effect. So you would need to subset based on the state and the year. To make conditional statements with multiple conditions we use | for or and &amp; for and. Condition 1 | Condition 2 2 == 3 | 2 &gt; 1 # [1] TRUE As it sounds, when using | as long as at least one condition is true (we can include as many conditions as we like) it will return TRUE. Condition 1 &amp; Condition 2 2 == 3 &amp; 2 &gt; 1 # [1] FALSE For &amp;, all of the conditions must be true. If even one condition is not true it will return FALSE. 10.3 Subsetting a data.frame Earlier we were using a simple vector. In this book - and in your own work - you will usually work on an entire data set. These generally come in the form called a data.frame, which you can imagine as being like an Excel file with multiple rows and columns. Section 3.3.2 covers data.frames in more detail. Lets load in data from the Uniform Crime Report (UCR), an FBI data set that well work on in a later lesson. This data has crime data every year from 1960-2020 and for nearly every agency in the country. ucr &lt;- readRDS(&quot;data/offenses_known_yearly_1960_2020.rds&quot;) Lets peek at the first 6 rows and 6 columns using the square bracket notation [] for data.frames, which well explain more below. ucr[1:6, 1:6] # ori ori9 agency_name state state_abb year # 1 AK00101 AK0010100 anchorage alaska AK 2020 # 2 AK00101 AK0010100 anchorage alaska AK 2019 # 3 AK00101 AK0010100 anchorage alaska AK 2018 # 4 AK00101 AK0010100 anchorage alaska AK 2017 # 5 AK00101 AK0010100 anchorage alaska AK 2016 # 6 AK00101 AK0010100 anchorage alaska AK 2015 The first 6 rows appear to be agency identification info for Anchorage, Alaska, from 2015-2020. For good measure lets check how many rows and columns are in this data. This will give us some guidance on subsetting, which well see below. nrow() gives us the number of rows and ncol() gives us the number of columns. nrow(ucr) # [1] 1032307 ncol(ucr) # [1] 222 This is a large file with 223 columns and over a million rows. Normally we wouldnt want to print out the names of all 223 columns, but lets do so here as we want to know the variables available to subset. We can use names() to see the name of every column in a data.frame. Inside the parentheses we put the data.frame name (without quotes). names(ucr) # [1] &quot;ori&quot; &quot;ori9&quot; &quot;agency_name&quot; # [4] &quot;state&quot; &quot;state_abb&quot; &quot;year&quot; # [7] &quot;number_of_months_missing&quot; &quot;last_month_reported&quot; &quot;arson_number_of_months_missing&quot; # [10] &quot;arson_last_month_reported&quot; &quot;fips_state_code&quot; &quot;fips_county_code&quot; # [13] &quot;fips_state_county_code&quot; &quot;fips_place_code&quot; &quot;agency_type&quot; # [16] &quot;crosswalk_agency_name&quot; &quot;census_name&quot; &quot;longitude&quot; # [19] &quot;latitude&quot; &quot;address_name&quot; &quot;address_street_line_1&quot; # [22] &quot;address_street_line_2&quot; &quot;address_city&quot; &quot;address_state&quot; # [25] &quot;address_zip_code&quot; &quot;population_group&quot; &quot;population_1&quot; # [28] &quot;population_1_county&quot; &quot;population_2&quot; &quot;population_2_county&quot; # [31] &quot;population_3&quot; &quot;population_3_county&quot; &quot;population&quot; # [34] &quot;country_division&quot; &quot;juvenile_age&quot; &quot;core_city_indication&quot; # [37] &quot;fbi_field_office&quot; &quot;followup_indication&quot; &quot;zip_code&quot; # [40] &quot;month_included_in&quot; &quot;covered_by_ori&quot; &quot;agency_count&quot; # [43] &quot;special_mailing_address&quot; &quot;first_line_of_mailing_address&quot; &quot;second_line_of_mailing_address&quot; # [46] &quot;third_line_of_mailing_address&quot; &quot;fourth_line_of_mailing_address&quot; &quot;officers_killed_by_felony&quot; # [49] &quot;officers_killed_by_accident&quot; &quot;officers_assaulted&quot; &quot;actual_murder&quot; # [52] &quot;actual_manslaughter&quot; &quot;actual_rape_total&quot; &quot;actual_rape_by_force&quot; # [55] &quot;actual_rape_attempted&quot; &quot;actual_robbery_total&quot; &quot;actual_robbery_with_a_gun&quot; # [58] &quot;actual_robbery_with_a_knife&quot; &quot;actual_robbery_other_weapon&quot; &quot;actual_robbery_unarmed&quot; # [61] &quot;actual_assault_total&quot; &quot;actual_assault_with_a_gun&quot; &quot;actual_assault_with_a_knife&quot; # [64] &quot;actual_assault_other_weapon&quot; &quot;actual_assault_unarmed&quot; &quot;actual_assault_simple&quot; # [67] &quot;actual_burg_total&quot; &quot;actual_burg_force_entry&quot; &quot;actual_burg_nonforce_entry&quot; # [70] &quot;actual_burg_attempted&quot; &quot;actual_theft_total&quot; &quot;actual_mtr_veh_theft_total&quot; # [73] &quot;actual_mtr_veh_theft_car&quot; &quot;actual_mtr_veh_theft_truck&quot; &quot;actual_mtr_veh_theft_other&quot; # [76] &quot;actual_all_crimes&quot; &quot;actual_assault_aggravated&quot; &quot;actual_index_violent&quot; # [79] &quot;actual_index_property&quot; &quot;actual_index_total&quot; &quot;actual_arson_single_occupancy&quot; # [82] &quot;actual_arson_other_residential&quot; &quot;actual_arson_storage&quot; &quot;actual_arson_industrial&quot; # [85] &quot;actual_arson_other_commercial&quot; &quot;actual_arson_community_public&quot; &quot;actual_arson_all_oth_structures&quot; # [88] &quot;actual_arson_total_structures&quot; &quot;actual_arson_motor_vehicles&quot; &quot;actual_arson_other_mobile&quot; # [91] &quot;actual_arson_total_mobile&quot; &quot;actual_arson_all_other&quot; &quot;actual_arson_grand_total&quot; # [94] &quot;tot_clr_murder&quot; &quot;tot_clr_manslaughter&quot; &quot;tot_clr_rape_total&quot; # [97] &quot;tot_clr_rape_by_force&quot; &quot;tot_clr_rape_attempted&quot; &quot;tot_clr_robbery_total&quot; # [100] &quot;tot_clr_robbery_with_a_gun&quot; &quot;tot_clr_robbery_with_a_knife&quot; &quot;tot_clr_robbery_other_weapon&quot; # [103] &quot;tot_clr_robbery_unarmed&quot; &quot;tot_clr_assault_total&quot; &quot;tot_clr_assault_with_a_gun&quot; # [106] &quot;tot_clr_assault_with_a_knife&quot; &quot;tot_clr_assault_other_weapon&quot; &quot;tot_clr_assault_unarmed&quot; # [109] &quot;tot_clr_assault_simple&quot; &quot;tot_clr_burg_total&quot; &quot;tot_clr_burg_force_entry&quot; # [112] &quot;tot_clr_burg_nonforce_entry&quot; &quot;tot_clr_burg_attempted&quot; &quot;tot_clr_theft_total&quot; # [115] &quot;tot_clr_mtr_veh_theft_total&quot; &quot;tot_clr_mtr_veh_theft_car&quot; &quot;tot_clr_mtr_veh_theft_truck&quot; # [118] &quot;tot_clr_mtr_veh_theft_other&quot; &quot;tot_clr_all_crimes&quot; &quot;tot_clr_assault_aggravated&quot; # [121] &quot;tot_clr_index_violent&quot; &quot;tot_clr_index_property&quot; &quot;tot_clr_index_total&quot; # [124] &quot;tot_clr_arson_single_occupancy&quot; &quot;tot_clr_arson_other_residential&quot; &quot;tot_clr_arson_storage&quot; # [127] &quot;tot_clr_arson_industrial&quot; &quot;tot_clr_arson_other_commercial&quot; &quot;tot_clr_arson_community_public&quot; # [130] &quot;tot_clr_arson_all_oth_structures&quot; &quot;tot_clr_arson_total_structures&quot; &quot;tot_clr_arson_motor_vehicles&quot; # [133] &quot;tot_clr_arson_other_mobile&quot; &quot;tot_clr_arson_total_mobile&quot; &quot;tot_clr_arson_all_other&quot; # [136] &quot;tot_clr_arson_grand_total&quot; &quot;clr_18_murder&quot; &quot;clr_18_manslaughter&quot; # [139] &quot;clr_18_rape_total&quot; &quot;clr_18_rape_by_force&quot; &quot;clr_18_rape_attempted&quot; # [142] &quot;clr_18_robbery_total&quot; &quot;clr_18_robbery_with_a_gun&quot; &quot;clr_18_robbery_with_a_knife&quot; # [145] &quot;clr_18_robbery_other_weapon&quot; &quot;clr_18_robbery_unarmed&quot; &quot;clr_18_assault_total&quot; # [148] &quot;clr_18_assault_with_a_gun&quot; &quot;clr_18_assault_with_a_knife&quot; &quot;clr_18_assault_other_weapon&quot; # [151] &quot;clr_18_assault_unarmed&quot; &quot;clr_18_assault_simple&quot; &quot;clr_18_burg_total&quot; # [154] &quot;clr_18_burg_force_entry&quot; &quot;clr_18_burg_nonforce_entry&quot; &quot;clr_18_burg_attempted&quot; # [157] &quot;clr_18_theft_total&quot; &quot;clr_18_mtr_veh_theft_total&quot; &quot;clr_18_mtr_veh_theft_car&quot; # [160] &quot;clr_18_mtr_veh_theft_truck&quot; &quot;clr_18_mtr_veh_theft_other&quot; &quot;clr_18_all_crimes&quot; # [163] &quot;clr_18_assault_aggravated&quot; &quot;clr_18_index_violent&quot; &quot;clr_18_index_property&quot; # [166] &quot;clr_18_index_total&quot; &quot;clr_18_arson_single_occupancy&quot; &quot;clr_18_arson_other_residential&quot; # [169] &quot;clr_18_arson_storage&quot; &quot;clr_18_arson_industrial&quot; &quot;clr_18_arson_other_commercial&quot; # [172] &quot;clr_18_arson_community_public&quot; &quot;clr_18_arson_all_oth_structures&quot; &quot;clr_18_arson_total_structures&quot; # [175] &quot;clr_18_arson_motor_vehicles&quot; &quot;clr_18_arson_other_mobile&quot; &quot;clr_18_arson_total_mobile&quot; # [178] &quot;clr_18_arson_all_other&quot; &quot;clr_18_arson_grand_total&quot; &quot;unfound_murder&quot; # [181] &quot;unfound_manslaughter&quot; &quot;unfound_rape_total&quot; &quot;unfound_rape_by_force&quot; # [184] &quot;unfound_rape_attempted&quot; &quot;unfound_robbery_total&quot; &quot;unfound_robbery_with_a_gun&quot; # [187] &quot;unfound_robbery_with_a_knife&quot; &quot;unfound_robbery_other_weapon&quot; &quot;unfound_robbery_unarmed&quot; # [190] &quot;unfound_assault_total&quot; &quot;unfound_assault_with_a_gun&quot; &quot;unfound_assault_with_a_knife&quot; # [193] &quot;unfound_assault_other_weapon&quot; &quot;unfound_assault_unarmed&quot; &quot;unfound_assault_simple&quot; # [196] &quot;unfound_burg_total&quot; &quot;unfound_burg_force_entry&quot; &quot;unfound_burg_nonforce_entry&quot; # [199] &quot;unfound_burg_attempted&quot; &quot;unfound_theft_total&quot; &quot;unfound_mtr_veh_theft_total&quot; # [202] &quot;unfound_mtr_veh_theft_car&quot; &quot;unfound_mtr_veh_theft_truck&quot; &quot;unfound_mtr_veh_theft_other&quot; # [205] &quot;unfound_all_crimes&quot; &quot;unfound_assault_aggravated&quot; &quot;unfound_index_violent&quot; # [208] &quot;unfound_index_property&quot; &quot;unfound_index_total&quot; &quot;unfound_arson_single_occupancy&quot; # [211] &quot;unfound_arson_other_residential&quot; &quot;unfound_arson_storage&quot; &quot;unfound_arson_industrial&quot; # [214] &quot;unfound_arson_other_commercial&quot; &quot;unfound_arson_community_public&quot; &quot;unfound_arson_all_oth_structures&quot; # [217] &quot;unfound_arson_total_structures&quot; &quot;unfound_arson_motor_vehicles&quot; &quot;unfound_arson_other_mobile&quot; # [220] &quot;unfound_arson_total_mobile&quot; &quot;unfound_arson_all_other&quot; &quot;unfound_arson_grand_total&quot; Now lets discuss how to subset this data into a smaller data set to answer a specific question. Lets subset the data to answer our above question of did Colorados marijuana legalization affect crime in the state? Like mentioned above, we need data just from Colorado and just for years around the legalization year - we can do 2011-2017 for simplicity. We also dont need all 223 columns in the current data. Lets say were only interested in whether murder changes. Wed need the column called actual_murder, the state column (as a check to make sure we subset only Colorado), the year column, the population column, the ori column, and the agency_name column (a real analysis would likely grab geographic variables too to see if changes depended on location, but here were just using it as an example). The last two columns - ori and agency_name - arent strictly necessary but would be useful for checking if an agencys values are reasonable (e.g. see if that agency had a sudden huge spike or decline in reported crimes) when checking for outliers, a step we wont do here. Before explaining how to subset from a data.frame, lets write pseudocode (essentially a description of what we are going to do that is readable to people but isnt real code) for our subset. We want Only rows where the state equals Colorado Only rows where the year is 2011-2017 Only the following columns: actual_murder, state, year, population, ori, agency_name 10.3.1 Select specific columns The way to select a specific column in R is called the dollar sign notation. data$column We write the data name followed by a $ and then the column name. Make sure there are no spaces, quotation marks, or misspellings (or capitalization issues). Just the data$column exactly as it is spelled. Since we are referring to data already read into R, there should not be any quotes for either the data or the column name. We can do this for the column agency_name in our UCR data. If we wrote this in the console it would print out every single row in the column. Because this data is large (over a million rows), I am going to wrap this in head() so it only displays the first 6 rows of the column rather than printing the entire column. head(ucr$agency_name) # [1] &quot;anchorage&quot; &quot;anchorage&quot; &quot;anchorage&quot; &quot;anchorage&quot; &quot;anchorage&quot; &quot;anchorage&quot; Theyre all the same name because Anchorage Police reported many times and are in the data set multiple times. Lets look at the column actual_murder, which shows the annual number of murders in that agency. head(ucr$actual_murder) # [1] 18 32 26 27 28 26 One hint is to write out the data set name in the console and hit the Tab key. Wait a couple of seconds and a popup will appear listing every column in the data set. You can scroll through this and then hit enter to select that column. 10.3.2 Select specific rows In the earlier examples, we used square bracket notation [] and just put a number or several numbers in the []. When dealing with data.frames, however, you need an extra step to tell R which columns to keep. The syntax in the square bracket is [row, column] We start the square bracket by saying which row we want. Now, since we also have to consider the columns, we need to tell it the number or name (in a vector using c() if more than one name and putting column names in quotes) of the column or columns we want. The exception to this is when we use the dollar sign notation to select a single column. In that case we dont need a comma (and indeed it will give us an error!). Lets see a few examples and then explain why this works the way it does. ucr[1, 1] # [1] &quot;AK00101&quot; If we input multiple numbers, we can get multiple rows and columns. ucr[1:6, 1:6] # ori ori9 agency_name state state_abb year # 1 AK00101 AK0010100 anchorage alaska AK 2020 # 2 AK00101 AK0010100 anchorage alaska AK 2019 # 3 AK00101 AK0010100 anchorage alaska AK 2018 # 4 AK00101 AK0010100 anchorage alaska AK 2017 # 5 AK00101 AK0010100 anchorage alaska AK 2016 # 6 AK00101 AK0010100 anchorage alaska AK 2015 The column section also accepts a vector of the names of the columns. These names must be spelled correctly and in quotes. ucr[1:6, c(&quot;ori&quot;, &quot;year&quot;)] # ori year # 1 AK00101 2020 # 2 AK00101 2019 # 3 AK00101 2018 # 4 AK00101 2017 # 5 AK00101 2016 # 6 AK00101 2015 In cases where we want every row or every column, we just dont put a number. By default, R will return every row/column if you dont specify which ones you want. However, you will still need to include the comma. Here is every column in the first row. Again, for real work wed likely not do this as it will print out hundreds of rows to the console. ucr[1, ] # ori ori9 agency_name state state_abb year number_of_months_missing last_month_reported # 1 AK00101 AK0010100 anchorage alaska AK 2020 0 december # arson_number_of_months_missing arson_last_month_reported fips_state_code fips_county_code fips_state_county_code # 1 0 december 02 020 02020 # fips_place_code agency_type crosswalk_agency_name census_name longitude latitude # 1 03000 local police department anchorage police department anchorage municipality -149.284329 61.17425 # address_name address_street_line_1 address_street_line_2 address_city address_state # 1 anchorage police department 4501 elmore rd &lt;NA&gt; anchorage ak # address_zip_code population_group population_1 population_1_county population_2 population_2_county # 1 99507 city 250,000 thru 499,999 286388 0 0 NA # population_3 population_3_county population country_division juvenile_age core_city_indication fbi_field_office # 1 0 NA 286388 pacific NA core city of msa 3030 # followup_indication zip_code month_included_in covered_by_ori agency_count special_mailing_address # 1 do not send a follow-up 99507 0 &lt;NA&gt; 1 not a special mailing address # first_line_of_mailing_address second_line_of_mailing_address third_line_of_mailing_address # 1 4501 elmore rd &lt;NA&gt; &lt;NA&gt; # fourth_line_of_mailing_address officers_killed_by_felony officers_killed_by_accident officers_assaulted # 1 &lt;NA&gt; 0 0 464 # actual_murder actual_manslaughter actual_rape_total actual_rape_by_force actual_rape_attempted # 1 18 0 558 534 24 # actual_robbery_total actual_robbery_with_a_gun actual_robbery_with_a_knife actual_robbery_other_weapon # 1 558 124 65 82 # actual_robbery_unarmed actual_assault_total actual_assault_with_a_gun actual_assault_with_a_knife # 1 287 5777 512 377 # actual_assault_other_weapon actual_assault_unarmed actual_assault_simple actual_burg_total # 1 840 609 3439 1444 # actual_burg_force_entry actual_burg_nonforce_entry actual_burg_attempted actual_theft_total # 1 900 453 91 7279 # actual_mtr_veh_theft_total actual_mtr_veh_theft_car actual_mtr_veh_theft_truck actual_mtr_veh_theft_other # 1 1149 807 278 64 # actual_all_crimes actual_assault_aggravated actual_index_violent actual_index_property actual_index_total # 1 16856 2338 3472 9945 13417 # actual_arson_single_occupancy actual_arson_other_residential actual_arson_storage actual_arson_industrial # 1 6 16 1 0 # actual_arson_other_commercial actual_arson_community_public actual_arson_all_oth_structures # 1 10 7 0 # actual_arson_total_structures actual_arson_motor_vehicles actual_arson_other_mobile actual_arson_total_mobile # 1 30 17 0 17 # actual_arson_all_other actual_arson_grand_total tot_clr_murder tot_clr_manslaughter tot_clr_rape_total # 1 0 73 15 0 46 # tot_clr_rape_by_force tot_clr_rape_attempted tot_clr_robbery_total tot_clr_robbery_with_a_gun # 1 41 5 207 30 # tot_clr_robbery_with_a_knife tot_clr_robbery_other_weapon tot_clr_robbery_unarmed tot_clr_assault_total # 1 33 27 117 3407 # tot_clr_assault_with_a_gun tot_clr_assault_with_a_knife tot_clr_assault_other_weapon tot_clr_assault_unarmed # 1 223 281 511 428 # tot_clr_assault_simple tot_clr_burg_total tot_clr_burg_force_entry tot_clr_burg_nonforce_entry # 1 1964 237 115 118 # tot_clr_burg_attempted tot_clr_theft_total tot_clr_mtr_veh_theft_total tot_clr_mtr_veh_theft_car # 1 4 865 197 153 # tot_clr_mtr_veh_theft_truck tot_clr_mtr_veh_theft_other tot_clr_all_crimes tot_clr_assault_aggravated # 1 39 5 5001 1443 # tot_clr_index_violent tot_clr_index_property tot_clr_index_total tot_clr_arson_single_occupancy # 1 1711 1326 3037 2 # tot_clr_arson_other_residential tot_clr_arson_storage tot_clr_arson_industrial tot_clr_arson_other_commercial # 1 8 0 0 5 # tot_clr_arson_community_public tot_clr_arson_all_oth_structures tot_clr_arson_total_structures # 1 3 0 13 # tot_clr_arson_motor_vehicles tot_clr_arson_other_mobile tot_clr_arson_total_mobile tot_clr_arson_all_other # 1 5 0 5 0 # tot_clr_arson_grand_total clr_18_murder clr_18_manslaughter clr_18_rape_total clr_18_rape_by_force # 1 27 0 0 11 11 # clr_18_rape_attempted clr_18_robbery_total clr_18_robbery_with_a_gun clr_18_robbery_with_a_knife # 1 0 5 2 0 # clr_18_robbery_other_weapon clr_18_robbery_unarmed clr_18_assault_total clr_18_assault_with_a_gun # 1 1 2 228 18 # clr_18_assault_with_a_knife clr_18_assault_other_weapon clr_18_assault_unarmed clr_18_assault_simple # 1 13 25 12 160 # clr_18_burg_total clr_18_burg_force_entry clr_18_burg_nonforce_entry clr_18_burg_attempted clr_18_theft_total # 1 4 4 0 0 36 # clr_18_mtr_veh_theft_total clr_18_mtr_veh_theft_car clr_18_mtr_veh_theft_truck clr_18_mtr_veh_theft_other # 1 9 8 1 0 # clr_18_all_crimes clr_18_assault_aggravated clr_18_index_violent clr_18_index_property clr_18_index_total # 1 295 68 84 51 135 # clr_18_arson_single_occupancy clr_18_arson_other_residential clr_18_arson_storage clr_18_arson_industrial # 1 0 0 0 0 # clr_18_arson_other_commercial clr_18_arson_community_public clr_18_arson_all_oth_structures # 1 0 0 0 # clr_18_arson_total_structures clr_18_arson_motor_vehicles clr_18_arson_other_mobile clr_18_arson_total_mobile # 1 0 0 0 0 # clr_18_arson_all_other clr_18_arson_grand_total unfound_murder unfound_manslaughter unfound_rape_total # 1 0 2 4 0 1 # unfound_rape_by_force unfound_rape_attempted unfound_robbery_total unfound_robbery_with_a_gun # 1 1 0 0 0 # unfound_robbery_with_a_knife unfound_robbery_other_weapon unfound_robbery_unarmed unfound_assault_total # 1 0 0 0 0 # unfound_assault_with_a_gun unfound_assault_with_a_knife unfound_assault_other_weapon unfound_assault_unarmed # 1 0 0 0 0 # unfound_assault_simple unfound_burg_total unfound_burg_force_entry unfound_burg_nonforce_entry # 1 0 4 2 1 # unfound_burg_attempted unfound_theft_total unfound_mtr_veh_theft_total unfound_mtr_veh_theft_car # 1 1 43 37 22 # unfound_mtr_veh_theft_truck unfound_mtr_veh_theft_other unfound_all_crimes unfound_assault_aggravated # 1 15 0 89 0 # unfound_index_violent unfound_index_property unfound_index_total unfound_arson_single_occupancy # 1 5 84 89 0 # unfound_arson_other_residential unfound_arson_storage unfound_arson_industrial unfound_arson_other_commercial # 1 0 0 0 0 # unfound_arson_community_public unfound_arson_all_oth_structures unfound_arson_total_structures # 1 0 0 0 # unfound_arson_motor_vehicles unfound_arson_other_mobile unfound_arson_total_mobile unfound_arson_all_other # 1 0 0 0 0 # unfound_arson_grand_total # 1 0 Since there are 223 columns in our data, normally wed want to avoid printing out all of them. And in most cases, we would save the output of subsets to a new object to be used later rather than just printing the output in the console. What happens if we forget the comma? If we put in numbers for both rows and columns but dont include a comma between them it will have an error. ucr[1 1] # Error: &lt;text&gt;:1:7: unexpected numeric constant # 1: ucr[1 1 # ^ If we only put in a single number and no comma, it will return the column that matches that number. Here we have number 1 and it will return the first column. Well wrap it in head() so it doesnt print out a million rows. head(ucr[1]) # ori # 1 AK00101 # 2 AK00101 # 3 AK00101 # 4 AK00101 # 5 AK00101 # 6 AK00101 Since R thinks you are requesting a column, and we only have 223 columns in the data, asking for any number above 223 will return an error. head(ucr[1000]) # Error in `[.data.frame`(ucr, 1000): undefined columns selected If you already specify a column using dollar sign notation $, you do not need to indicate any column in the square brackets[]. All you need to do is say which row or rows you want. ucr$agency_name[15] # [1] &quot;anchorage&quot; 10.3.3 Subset Colorado data Now we have the tools to subset our UCR data to just be Colorado from 2011-2017. There are three conditional statements we need to make, two for rows and one for columns. Only rows where the state equals Colorado Only rows where the year is 2011-2017 Only the following columns: actual_murder, state, year, population, ori, agency_name We could use the &amp; operator to say rows must meet condition 1 and condition 2. Since this is an intro lesson, we will do them as two separate conditional statements. For the first step we want to get all rows in the data where the state equals colorado (in this data all state names are lowercase). And at this point we want to keep all columns in the data. So lets make a new object called colorado to save the result of this subset. Remember that we want to put the object to the left of the [] (and touching the []) to make sure it returns the data. Just having the conditional statement will only return TRUE or FALSE values. Since we want all columns, we dont need to put anything after the comma (but we must include the comma!). colorado &lt;- ucr[ucr$state == &quot;colorado&quot;, ] Now we want to get all the rows where the year is 2011-2017. Since we want to check if the year is one of the years 2011-2017, we will use %in% and put the years in a vector 2011:2017. This time our primary data set is colorado, not ucr since colorado has already subsetted to just the state we want. This is how subsetting generally works. You take a large data set, subset it to a smaller one and continue to subset the smaller one to only the data you want. colorado &lt;- colorado[colorado$year %in% 2011:2017, ] Finally we want the columns stated above and to keep every row in the current data. Since the format is [row, column] in this case we keep the row part blank to indicate that we want every row. colorado &lt;- colorado[, c( &quot;actual_murder&quot;, &quot;state&quot;, &quot;year&quot;, &quot;population&quot;, &quot;ori&quot;, &quot;agency_name&quot; )] We can do a quick check using the unique() function. The unique() function prints all the unique values in a category, such as a column. We will use it on the state and year columns to make sure only the values that we want are present. unique(colorado$state) # [1] &quot;colorado&quot; unique(colorado$year) # [1] 2017 2016 2015 2014 2013 2012 2011 The only state is Colorado and the only years are 2011-2017 so our subset worked! This data shows the number of murders in each agency. We want to look at state trends so in Section 11.3 we will sum up all the murders per year and see if marijuana legalization affected it. 10.3.3.1 Subsetting using dplyr Above, we did subsetting through whats called the base R method. Base R just means that we use functions that are built into R and dont use any packages. A very popular alternative way to do most of the work done in this chapter is to use the dplyr package. dplyr is a very useful package to handle data and includes functions that let us subset data, select only certain columns, and aggregate the data. For the packages website, which covers all of the features in this package, please see here. dplyr is part of what is called the tidyverse, which is a collection of R packages written by mostly the same people that include lots of functions that are useful for working with the kind of data we use in this book. Well cover many of the tidyverse packages in this book. Theres nothing special about a package being a tidyverse package; they operate exactly the same as other packages. I just mention it because it is a very popular set of packages, and people will often talk about tidyverse approaches to R meaning using these packages. So its good to know the terminology. To look at the full list of tidyverse packages, their website here is an excellent overview of them. In a lot of ways the functions well use from dplyr are simpler and easier to use than what we wrote earlier in this chapter. In fact, a lot of people learn only dplyr functions and do not learn (or at least do not spend much time on) base R. For the rest of this book well use base R and tidyverse functions alongside each other. I do this for two reasons. First, its important to understand how R works and using base R is the best way to learn. This is a programming-for-a-purpose book, not a pure programming book, so the focus isnt on knowing all the ins and outs of R. However, I think it is still important to have some understanding of how R works and tidyverse functions tend to obfuscate that. In most cases this obfuscation is a good thing as it lets you focus on working with the data instead of thinking about how R works (and this is one of the tidyverse authors motivations behind their work). In some cases, however, youll encounter issues with either the code or your data where its important to understand how R works. In these (luckily relatively) rare cases, base R tends to be more useful in solving these problems than the tidyverse. The second reason is that base R functions are incredibly stable. Most havent changed since R was first created in the early 1990s. The benefit is that code you write using base R functions will work for a very long time. Using packages outside of base R (all packages, not just tidyverse packages) always carries the risk that a new version of the package will change the behavior of a function, or remove that function entirely. Thankfully this is quite rare as package developers often take care to ensure that old features remain available even as they update their package. But it is always a risk, and for programming for research we want to try to make our code as reproducible as possible, which means trying to ensure that functions we use will keep working in the future. That said, please dont avoid packages too much out of fear of this issue. Packages in R are enormously useful, and well use many of them throughout this book. Well cover two functions from dplyr here, and well also cover a couple more in the next chapter. For now, well look only at filter() and select(). The filter() function is how dplyr does subsetting. It takes a conditional statement and filters the data to only return rows where that conditional statement is true. You can include multiple conditional statements in the parentheses of filter() and itll return only rows where all of the statements are true. The select() function does roughly that with columns where we can input a conditional statement about the name of the column (e.g. columns ending in rate) and itll return only those columns. select() also lets you choose columns just by putting the name of the column(s) in the parentheses and thats all well be using it for here. Lets first copy back some of the code we used earlier when we used base R to subset Colorado data from the UCR data set. colorado &lt;- ucr[ucr$state == &quot;colorado&quot;, ] colorado &lt;- colorado[colorado$year %in% 2011:2017, ] colorado &lt;- colorado[, c( &quot;actual_murder&quot;, &quot;state&quot;, &quot;year&quot;, &quot;population&quot;, &quot;ori&quot;, &quot;agency_name&quot; )] We have two conditional statements - keep only rows where state is Colorado and where years are between 2011 and 2017 (including 2017) - and then we kept only a small number of columns. Well do this one step at a time using the dplyr functions. For filter() we first include the name of our data.frame, which in this case starts as ucr and then becomes colorado as we make a new object during the first line of code, and then we include our conditional statement. Using base R, we have to say which data.frame we used every time we included a column. Using filter() we dont need to do this. filter() is smart enough to select the column from the data.frame we input. For our first filter we can write filter(ucr, state == \"colorado\") and we will save the resulting object into a data set called colorado like we did above. To use any dplyr functions we first need to install that package and then tell R we want to use it through the library() function. install.packages(&quot;dplyr&quot;) library(dplyr) colorado &lt;- filter(ucr, state == &quot;colorado&quot;) Now we can do our second conditional statement where we keep only years 2011 through 2017. colorado &lt;- filter(ucr, year %in% 2011:2017) If we wanted to, we could combine these lines of code into a single line by including both conditional statements into a single filter() function by just including a comma after the first statement. colorado &lt;- filter(ucr, state == &quot;colorado&quot;, year %in% 2011:2017) We follow similar syntax for select() by starting with the name of the data set and then the name of every column you want to keep. Unlike in base R we dont need to put the columns in a vector or to put the names in quotes (though you can put the names in quotes if youd like). The order you put the column names in is also the order it will arrange them, so this function can be used to reorder your columns. colorado &lt;- select( colorado, actual_murder, state, year, population, ori, agency_name ) If we run the same checks on unique states and years as we did after our base R code, well get the same results. This shows that our dplyr code did the same thing as our base R code. unique(colorado$state) # [1] &quot;colorado&quot; unique(colorado$year) # [1] 2017 2016 2015 2014 2013 2012 2011 Some languages use zero indexing, which means the first index is index 0, the second is index 1. So in our example cat would be index 0. R does not do that, and the first value is index 1, the second is index 2, and so on. If you encounter some conditional statements that confuse you - which will be more common as you combine many statements together - I encourage you to make a matrix like this yourself. Even if it isnt that complicated, I think its easier to see it written down than to try to keep all of the possible conditions in your head. "],["explore.html", "11 Exploratory data analysis 11.1 Summary and Table 11.2 Graphing 11.3 Aggregating (summaries of groups) 11.4 Pipes in dplyr", " 11 Exploratory data analysis For this chapter youll need the following files, which are available for download here: ucr2017.rda and offenses_known_yearly_1960_2020.rds. When you first start working on new data it is important to spend some time getting familiar with the data. This includes understanding how many rows and columns it has, what each row means (is each row an offender? a victim? crime in a city over a day/month/year?, etc.), and what columns it has. Basically you want to know if the data is capable of answering the question you are asking. While not a comprehensive list, the following is a good start for exploratory data analysis of new data sets. What are the units (what does each row represent?)? What variables are available? What time period does it cover? Are there outliers? How many? Are there missing values? How many? For the first part of this lesson we will use a data set of FBI Uniform Crime Reporting (UCR) data for 2017. This data includes every agency that reported their data for all 12 months of the year. In this part of the chapter we will look at some summary statistics for the variables we are interested in and make some basic graphs to visualize the data. First, we need to load the data. Make sure your working directory is set to the folder where the data is. load(&quot;data/ucr2017.rda&quot;) The function head() will print out the first 6 rows of every column in the data. Since we only have 9 columns, we will use this function. Be careful when you have many columns (100+) as printing all of them out makes it difficult to read. head(ucr2017) # ori year agency_name state population actual_murder actual_rape_total actual_robbery_total # 1 AK00101 2017 anchorage alaska 296188 27 391 778 # 2 AK00102 2017 fairbanks alaska 32937 10 24 40 # 3 AK00103 2017 juneau alaska 32344 1 50 46 # 4 AK00104 2017 ketchikan alaska 8230 1 19 0 # 5 AK00105 2017 kodiak alaska 6198 0 15 4 # 6 AK00106 2017 nome alaska 3829 0 7 0 # actual_assault_aggravated # 1 2368 # 2 131 # 3 206 # 4 14 # 5 41 # 6 52 From these results it appears that each row is a single agencys annual data for 2017, and the columns show the number of crimes for four crime categories included. Finally, we can run names() to print out every column name. We can already see every name from head(), but this is useful when we have many columns and dont want to use head(). names(ucr2017) # [1] &quot;ori&quot; &quot;year&quot; &quot;agency_name&quot; &quot;state&quot; # [5] &quot;population&quot; &quot;actual_murder&quot; &quot;actual_rape_total&quot; &quot;actual_robbery_total&quot; # [9] &quot;actual_assault_aggravated&quot; 11.1 Summary and Table An important function in understanding the data you have is summary() which, as discussed in Section 2.5, provides summary statistics on the numeric columns you have. Lets take a look at the results before seeing how to do something similar for categorical columns. summary(ucr2017) # ori year agency_name state population actual_murder # Length:15764 Min. :2017 Length:15764 Length:15764 Min. : 0 Min. : 0.000 # Class :character 1st Qu.:2017 Class :character Class :character 1st Qu.: 914 1st Qu.: 0.000 # Mode :character Median :2017 Mode :character Mode :character Median : 4460 Median : 0.000 # Mean :2017 Mean : 19872 Mean : 1.069 # 3rd Qu.:2017 3rd Qu.: 15390 3rd Qu.: 0.000 # Max. :2017 Max. :8616333 Max. :653.000 # actual_rape_total actual_robbery_total actual_assault_aggravated # Min. : -2.000 Min. : -1.00 Min. : -1.00 # 1st Qu.: 0.000 1st Qu.: 0.00 1st Qu.: 1.00 # Median : 1.000 Median : 0.00 Median : 5.00 # Mean : 8.262 Mean : 19.85 Mean : 49.98 # 3rd Qu.: 5.000 3rd Qu.: 4.00 3rd Qu.: 21.00 # Max. :2455.000 Max. :13995.00 Max. :29771.00 The table() function returns every unique value in a category and how often that value appears. Unlike summary() we cant just put the entire data set into the (), we need to specify a single column. To specify a column you use the dollar sign notation, which is data$column. For most functions we use to examine the data as a whole, such as head(), you can do the same for a specific column. head(ucr2017$agency_name) # [1] &quot;anchorage&quot; &quot;fairbanks&quot; &quot;juneau&quot; &quot;ketchikan&quot; &quot;kodiak&quot; &quot;nome&quot; There are only two columns in our data with categorical values that we can use - year and state, so lets use table() on both of them. The columns ori and agency_name are also categorical but as each row of data has a unique ORI and name, running table() on those columns would not be helpful. table(ucr2017$year) # # 2017 # 15764 We can see that every year in our data is 2017, as expected based on the data name. year is a numerical column so why can we use table() on it? R doesnt differentiate between numbers and characters when seeing how often each value appears. If we ran table() on the column actual_murder it would tell us how many times each unique value in the column appeared in the data. That wouldnt be very useful as we dont really care how many times an agency has, for example, 7 murders. As numeric variables often have many more unique values than character variables, it also leads to many values being printed, making it harder to understand. For columns where the number of categories is important to us, such as years, states, neighborhoods, we should use table(). table(ucr2017$state) # # alabama alaska arizona arkansas california # 305 32 107 273 732 # colorado connecticut delaware district of columbia florida # 213 107 63 3 603 # georgia guam hawaii idaho illinois # 522 1 4 95 696 # indiana iowa kansas kentucky louisiana # 247 216 309 352 192 # maine maryland massachusetts michigan minnesota # 135 152 346 625 397 # mississippi missouri montana nebraska nevada # 71 580 108 225 59 # new hampshire new jersey new mexico new york north carolina # 176 576 116 532 310 # north dakota ohio oklahoma oregon pennsylvania # 108 532 409 172 1473 # rhode island south carolina south dakota tennessee texas # 49 427 92 466 999 # utah vermont virginia washington west virginia # 125 85 407 250 200 # wisconsin wyoming # 433 57 This shows us how many times each state is present in the data. States with a larger population tend to appear more often; this makes sense as those states have more agencies to report. Right now the results are in alphabetical order, but when knowing how frequently something appears, we usually want it ordered by frequency. We can use the sort() function to order the results from table(). Just put the entire table() function inside of the () in sort(). sort(table(ucr2017$state)) # # guam district of columbia hawaii alaska rhode island # 1 3 4 32 49 # wyoming nevada delaware mississippi vermont # 57 59 63 71 85 # south dakota idaho arizona connecticut montana # 92 95 107 107 108 # north dakota new mexico utah maine maryland # 108 116 125 135 152 # oregon new hampshire louisiana west virginia colorado # 172 176 192 200 213 # iowa nebraska indiana washington arkansas # 216 225 247 250 273 # alabama kansas north carolina massachusetts kentucky # 305 309 310 346 352 # minnesota virginia oklahoma south carolina wisconsin # 397 407 409 427 433 # tennessee georgia new york ohio new jersey # 466 522 532 532 576 # missouri florida michigan illinois california # 580 603 625 696 732 # texas pennsylvania # 999 1473 And if we want to sort it in decreasing order of frequency, we can use the parameter decreasing in sort() and set it to TRUE. A parameter is just an option used in an R function to change the way the function is used or what output it gives. Almost all functions have these parameters, and they are useful if you dont want to use the default setting in the function. This parameter, decreasing, changes the sort() output to print from largest to smallest. By default this parameter is set to FALSE, and here we say it is equal to TRUE. sort(table(ucr2017$state), decreasing = TRUE) # # pennsylvania texas california illinois michigan # 1473 999 732 696 625 # florida missouri new jersey new york ohio # 603 580 576 532 532 # georgia tennessee wisconsin south carolina oklahoma # 522 466 433 427 409 # virginia minnesota kentucky massachusetts north carolina # 407 397 352 346 310 # kansas alabama arkansas washington indiana # 309 305 273 250 247 # nebraska iowa colorado west virginia louisiana # 225 216 213 200 192 # new hampshire oregon maryland maine utah # 176 172 152 135 125 # new mexico montana north dakota arizona connecticut # 116 108 108 107 107 # idaho south dakota vermont mississippi delaware # 95 92 85 71 63 # nevada wyoming rhode island alaska hawaii # 59 57 49 32 4 # district of columbia guam # 3 1 11.2 Graphing We often want to make quick plots of our data to get a visual understanding of the data. We will learn a different - and in my opinion a superior - way to make graphs in Chapters 14 and 15, but for now lets use the function plot(). The plot() function is built into R so we dont need to use any packages for it. Lets make a few scatterplots showing the relationship between two variables. With plot() the syntax (how you write the code) is plot(x_axis_variable, y_axis_variable). So all we need to do is give it the variable for the x- and y-axis. Each dot will represent a single agency (a single row in our data). plot( ucr2017$actual_murder, ucr2017$actual_robbery_total ) Above we are telling R to plot the number of murders on the x-axis and the number of robberies on the y-axis. This shows the relationship between a citys number of murders and number of robberies. We can see that there is a relationship where more murders is correlated with more robberies. However, there are a huge number of agencies in the bottom-left corner that have very few murders or robberies. This makes sense as - as we see in the summary() above - most agencies are small, with the median population under 5,000 people. To try to avoid that clump of small agencies at the bottom, lets make a new data set of only agencies with a population over 1 million. We will use the filter() function from the dplyr package that was introduced in Chapter 10. For filter(), we need to first include our data set name, which is ucr2017, and then say our conditional statement. Our conditional statement is that rows in the population column have a value of over 1 million. For the dplyr functions we dont put our column name in quotes. And well assign our results to a new object called ucr2017_big_cities Since were using the dplyr package we need to tell R that we want to use it by using library(dplyr). library(dplyr) ucr2017_big_cities &lt;- filter(ucr2017, population &gt; 1000000) Now we have 18 agencies with a population of over 1 million people. Now we can do the same graph as above but using this new data set. plot( ucr2017_big_cities$actual_murder, ucr2017_big_cities$actual_robbery_total ) The problem is somewhat solved. There is still a small clumping of agencies with few robberies or murders, but the issue is much better. And interestingly the trend is similar with this small subset of data as with all agencies included. To make our graph look better, we can add labels for the axes and a title (there are many options for changing the appearance of this graph, we will just use these three). xlab - X-axis label ylab - Y-axis label main - Graph title Like all parameters, we add them in the () of plot() and separate each parameter by a comma. Since we are adding text to write in the plot, all of these parameter inputs must be in quotes. plot(ucr2017_big_cities$actual_murder, ucr2017_big_cities$actual_robbery_total, xlab = &quot;Murders&quot;, ylab = &quot;Robberies&quot;, main = &quot;Relationship between murder and robbery&quot; ) 11.3 Aggregating (summaries of groups) Right now we have the number of crimes in each agency. For many policy analyses wed be looking at the effect on the state as a whole, rather than at the agency-level. If we wanted to do this in our data, we would need to aggregate up to the state level. Aggregating data means that we group values at some higher level than they currently are (e.g. from agency to state, from day to month, from city street to city neighborhood) and then do some mathematical operation of our choosing (in our case usually sum) to that group. In Section 10.3.3 we started to see if marijuana legalization affected murder in Colorado. We subsetted the data to only include agencies in Colorado from 2011-2017. Now we can continue to answer the question by aggregating to the state-level to see the total number of murders per year. Lets think about how our data are and how we would (theoretically, before we write any code) find that out. Our data has a single row for each agency, and we have a column indicating the year the agency reported. So how would we find out how many murders happened in Colorado for each year? Well, first we take all the agencies in 2011 (the first year were looking at) and add up the murders for all agencies that reported that year. Then take all the rows in 2012 and add up their murders. And so on for all the years. To do this in R, well be using two new functions from the dplyr package: group_by() and summarize(). These functions do the aggregation process in two steps. First we use group_by() to tell R which columns we want to group our data by - these are the higher level of aggregation columns so in our case will be the year of data. Then we need to sum up the number of murders each year. We do this using summarize(), and well specify in the function that we want to sum up the data, rather than use some other math operation on it like finding the average number of murders each year. First, lets load back in the data and then repeat the subsetting code we did in Chapter 10.3.3 to keep only data for Colorado from 2011 through 2017. Well also include the actual_robbery_total column that we excluded in Chapter 10.3.3 so we can see how easy it is to aggregate multiple columns at once using this method. ucr &lt;- readRDS(&quot;data/offenses_known_yearly_1960_2020.rds&quot;) colorado &lt;- filter( ucr, state == &quot;colorado&quot;, year %in% 2011:2017 ) colorado &lt;- select( colorado, actual_murder, actual_robbery_total, state, year, population, ori, agency_name ) First we must group the data by using the group_by() function. Here were just grouping the data by year, but we could group it by multiple columns if we want by adding a comma and then the next column we want. Following other dplyr function syntax, we first input the data set name and then the column name - neither of which need to be in quotes. colorado &lt;- group_by(colorado, year) Now we can summarize the data using the summarize() function. As with other dplyr functions the first input is the data set name. Then we choose our math function (sum, mean, median, etc.) and just apply that function on the column we want. So in our case we want the sum of murders so we use sum() and include the column we want to aggregate inside of sum()s parentheses. summarize(colorado, sum(actual_murder)) # # A tibble: 7 x 2 # year `sum(actual_murder)` # &lt;dbl&gt; &lt;dbl&gt; # 1 2011 154 # 2 2012 163 # 3 2013 172 # 4 2014 148 # 5 2015 173 # 6 2016 203 # 7 2017 218 If we want to aggregate another column we just add a comma after our initial column and add another math operation function and the column we want. Here were also using sum(), but we could use different math operations if we want - they dont need to be the same. summarize( colorado, sum(actual_murder), sum(actual_robbery_total) ) # # A tibble: 7 x 3 # year `sum(actual_murder)` `sum(actual_robbery_total)` # &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 2011 154 3287 # 2 2012 163 3369 # 3 2013 172 3122 # 4 2014 148 3021 # 5 2015 173 3305 # 6 2016 203 3513 # 7 2017 218 3811 We could even do different math operations on the same column and wed get multiple columns from it. Lets add another column showing the average number of robberies as an example. summarize( colorado, sum(actual_murder), sum(actual_robbery_total), mean(actual_robbery_total) ) # # A tibble: 7 x 4 # year `sum(actual_murder)` `sum(actual_robbery_total)` `mean(actual_robbery_total)` # &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 2011 154 3287 11.2 # 2 2012 163 3369 11.2 # 3 2013 172 3122 10.3 # 4 2014 148 3021 9.94 # 5 2015 173 3305 10.9 # 6 2016 203 3513 11.6 # 7 2017 218 3811 12.5 By default summarize() calls the columns it makes using what we include in the parentheses. Since we said sum(actual_murder), to get the sum of the murder column, it names that new column sum(actual_murder). Usually well want to name the columns ourselves. We can do this by assigning the summarized column to a name using name = before it. For example, we could write murders = sum(actual_murder) and it will name that column murders instead of sum(actual_murder). Like other things in dplyr functions, we dont need to put quotes around our new column name. Well assign this final summarized data to an object called colorado_agg so we can use it to make graphs. And to be able to create crime rates per population, well also find the sum of the population for each year. colorado_agg &lt;- summarize(colorado, murders = sum(actual_murder), robberies = sum(actual_robbery_total), population = sum(population) ) colorado_agg # # A tibble: 7 x 4 # year murders robberies population # &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 2011 154 3287 5155993 # 2 2012 163 3369 5227884 # 3 2013 172 3122 5308236 # 4 2014 148 3021 5402555 # 5 2015 173 3305 5505856 # 6 2016 203 3513 5590124 # 7 2017 218 3811 5661529 Now we can see that the total number of murders increased over time. So can we conclude that marijuana legalization increases murder? No, all this analysis shows is that the years following marijuana legalization, murders increased in Colorado. But that can be due to many reasons other than marijuana. For a proper analysis youd need a comparison state that is similar to Colorado prior to legalization (and that didnt legalize marijuana) and see if their murders changes following Colorados legalization. To control for population, well standardize our murder data by creating a murder rate per 100,000 people. We can do this by dividing the murder column by the population column and then multiplying by 100,000. Lets do that and assign the result into a new column called murder_rate. colorado_agg$murder_rate &lt;- colorado_agg$murders / colorado_agg$population * 100000 If we also wanted a robbery rate wed do the same with the robberies column. colorado_agg$robbery_rate &lt;- colorado_agg$robberies / colorado_agg$population * 100000 The dplyr package has a helpful function that can do this too, and allows us to do it while writing less code. The mutate() function lets us create or alter columns in our data. Like other dplyr functions we start by including our data set in the parentheses, and then we can follow standard assignment (covered in Section 2.2) though we must use = here and not &lt;-. A benefit of using mutate() is that we dont have to write out our data set name each time. So wed write murder_rate = murders / population * 100000. And if we wanted to make two (or more) columns at the same time we just add a comma after our first assignment and then do the next assignment. mutate(colorado_agg, murder_rate = murders / population * 100000, robbery_rate = robberies / population * 100000 ) # # A tibble: 7 x 6 # year murders robberies population murder_rate robbery_rate # &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 2011 154 3287 5155993 2.99 63.8 # 2 2012 163 3369 5227884 3.12 64.4 # 3 2013 172 3122 5308236 3.24 58.8 # 4 2014 148 3021 5402555 2.74 55.9 # 5 2015 173 3305 5505856 3.14 60.0 # 6 2016 203 3513 5590124 3.63 62.8 # 7 2017 218 3811 5661529 3.85 67.3 Now lets make a plot of this data showing the murder rate over time. With time-series graphs we want the time variable to be on the x-axis and the numeric variable that we are measuring to be on the y-axis. plot( x = colorado_agg$year, y = colorado_agg$murder_rate ) By default plot() makes a scatterplot. If we set the parameter type to l it will be a line plot. plot( x = colorado_agg$year, y = colorado_agg$murder_rate, type = &quot;l&quot; ) We can add some labels and a title to make this graph easier to read. plot( x = colorado_agg$year, y = colorado_agg$murder_rate, type = &quot;l&quot;, xlab = &quot;Year&quot;, ylab = &quot;Murders per 100k Population&quot;, main = &quot;Murder Rate in Colorado, 2011-2017&quot; ) 11.4 Pipes in dplyr To end this chapter well talk about something called a pipe that is a very useful and powerful part of dplyr. Think about the math equation 1 + 2 + 3 + 4. Here we know that we add 1 and 2 together, and then add the result to 3 and then add the result of that to 4. This is much simpler to write than splitting everything up and summing each value together in a different line. In terms of R, we have so far been doing things as if we could only add two numbers together and then need a separate line to add the third (and another line to add the fourth) number. For example, below are the two lines of code we used to subset the data to just the right state and years we wanted, and the columns we wanted. We did this in two separate lines. In our math example, we did 1 + 2. And then found the answer, and separately did 3 + 3. And then again found the answer and did 6 + 4. colorado &lt;- filter( ucr, state == &quot;colorado&quot;, year %in% 2011:2017 ) colorado &lt;- select( colorado, actual_murder, actual_robbery_total, state, year, population, ori, agency_name ) head(colorado) # actual_murder actual_robbery_total state year population ori agency_name # 1 7 80 colorado 2017 99940 CO00100 adams # 2 11 93 colorado 2016 100526 CO00100 adams # 3 6 68 colorado 2015 100266 CO00100 adams # 4 6 58 colorado 2014 98569 CO00100 adams # 5 7 44 colorado 2013 97146 CO00100 adams # 6 7 55 colorado 2012 93542 CO00100 adams With dplyr we actually do have a way to chain together functions; to do the programming equivalent of 1 + 2 + 3 + 4 all at once.17 We do this through what is called a pipe, which allows us to take the result of one function and immediately put it into another function without having to save the initial result or start a new line of code. To use a pipe we put the following code after the end of a function: %&gt;%. These three characters, %&gt;%, are the pipe, and they must be written exactly like this. The pipe is itself actually a function, but it is a special type of function we wont go into detail about. Personally I dont think this really looks like a pipe at all, but it is called a pipe so thats the terminology Ill be using. How a pipe technically works is that it takes the output of the initial function (which is usually a tibble, which is the tidyverses modified version of a data.frame) and puts it automatically in the first input in the next function. This wont work for all functions but nearly all functions from the tidyverse collection of packages have a data set as the first input so it will work here. The benefit is that we dont need to keep saving the output from functions or specifying which data set to include in each function. As an example, well rewrite the previous code using a pipe. We start with our data.frame, which is normally the first thing we put in any dplyr function, and then immediately have a pipe %&gt;% into a dplyr function, which here is filter(). Now we dont need to say what the data set is because it takes the last thing that was piped into the function, which in our case is the entire data.frame ucr. After our filter() is done we have another pipe and go into select(). Now select() will use as its first input whatever is outputted from the filter(). So the input to select() will be the subsetted data output from filter(). We can have as many pipes as we wish, and chain many different dplyr functions together, but we just use two functions here so well end after our select() function. colorado &lt;- ucr %&gt;% filter( state == &quot;colorado&quot;, year %in% 2011:2017 ) %&gt;% select( actual_murder, actual_robbery_total, state, year, population, ori, agency_name ) If we check results using head(), we can see that this code is exactly the same as not using pipes. head(colorado) # actual_murder actual_robbery_total state year population ori agency_name # 1 7 80 colorado 2017 99940 CO00100 adams # 2 11 93 colorado 2016 100526 CO00100 adams # 3 6 68 colorado 2015 100266 CO00100 adams # 4 6 58 colorado 2014 98569 CO00100 adams # 5 7 44 colorado 2013 97146 CO00100 adams # 6 7 55 colorado 2012 93542 CO00100 adams The normal way to write code using pipes is to have a new line after the pipe and after each comma in filter() and select(). This doesnt change how the code works at all, but it is easier to read now because it has less code bunched together in a single line. colorado &lt;- ucr %&gt;% filter( state == &quot;colorado&quot;, year %in% 2011:2017 ) %&gt;% select( actual_murder, actual_robbery_total, state, year, population, ori, agency_name ) Pipes are technically from the magrittr package, but well just be using pipes in the context of using functions from dplyr or other tidyverse packages. "],["regular-expressions.html", "12 Regular Expressions 12.1 Finding patterns in text with grep() 12.2 Finding and replacing patterns in text with gsub() 12.3 Useful special characters 12.4 Changing capitalization", " 12 Regular Expressions Many word processing programs like Microsoft Word or Google Docs let you search for a pattern - usually a word or phrase - and it will show you where on the page that pattern appears. It also lets you replace that word or phrase with something new. R does the same using the function grep() to search for a pattern and tell you where in the data it appears, and gsub(), which lets you search for a pattern and then replace it with a new pattern. grep() - Find gsub() - Find and Replace The grep() function lets you find a pattern in the text and it will return a number saying which element has the pattern (in a data.frame this tells you which row has a match). gsub() lets you input a pattern to find and a pattern to replace it with, just like Find and Replace features elsewhere. You can remember the difference because gsub() has the word sub in it, and what it does is substitute text with new text. A useful cheat sheet on regular expressions is available here. For this lesson we will use a vector of 50 crime categories. These are all of the crimes in San Francisco Police data. As well see, there are some issues with the crime names that we need to fix. crimes &lt;- c( &quot;Arson&quot;, &quot;Assault&quot;, &quot;Burglary&quot;, &quot;Case Closure&quot;, &quot;Civil Sidewalks&quot;, &quot;Courtesy Report&quot;, &quot;Disorderly Conduct&quot;, &quot;Drug Offense&quot;, &quot;Drug Violation&quot;, &quot;Embezzlement&quot;, &quot;Family Offense&quot;, &quot;Fire Report&quot;, &quot;Forgery And Counterfeiting&quot;, &quot;Fraud&quot;, &quot;Gambling&quot;, &quot;Homicide&quot;, &quot;Human Trafficking (A), Commercial Sex Acts&quot;, &quot;Human Trafficking, Commercial Sex Acts&quot;, &quot;Juvenile Offenses&quot;, &quot;Larceny Theft&quot;, &quot;Liquor Laws&quot;, &quot;Lost Property&quot;, &quot;Malicious Mischief&quot;, &quot;Miscellaneous Investigation&quot;, &quot;Missing Person&quot;, &quot;Motor Vehicle Theft&quot;, &quot;Motor Vehicle Theft?&quot;, &quot;Non-Criminal&quot;, &quot;Offences Against The Family And Children&quot;, &quot;Other&quot;, &quot;Other Miscellaneous&quot;, &quot;Other Offenses&quot;, &quot;Prostitution&quot;, &quot;Rape&quot;, &quot;Recovered Vehicle&quot;, &quot;Robbery&quot;, &quot;Sex Offense&quot;, &quot;Stolen Property&quot;, &quot;Suicide&quot;, &quot;Suspicious&quot;, &quot;Suspicious Occ&quot;, &quot;Traffic Collision&quot;, &quot;Traffic Violation Arrest&quot;, &quot;Vandalism&quot;, &quot;Vehicle Impounded&quot;, &quot;Vehicle Misplaced&quot;, &quot;Warrant&quot;, &quot;Weapons Carrying Etc&quot;, &quot;Weapons Offence&quot;, &quot;Weapons Offense&quot; ) When looking closely at these crimes it is clear that some may overlap in certain categories such as theft, and there are several duplicates with slight differences in spelling. For example the last two crimes are Weapons Offence and Weapons Offense. These should be the same crime but the first one spelled offense wrong. And take a look at motor vehicle theft. There are two crimes here because one of them adds a question mark at the end for some reason. 12.1 Finding patterns in text with grep() Well start with grep() which allows us to search a vector of data (in R, columns in a data.frame operate the same as a vector) and find where there is a match for the pattern we want to look for. The syntax for grep() is grep(\"pattern\", data) where pattern is the pattern you are searching for, such as a if you want to find all values with the letter a. The pattern must always be in quotes. data is a vector of strings (such as crimes we made above or a column in a data.frame) that you are searching in to find the pattern. The output of this function is a number that says which element(s) in the vector the pattern was found in. If it returns, for example, the numbers 1 and 3 you know that the first and third element in your vector has the pattern - and that no other elements do. It is essentially returning the index where the conditional statement is this pattern present is true. So since our data is crimes our grep() function will be grep(\"\", crimes). What we put in the  is the pattern we want to search for. Lets start with the letter a. grep(&quot;a&quot;, crimes) # [1] 2 3 4 5 9 11 14 15 17 18 20 21 23 24 28 29 31 34 42 43 44 46 47 48 49 50 It gives us a bunch of numbers where the letter a is present in that element of crimes. This is useful for subsetting. We can use grep() to find all values that match a pattern we want and subset to keep just those values. crimes[grep(&quot;a&quot;, crimes)] # [1] &quot;Assault&quot; &quot;Burglary&quot; # [3] &quot;Case Closure&quot; &quot;Civil Sidewalks&quot; # [5] &quot;Drug Violation&quot; &quot;Family Offense&quot; # [7] &quot;Fraud&quot; &quot;Gambling&quot; # [9] &quot;Human Trafficking (A), Commercial Sex Acts&quot; &quot;Human Trafficking, Commercial Sex Acts&quot; # [11] &quot;Larceny Theft&quot; &quot;Liquor Laws&quot; # [13] &quot;Malicious Mischief&quot; &quot;Miscellaneous Investigation&quot; # [15] &quot;Non-Criminal&quot; &quot;Offences Against The Family And Children&quot; # [17] &quot;Other Miscellaneous&quot; &quot;Rape&quot; # [19] &quot;Traffic Collision&quot; &quot;Traffic Violation Arrest&quot; # [21] &quot;Vandalism&quot; &quot;Vehicle Misplaced&quot; # [23] &quot;Warrant&quot; &quot;Weapons Carrying Etc&quot; # [25] &quot;Weapons Offence&quot; &quot;Weapons Offense&quot; Searching for the letter a isnt that useful. Lets say we want to subset the data to only include theft-related crimes. From reading the list of crimes we can see there are multiple theft crimes - Larceny Theft, Motor Vehicle Theft, and Motor Vehicle Theft?. We may also want to include Stolen Property in this search, but well wait until later in this lesson for how to search for multiple patterns. Since those three crimes all have the word Theft in the name we can search for that pattern, and it will return only those crimes. grep(&quot;Theft&quot;, crimes) # [1] 20 26 27 crimes[grep(&quot;Theft&quot;, crimes)] # [1] &quot;Larceny Theft&quot; &quot;Motor Vehicle Theft&quot; &quot;Motor Vehicle Theft?&quot; A very useful parameter in grep() is value. When we set value to TRUE, it will print out the actual strings that are a match rather than the element number. While this prevents us from using it to subset (since R no longer knows which rows are a match), it is an excellent tool to check if the grep() was successful as we can visually confirm it returns what we want. When we start to learn about special characters that make the patterns more complicated, this will be important. grep(&quot;Theft&quot;, crimes, value = TRUE) # [1] &quot;Larceny Theft&quot; &quot;Motor Vehicle Theft&quot; &quot;Motor Vehicle Theft?&quot; Note that grep() (and gsub()) is case sensitive so you must capitalize properly. grep(&quot;theft&quot;, crimes, value = TRUE) # character(0) Setting the parameter ignore.case to be TRUE makes grep() ignore capitalization. grep(&quot;theft&quot;, crimes, value = TRUE, ignore.case = TRUE) # [1] &quot;Larceny Theft&quot; &quot;Motor Vehicle Theft&quot; &quot;Motor Vehicle Theft?&quot; If we want to find values that do not match with theft, we can set the parameter invert to TRUE. grep(&quot;theft&quot;, crimes, value = TRUE, ignore.case = TRUE, invert = TRUE) # [1] &quot;Arson&quot; &quot;Assault&quot; # [3] &quot;Burglary&quot; &quot;Case Closure&quot; # [5] &quot;Civil Sidewalks&quot; &quot;Courtesy Report&quot; # [7] &quot;Disorderly Conduct&quot; &quot;Drug Offense&quot; # [9] &quot;Drug Violation&quot; &quot;Embezzlement&quot; # [11] &quot;Family Offense&quot; &quot;Fire Report&quot; # [13] &quot;Forgery And Counterfeiting&quot; &quot;Fraud&quot; # [15] &quot;Gambling&quot; &quot;Homicide&quot; # [17] &quot;Human Trafficking (A), Commercial Sex Acts&quot; &quot;Human Trafficking, Commercial Sex Acts&quot; # [19] &quot;Juvenile Offenses&quot; &quot;Liquor Laws&quot; # [21] &quot;Lost Property&quot; &quot;Malicious Mischief&quot; # [23] &quot;Miscellaneous Investigation&quot; &quot;Missing Person&quot; # [25] &quot;Non-Criminal&quot; &quot;Offences Against The Family And Children&quot; # [27] &quot;Other&quot; &quot;Other Miscellaneous&quot; # [29] &quot;Other Offenses&quot; &quot;Prostitution&quot; # [31] &quot;Rape&quot; &quot;Recovered Vehicle&quot; # [33] &quot;Robbery&quot; &quot;Sex Offense&quot; # [35] &quot;Stolen Property&quot; &quot;Suicide&quot; # [37] &quot;Suspicious&quot; &quot;Suspicious Occ&quot; # [39] &quot;Traffic Collision&quot; &quot;Traffic Violation Arrest&quot; # [41] &quot;Vandalism&quot; &quot;Vehicle Impounded&quot; # [43] &quot;Vehicle Misplaced&quot; &quot;Warrant&quot; # [45] &quot;Weapons Carrying Etc&quot; &quot;Weapons Offence&quot; # [47] &quot;Weapons Offense&quot; 12.2 Finding and replacing patterns in text with gsub() gsub() takes patterns and replaces them with other patterns. An important use in criminology for gsub() is to fix spelling mistakes in the text, such as the way offense was spelled wrong in our data. This will be a standard part of your data cleaning process and is important as a misspelled word can cause significant issues. For example if our previous example of marijuana legalization in Colorado had half of agencies misspelling the name Colorado, aggregating the data by the state (or simply subsetting to just Colorado agencies) would give completely different results as youd lose half your data. gsub() is also useful when you want to take subcategories and change the value to larger categories. For example we could take any crime with the word Theft in it and change the whole crime name to Theft. In our data that would take 3 subcategories of thefts and turn it into a larger category we could aggregate to. This will be useful in city-level data where you may only care about a certain type of crime but it has many subcategories that you need to aggregate. The syntax of gsub() is similar to grep() with the addition of a pattern to replace the pattern we found. gsub(\"find_pattern\", \"replace_pattern\", data) Lets start with a simple example of finding the letter a and replacing it with z. Our data will be the word cat. gsub(&quot;a&quot;, &quot;z&quot;, &quot;cat&quot;) # [1] &quot;czt&quot; Like grep(), gsub() is case sensitive and has the parameter ignore.case to ignore capitalization. gsub(&quot;A&quot;, &quot;z&quot;, &quot;cat&quot;) # [1] &quot;cat&quot; gsub(&quot;A&quot;, &quot;z&quot;, &quot;cat&quot;, ignore.case = TRUE) # [1] &quot;czt&quot; gsub() returns the same data you input but with the pattern already replaced. Above you can see that when using capital A, it returns cat unchanged as it never found the pattern. When ignore.case was set to TRUE it returned czt as it then matched to letter A. We can use gsub() to replace some issues in the crimes data, such as Offense being spelled Offence. gsub(&quot;Offence&quot;, &quot;Offense&quot;, crimes) # [1] &quot;Arson&quot; &quot;Assault&quot; # [3] &quot;Burglary&quot; &quot;Case Closure&quot; # [5] &quot;Civil Sidewalks&quot; &quot;Courtesy Report&quot; # [7] &quot;Disorderly Conduct&quot; &quot;Drug Offense&quot; # [9] &quot;Drug Violation&quot; &quot;Embezzlement&quot; # [11] &quot;Family Offense&quot; &quot;Fire Report&quot; # [13] &quot;Forgery And Counterfeiting&quot; &quot;Fraud&quot; # [15] &quot;Gambling&quot; &quot;Homicide&quot; # [17] &quot;Human Trafficking (A), Commercial Sex Acts&quot; &quot;Human Trafficking, Commercial Sex Acts&quot; # [19] &quot;Juvenile Offenses&quot; &quot;Larceny Theft&quot; # [21] &quot;Liquor Laws&quot; &quot;Lost Property&quot; # [23] &quot;Malicious Mischief&quot; &quot;Miscellaneous Investigation&quot; # [25] &quot;Missing Person&quot; &quot;Motor Vehicle Theft&quot; # [27] &quot;Motor Vehicle Theft?&quot; &quot;Non-Criminal&quot; # [29] &quot;Offenses Against The Family And Children&quot; &quot;Other&quot; # [31] &quot;Other Miscellaneous&quot; &quot;Other Offenses&quot; # [33] &quot;Prostitution&quot; &quot;Rape&quot; # [35] &quot;Recovered Vehicle&quot; &quot;Robbery&quot; # [37] &quot;Sex Offense&quot; &quot;Stolen Property&quot; # [39] &quot;Suicide&quot; &quot;Suspicious&quot; # [41] &quot;Suspicious Occ&quot; &quot;Traffic Collision&quot; # [43] &quot;Traffic Violation Arrest&quot; &quot;Vandalism&quot; # [45] &quot;Vehicle Impounded&quot; &quot;Vehicle Misplaced&quot; # [47] &quot;Warrant&quot; &quot;Weapons Carrying Etc&quot; # [49] &quot;Weapons Offense&quot; &quot;Weapons Offense&quot; A useful pattern is an empty string  which says replace whatever the find_pattern is with nothing, deleting it. Lets delete the letter a (lowercase only) from the data. gsub(&quot;a&quot;, &quot;&quot;, crimes) # [1] &quot;Arson&quot; &quot;Assult&quot; # [3] &quot;Burglry&quot; &quot;Cse Closure&quot; # [5] &quot;Civil Sidewlks&quot; &quot;Courtesy Report&quot; # [7] &quot;Disorderly Conduct&quot; &quot;Drug Offense&quot; # [9] &quot;Drug Violtion&quot; &quot;Embezzlement&quot; # [11] &quot;Fmily Offense&quot; &quot;Fire Report&quot; # [13] &quot;Forgery And Counterfeiting&quot; &quot;Frud&quot; # [15] &quot;Gmbling&quot; &quot;Homicide&quot; # [17] &quot;Humn Trfficking (A), Commercil Sex Acts&quot; &quot;Humn Trfficking, Commercil Sex Acts&quot; # [19] &quot;Juvenile Offenses&quot; &quot;Lrceny Theft&quot; # [21] &quot;Liquor Lws&quot; &quot;Lost Property&quot; # [23] &quot;Mlicious Mischief&quot; &quot;Miscellneous Investigtion&quot; # [25] &quot;Missing Person&quot; &quot;Motor Vehicle Theft&quot; # [27] &quot;Motor Vehicle Theft?&quot; &quot;Non-Criminl&quot; # [29] &quot;Offences Aginst The Fmily And Children&quot; &quot;Other&quot; # [31] &quot;Other Miscellneous&quot; &quot;Other Offenses&quot; # [33] &quot;Prostitution&quot; &quot;Rpe&quot; # [35] &quot;Recovered Vehicle&quot; &quot;Robbery&quot; # [37] &quot;Sex Offense&quot; &quot;Stolen Property&quot; # [39] &quot;Suicide&quot; &quot;Suspicious&quot; # [41] &quot;Suspicious Occ&quot; &quot;Trffic Collision&quot; # [43] &quot;Trffic Violtion Arrest&quot; &quot;Vndlism&quot; # [45] &quot;Vehicle Impounded&quot; &quot;Vehicle Misplced&quot; # [47] &quot;Wrrnt&quot; &quot;Wepons Crrying Etc&quot; # [49] &quot;Wepons Offence&quot; &quot;Wepons Offense&quot; 12.3 Useful special characters So far, we have just searched for a single character or word and expected a return only if an exact match was found. Now well discuss a number of characters called special characters that allow us to make more complex grep() and gsub() pattern searches. 12.3.1 Multiple characters [] To search for multiple matches we can put the pattern we want to search for inside square brackets [] (note that we use the same square brackets for subsetting, but they operate very differently in this context). For example, we can find all the crimes that contain the letters x, y, or z. The grep() searches if any of the letters inside of the [] are present in our crimes vector. grep(&quot;[xyz]&quot;, crimes, value = TRUE) # [1] &quot;Burglary&quot; &quot;Courtesy Report&quot; # [3] &quot;Disorderly Conduct&quot; &quot;Embezzlement&quot; # [5] &quot;Family Offense&quot; &quot;Forgery And Counterfeiting&quot; # [7] &quot;Human Trafficking (A), Commercial Sex Acts&quot; &quot;Human Trafficking, Commercial Sex Acts&quot; # [9] &quot;Larceny Theft&quot; &quot;Lost Property&quot; # [11] &quot;Offences Against The Family And Children&quot; &quot;Robbery&quot; # [13] &quot;Sex Offense&quot; &quot;Stolen Property&quot; # [15] &quot;Weapons Carrying Etc&quot; As it searches for any letter inside of the square brackets, the order does not matter. grep(&quot;[zyx]&quot;, crimes, value = TRUE) # [1] &quot;Burglary&quot; &quot;Courtesy Report&quot; # [3] &quot;Disorderly Conduct&quot; &quot;Embezzlement&quot; # [5] &quot;Family Offense&quot; &quot;Forgery And Counterfeiting&quot; # [7] &quot;Human Trafficking (A), Commercial Sex Acts&quot; &quot;Human Trafficking, Commercial Sex Acts&quot; # [9] &quot;Larceny Theft&quot; &quot;Lost Property&quot; # [11] &quot;Offences Against The Family And Children&quot; &quot;Robbery&quot; # [13] &quot;Sex Offense&quot; &quot;Stolen Property&quot; # [15] &quot;Weapons Carrying Etc&quot; This also works for numbers though we do not have any numbers in the data. grep(&quot;[01234567890]&quot;, crimes, value = TRUE) # character(0) If we wanted to search for a pattern, such as vowels, that is repeated we could put multiple [] patterns together. We will see another way to search for a repeated pattern soon. grep(&quot;[aeiou][aeiou][aeiou]&quot;, crimes, value = TRUE) # [1] &quot;Malicious Mischief&quot; &quot;Miscellaneous Investigation&quot; &quot;Other Miscellaneous&quot; # [4] &quot;Suspicious&quot; &quot;Suspicious Occ&quot; Inside the [] we can also use the dash sign - to make intervals between certain values. For numbers, n-m means any number between n and m (inclusive). For letters, a-z means all lowercase letters and A-Z means all uppercase letters in that range (inclusive). grep(&quot;[x-z]&quot;, crimes, value = TRUE) # [1] &quot;Burglary&quot; &quot;Courtesy Report&quot; # [3] &quot;Disorderly Conduct&quot; &quot;Embezzlement&quot; # [5] &quot;Family Offense&quot; &quot;Forgery And Counterfeiting&quot; # [7] &quot;Human Trafficking (A), Commercial Sex Acts&quot; &quot;Human Trafficking, Commercial Sex Acts&quot; # [9] &quot;Larceny Theft&quot; &quot;Lost Property&quot; # [11] &quot;Offences Against The Family And Children&quot; &quot;Robbery&quot; # [13] &quot;Sex Offense&quot; &quot;Stolen Property&quot; # [15] &quot;Weapons Carrying Etc&quot; 12.3.2 n-many of previous character {n} {n} means the preceding item will be matched exactly n times. We can use it to rewrite the above grep() to say the values in the [] should be repeated three times. grep(&quot;[aeiou]{3}&quot;, crimes, value = TRUE) # [1] &quot;Malicious Mischief&quot; &quot;Miscellaneous Investigation&quot; &quot;Other Miscellaneous&quot; # [4] &quot;Suspicious&quot; &quot;Suspicious Occ&quot; 12.3.3 n-many to m-many of previous character {n,m} While {n} says the previous character (or characters inside a []) must be present exactly n times, we can allow a range by using {n,m}. Here the previous character must be present between n and m times (inclusive). We can check for values where there are 2-3 vowels in a row. Note that there cannot be a space before or after the comma. grep(&quot;[aeiou]{2,3}&quot;, crimes, value = TRUE) # [1] &quot;Assault&quot; &quot;Courtesy Report&quot; # [3] &quot;Drug Violation&quot; &quot;Forgery And Counterfeiting&quot; # [5] &quot;Fraud&quot; &quot;Human Trafficking (A), Commercial Sex Acts&quot; # [7] &quot;Human Trafficking, Commercial Sex Acts&quot; &quot;Liquor Laws&quot; # [9] &quot;Malicious Mischief&quot; &quot;Miscellaneous Investigation&quot; # [11] &quot;Offences Against The Family And Children&quot; &quot;Other Miscellaneous&quot; # [13] &quot;Prostitution&quot; &quot;Suicide&quot; # [15] &quot;Suspicious&quot; &quot;Suspicious Occ&quot; # [17] &quot;Traffic Collision&quot; &quot;Traffic Violation Arrest&quot; # [19] &quot;Vehicle Impounded&quot; &quot;Weapons Carrying Etc&quot; # [21] &quot;Weapons Offence&quot; &quot;Weapons Offense&quot; If we wanted only crimes with exactly three vowels in a row wed use {3,3}. grep(&quot;[aeiou]{3,3}&quot;, crimes, value = TRUE) # [1] &quot;Malicious Mischief&quot; &quot;Miscellaneous Investigation&quot; &quot;Other Miscellaneous&quot; # [4] &quot;Suspicious&quot; &quot;Suspicious Occ&quot; If we leave n blank, such as {,m}, it says, previous character must be present up to m times. grep(&quot;[aeiou]{,3}&quot;, crimes, value = TRUE) # [1] &quot;Arson&quot; &quot;Assault&quot; # [3] &quot;Burglary&quot; &quot;Case Closure&quot; # [5] &quot;Civil Sidewalks&quot; &quot;Courtesy Report&quot; # [7] &quot;Disorderly Conduct&quot; &quot;Drug Offense&quot; # [9] &quot;Drug Violation&quot; &quot;Embezzlement&quot; # [11] &quot;Family Offense&quot; &quot;Fire Report&quot; # [13] &quot;Forgery And Counterfeiting&quot; &quot;Fraud&quot; # [15] &quot;Gambling&quot; &quot;Homicide&quot; # [17] &quot;Human Trafficking (A), Commercial Sex Acts&quot; &quot;Human Trafficking, Commercial Sex Acts&quot; # [19] &quot;Juvenile Offenses&quot; &quot;Larceny Theft&quot; # [21] &quot;Liquor Laws&quot; &quot;Lost Property&quot; # [23] &quot;Malicious Mischief&quot; &quot;Miscellaneous Investigation&quot; # [25] &quot;Missing Person&quot; &quot;Motor Vehicle Theft&quot; # [27] &quot;Motor Vehicle Theft?&quot; &quot;Non-Criminal&quot; # [29] &quot;Offences Against The Family And Children&quot; &quot;Other&quot; # [31] &quot;Other Miscellaneous&quot; &quot;Other Offenses&quot; # [33] &quot;Prostitution&quot; &quot;Rape&quot; # [35] &quot;Recovered Vehicle&quot; &quot;Robbery&quot; # [37] &quot;Sex Offense&quot; &quot;Stolen Property&quot; # [39] &quot;Suicide&quot; &quot;Suspicious&quot; # [41] &quot;Suspicious Occ&quot; &quot;Traffic Collision&quot; # [43] &quot;Traffic Violation Arrest&quot; &quot;Vandalism&quot; # [45] &quot;Vehicle Impounded&quot; &quot;Vehicle Misplaced&quot; # [47] &quot;Warrant&quot; &quot;Weapons Carrying Etc&quot; # [49] &quot;Weapons Offence&quot; &quot;Weapons Offense&quot; This returns every crime as up to m times includes zero times. And the same works for leaving m blank, but it will be present at least n times. grep(&quot;[aeiou]{3,}&quot;, crimes, value = TRUE) # [1] &quot;Malicious Mischief&quot; &quot;Miscellaneous Investigation&quot; &quot;Other Miscellaneous&quot; # [4] &quot;Suspicious&quot; &quot;Suspicious Occ&quot; 12.3.4 Start of string The ^ symbol (called a caret) signifies that what follows it is the start of the string. We put the ^ at the beginning of the quotes and then anything that follows it must be the very start of the string. As an example lets search for Family. Our data has both the Family Offense crime and the Offences Against The Family And Children crime (which likely are the same crime written differently). If we use ^ then we should only have the first one returned. grep(&quot;^Family&quot;, crimes, value = TRUE) # [1] &quot;Family Offense&quot; 12.3.5 End of string $ The dollar sign $ acts similar to the caret ^ except that it signifies that the value before it is the end of the string. We put the $ at the very end of our search pattern and whatever character is before it is the end of the string. For example, lets search for all crimes that end with the word Theft. grep(&quot;Theft$&quot;, crimes, value = TRUE) # [1] &quot;Larceny Theft&quot; &quot;Motor Vehicle Theft&quot; Note that the crime Motor Vehicle Theft? doesnt get selected as it ends with a question mark. 12.3.6 Anything . The . symbol is a stand-in for any value. This is useful when you arent sure about every part of the pattern you are searching. It can also be used when there are slight differences in words such as our incorrect Offence and Offense. We can replace the c and s with the .. grep(&quot;Weapons Offen.e&quot;, crimes, value = TRUE) # [1] &quot;Weapons Offence&quot; &quot;Weapons Offense&quot; 12.3.7 One or more of previous + The + means that the character immediately before it is present at least one time. This is the same as writing {1,}. If we wanted to find all values with only two words, we would start with some number of letters followed by a space followed by some more letters and the string would end. grep(&quot;^[A-Za-z]+ [A-Za-z]+$&quot;, crimes, value = TRUE) # [1] &quot;Case Closure&quot; &quot;Civil Sidewalks&quot; &quot;Courtesy Report&quot; # [4] &quot;Disorderly Conduct&quot; &quot;Drug Offense&quot; &quot;Drug Violation&quot; # [7] &quot;Family Offense&quot; &quot;Fire Report&quot; &quot;Juvenile Offenses&quot; # [10] &quot;Larceny Theft&quot; &quot;Liquor Laws&quot; &quot;Lost Property&quot; # [13] &quot;Malicious Mischief&quot; &quot;Miscellaneous Investigation&quot; &quot;Missing Person&quot; # [16] &quot;Other Miscellaneous&quot; &quot;Other Offenses&quot; &quot;Recovered Vehicle&quot; # [19] &quot;Sex Offense&quot; &quot;Stolen Property&quot; &quot;Suspicious Occ&quot; # [22] &quot;Traffic Collision&quot; &quot;Vehicle Impounded&quot; &quot;Vehicle Misplaced&quot; # [25] &quot;Weapons Offence&quot; &quot;Weapons Offense&quot; 12.3.8 Zero or more of previous * The * special character says match zero or more of the previous character and is the same as {0,}. Combining . with * is powerful when used in gsub() to delete text before or after a pattern. Lets write a pattern that searches the text for the word Weapons and then deletes any text after that. Our pattern would be Weapons.* which is the word Weapons followed by anything zero or more times. gsub(&quot;Weapons.*&quot;, &quot;Weapons&quot;, crimes) # [1] &quot;Arson&quot; &quot;Assault&quot; # [3] &quot;Burglary&quot; &quot;Case Closure&quot; # [5] &quot;Civil Sidewalks&quot; &quot;Courtesy Report&quot; # [7] &quot;Disorderly Conduct&quot; &quot;Drug Offense&quot; # [9] &quot;Drug Violation&quot; &quot;Embezzlement&quot; # [11] &quot;Family Offense&quot; &quot;Fire Report&quot; # [13] &quot;Forgery And Counterfeiting&quot; &quot;Fraud&quot; # [15] &quot;Gambling&quot; &quot;Homicide&quot; # [17] &quot;Human Trafficking (A), Commercial Sex Acts&quot; &quot;Human Trafficking, Commercial Sex Acts&quot; # [19] &quot;Juvenile Offenses&quot; &quot;Larceny Theft&quot; # [21] &quot;Liquor Laws&quot; &quot;Lost Property&quot; # [23] &quot;Malicious Mischief&quot; &quot;Miscellaneous Investigation&quot; # [25] &quot;Missing Person&quot; &quot;Motor Vehicle Theft&quot; # [27] &quot;Motor Vehicle Theft?&quot; &quot;Non-Criminal&quot; # [29] &quot;Offences Against The Family And Children&quot; &quot;Other&quot; # [31] &quot;Other Miscellaneous&quot; &quot;Other Offenses&quot; # [33] &quot;Prostitution&quot; &quot;Rape&quot; # [35] &quot;Recovered Vehicle&quot; &quot;Robbery&quot; # [37] &quot;Sex Offense&quot; &quot;Stolen Property&quot; # [39] &quot;Suicide&quot; &quot;Suspicious&quot; # [41] &quot;Suspicious Occ&quot; &quot;Traffic Collision&quot; # [43] &quot;Traffic Violation Arrest&quot; &quot;Vandalism&quot; # [45] &quot;Vehicle Impounded&quot; &quot;Vehicle Misplaced&quot; # [47] &quot;Warrant&quot; &quot;Weapons&quot; # [49] &quot;Weapons&quot; &quot;Weapons&quot; And now our last three crimes are all the same. 12.3.9 Multiple patterns | The vertical bar | special character allows us to check for multiple patterns. It essentially functions as pattern A or Pattern B with the | symbol replacing the word or (and making sure to not have any space between patterns.). To check our crimes for the word Drug or the word Weapons, we could write Drug|Weapon, which searches for Drug or Weapons in the text. grep(&quot;Drug|Weapons&quot;, crimes, value = TRUE) # [1] &quot;Drug Offense&quot; &quot;Drug Violation&quot; &quot;Weapons Carrying Etc&quot; &quot;Weapons Offence&quot; # [5] &quot;Weapons Offense&quot; 12.3.10 Parentheses () Parentheses act similar to the square brackets [], where we want everything inside but with parentheses the values must be in the proper order. grep(&quot;(Offense)&quot;, crimes, value = TRUE) # [1] &quot;Drug Offense&quot; &quot;Family Offense&quot; &quot;Juvenile Offenses&quot; &quot;Other Offenses&quot; &quot;Sex Offense&quot; # [6] &quot;Weapons Offense&quot; Running the above code returns the same results as if we didnt include the parentheses. The usefulness of parentheses comes when combining it with the | symbol to be able to check (X|Y) Z), which says, look for either X or Y which must be followed by Z. Running just (Offense) returns values for multiple types of offenses. Lets say we just care about Drug and Weapon Offenses. We can search for Offense normally and combine () and | to say, search for either the word Drug or the word Family and they should be followed by the word Offense. grep(&quot;(Drug|Weapons) Offense&quot;, crimes, value = TRUE) # [1] &quot;Drug Offense&quot; &quot;Weapons Offense&quot; 12.3.11 Optional text ? The question mark indicates that the character immediately before the ? is optional. Lets search for the term offens and add a ? at the end. This says search for the pattern offen, and we expect an exact match for that pattern. And if the letter s follows offen return that too, but it isnt required to be there. grep(&quot;Offens?&quot;, crimes, value = TRUE) # [1] &quot;Drug Offense&quot; &quot;Family Offense&quot; # [3] &quot;Juvenile Offenses&quot; &quot;Offences Against The Family And Children&quot; # [5] &quot;Other Offenses&quot; &quot;Sex Offense&quot; # [7] &quot;Weapons Offence&quot; &quot;Weapons Offense&quot; We can further combine it with () and | to get both spellings of Weapon Offense. grep(&quot;(Drug|Weapons) Offens?&quot;, crimes, value = TRUE) # [1] &quot;Drug Offense&quot; &quot;Weapons Offence&quot; &quot;Weapons Offense&quot; 12.4 Changing capitalization If youre dealing with data where the only difference is capitalization (as is common in crime data) instead of using gsub() to change individual values, you can use the functions toupper() and tolower() to change every letters capitalization. These functions take as an input a vector of strings (or a column from a data.frame) and return those strings either upper or lowercase. toupper(crimes) # [1] &quot;ARSON&quot; &quot;ASSAULT&quot; # [3] &quot;BURGLARY&quot; &quot;CASE CLOSURE&quot; # [5] &quot;CIVIL SIDEWALKS&quot; &quot;COURTESY REPORT&quot; # [7] &quot;DISORDERLY CONDUCT&quot; &quot;DRUG OFFENSE&quot; # [9] &quot;DRUG VIOLATION&quot; &quot;EMBEZZLEMENT&quot; # [11] &quot;FAMILY OFFENSE&quot; &quot;FIRE REPORT&quot; # [13] &quot;FORGERY AND COUNTERFEITING&quot; &quot;FRAUD&quot; # [15] &quot;GAMBLING&quot; &quot;HOMICIDE&quot; # [17] &quot;HUMAN TRAFFICKING (A), COMMERCIAL SEX ACTS&quot; &quot;HUMAN TRAFFICKING, COMMERCIAL SEX ACTS&quot; # [19] &quot;JUVENILE OFFENSES&quot; &quot;LARCENY THEFT&quot; # [21] &quot;LIQUOR LAWS&quot; &quot;LOST PROPERTY&quot; # [23] &quot;MALICIOUS MISCHIEF&quot; &quot;MISCELLANEOUS INVESTIGATION&quot; # [25] &quot;MISSING PERSON&quot; &quot;MOTOR VEHICLE THEFT&quot; # [27] &quot;MOTOR VEHICLE THEFT?&quot; &quot;NON-CRIMINAL&quot; # [29] &quot;OFFENCES AGAINST THE FAMILY AND CHILDREN&quot; &quot;OTHER&quot; # [31] &quot;OTHER MISCELLANEOUS&quot; &quot;OTHER OFFENSES&quot; # [33] &quot;PROSTITUTION&quot; &quot;RAPE&quot; # [35] &quot;RECOVERED VEHICLE&quot; &quot;ROBBERY&quot; # [37] &quot;SEX OFFENSE&quot; &quot;STOLEN PROPERTY&quot; # [39] &quot;SUICIDE&quot; &quot;SUSPICIOUS&quot; # [41] &quot;SUSPICIOUS OCC&quot; &quot;TRAFFIC COLLISION&quot; # [43] &quot;TRAFFIC VIOLATION ARREST&quot; &quot;VANDALISM&quot; # [45] &quot;VEHICLE IMPOUNDED&quot; &quot;VEHICLE MISPLACED&quot; # [47] &quot;WARRANT&quot; &quot;WEAPONS CARRYING ETC&quot; # [49] &quot;WEAPONS OFFENCE&quot; &quot;WEAPONS OFFENSE&quot; tolower(crimes) # [1] &quot;arson&quot; &quot;assault&quot; # [3] &quot;burglary&quot; &quot;case closure&quot; # [5] &quot;civil sidewalks&quot; &quot;courtesy report&quot; # [7] &quot;disorderly conduct&quot; &quot;drug offense&quot; # [9] &quot;drug violation&quot; &quot;embezzlement&quot; # [11] &quot;family offense&quot; &quot;fire report&quot; # [13] &quot;forgery and counterfeiting&quot; &quot;fraud&quot; # [15] &quot;gambling&quot; &quot;homicide&quot; # [17] &quot;human trafficking (a), commercial sex acts&quot; &quot;human trafficking, commercial sex acts&quot; # [19] &quot;juvenile offenses&quot; &quot;larceny theft&quot; # [21] &quot;liquor laws&quot; &quot;lost property&quot; # [23] &quot;malicious mischief&quot; &quot;miscellaneous investigation&quot; # [25] &quot;missing person&quot; &quot;motor vehicle theft&quot; # [27] &quot;motor vehicle theft?&quot; &quot;non-criminal&quot; # [29] &quot;offences against the family and children&quot; &quot;other&quot; # [31] &quot;other miscellaneous&quot; &quot;other offenses&quot; # [33] &quot;prostitution&quot; &quot;rape&quot; # [35] &quot;recovered vehicle&quot; &quot;robbery&quot; # [37] &quot;sex offense&quot; &quot;stolen property&quot; # [39] &quot;suicide&quot; &quot;suspicious&quot; # [41] &quot;suspicious occ&quot; &quot;traffic collision&quot; # [43] &quot;traffic violation arrest&quot; &quot;vandalism&quot; # [45] &quot;vehicle impounded&quot; &quot;vehicle misplaced&quot; # [47] &quot;warrant&quot; &quot;weapons carrying etc&quot; # [49] &quot;weapons offence&quot; &quot;weapons offense&quot; "],["reshaping.html", "13 Reshaping data 13.1 Reshaping a single column 13.2 Reshaping multiple columns", " 13 Reshaping data For this chapter youll need the following file, which is available for download here: sqf-2019.xlsx. This file was initially downloaded from the New York City Police Departments page here. When youre using data for research, the end result is usually a regression or a graph (or both), and that requires your data to be in a particular format. Usually your data should have one row for each unit of analysis, and each column should have information about that unit. As an example, if you wanted to study city-level crime over time, youd have each row be a single city in a single time period. If you looked at 10 different time periods, say 10 years, youd have 10 rows for each city. And each column would have information about that city in that time period, such as the number of murders that occurred. This is what is known as long format, as this data often has many rows and few columns. The alternative is whats known as wide format, where each row (in our example) is a single agency and youd have 10 times as many columns in long data as there are 10 time periods. Whereas with long data youd have, for example, a murder column and 10 rows showing the murder in each year, with wide data youd have 10 murder columns showing the value for each years murder count. While long data is more commonly used in criminology research, some statistical and graphing approaches require wide data. If this seems like an abstract concept, bear with it for a bit as well go over several examples in this chapter. Ideally, our data comes in the exact format we need. In cases where it doesnt, we need to be able to convert the data from long to wide or from wide to long format. This conversion is called reshaping. There are many approaches and packages to doing reshaping in R, which makes reshaping actually one of the most confusing things to do in R. In this chapter well use an approach from the package tidyr, which, as the name suggests, is one of the packages from the tidyverse. To use this package we need to install it using install.packages(). install.packages(&quot;tidyr&quot;) For this chapter well use microdata from the New York City Police Department for every stop and frisk they conducted in 2019. The data is in an .xlsx format so we can use the readxl package to read it into R. Lets call the data sqf as it is the abbreviation for stop, question, and frisk, which is the full name of the data set. Following traditional R naming conventions, well keep the object name lowercased. library(readxl) sqf &lt;- read_excel(&quot;data/sqf-2019.xlsx&quot;) The first thing we want to do when loading in a new data set to R is to look at it. We can do this a few different ways, including using View(), which opens up an Excel-like tab where we can scroll through the data, and through head() which prints the first 6 rows of every column to the console. The readxl package converts the data into a tibble. which is essentially a modified data.frame. One of the modifications is that it doesnt print out every single column when we use head() to view the first 6 rows of each column. Thats because tibbles dont want to print large amounts of text to the console. Normally wed use head() to look at the data, but as there are dozens of columns in this data, I dont want to print out the first six rows of every column to the console as it will take up a lot of space in the book. If this were real data you were working on, you would use head() or View() to look at the data. Instead, here well use names() to see what columns are in the data. Lets also use nrow() to see how many rows of data we have. sqf &lt;- data.frame(sqf) names(sqf) # [1] &quot;STOP_ID_ANONY&quot; # [2] &quot;STOP_FRISK_DATE&quot; # [3] &quot;STOP_FRISK_TIME&quot; # [4] &quot;YEAR2&quot; # [5] &quot;MONTH2&quot; # [6] &quot;DAY2&quot; # [7] &quot;STOP_WAS_INITIATED&quot; # [8] &quot;RECORD_STATUS_CODE&quot; # [9] &quot;ISSUING_OFFICER_RANK&quot; # [10] &quot;ISSUING_OFFICER_COMMAND_CODE&quot; # [11] &quot;SUPERVISING_OFFICER_RANK&quot; # [12] &quot;SUPERVISING_OFFICER_COMMAND_CODE&quot; # [13] &quot;LOCATION_IN_OUT_CODE&quot; # [14] &quot;JURISDICTION_CODE&quot; # [15] &quot;JURISDICTION_DESCRIPTION&quot; # [16] &quot;OBSERVED_DURATION_MINUTES&quot; # [17] &quot;SUSPECTED_CRIME_DESCRIPTION&quot; # [18] &quot;STOP_DURATION_MINUTES&quot; # [19] &quot;OFFICER_EXPLAINED_STOP_FLAG&quot; # [20] &quot;OFFICER_NOT_EXPLAINED_STOP_DESCRIPTION&quot; # [21] &quot;OTHER_PERSON_STOPPED_FLAG&quot; # [22] &quot;SUSPECT_ARRESTED_FLAG&quot; # [23] &quot;SUSPECT_ARREST_OFFENSE&quot; # [24] &quot;SUMMONS_ISSUED_FLAG&quot; # [25] &quot;SUMMONS_OFFENSE_DESCRIPTION&quot; # [26] &quot;OFFICER_IN_UNIFORM_FLAG&quot; # [27] &quot;ID_CARD_IDENTIFIES_OFFICER_FLAG&quot; # [28] &quot;SHIELD_IDENTIFIES_OFFICER_FLAG&quot; # [29] &quot;VERBAL_IDENTIFIES_OFFICER_FLAG&quot; # [30] &quot;FRISKED_FLAG&quot; # [31] &quot;SEARCHED_FLAG&quot; # [32] &quot;ASK_FOR_CONSENT_FLG&quot; # [33] &quot;CONSENT_GIVEN_FLG&quot; # [34] &quot;OTHER_CONTRABAND_FLAG&quot; # [35] &quot;FIREARM_FLAG&quot; # [36] &quot;KNIFE_CUTTER_FLAG&quot; # [37] &quot;OTHER_WEAPON_FLAG&quot; # [38] &quot;WEAPON_FOUND_FLAG&quot; # [39] &quot;PHYSICAL_FORCE_CEW_FLAG&quot; # [40] &quot;PHYSICAL_FORCE_DRAW_POINT_FIREARM_FLAG&quot; # [41] &quot;PHYSICAL_FORCE_HANDCUFF_SUSPECT_FLAG&quot; # [42] &quot;PHYSICAL_FORCE_OC_SPRAY_USED_FLAG&quot; # [43] &quot;PHYSICAL_FORCE_OTHER_FLAG&quot; # [44] &quot;PHYSICAL_FORCE_RESTRAINT_USED_FLAG&quot; # [45] &quot;PHYSICAL_FORCE_VERBAL_INSTRUCTION_FLAG&quot; # [46] &quot;PHYSICAL_FORCE_WEAPON_IMPACT_FLAG&quot; # [47] &quot;BACKROUND_CIRCUMSTANCES_VIOLENT_CRIME_FLAG&quot; # [48] &quot;BACKROUND_CIRCUMSTANCES_SUSPECT_KNOWN_TO_CARRY_WEAPON_FLAG&quot; # [49] &quot;SUSPECTS_ACTIONS_CASING_FLAG&quot; # [50] &quot;SUSPECTS_ACTIONS_CONCEALED_POSSESSION_WEAPON_FLAG&quot; # [51] &quot;SUSPECTS_ACTIONS_DECRIPTION_FLAG&quot; # [52] &quot;SUSPECTS_ACTIONS_DRUG_TRANSACTIONS_FLAG&quot; # [53] &quot;SUSPECTS_ACTIONS_IDENTIFY_CRIME_PATTERN_FLAG&quot; # [54] &quot;SUSPECTS_ACTIONS_LOOKOUT_FLAG&quot; # [55] &quot;SUSPECTS_ACTIONS_OTHER_FLAG&quot; # [56] &quot;SUSPECTS_ACTIONS_PROXIMITY_TO_SCENE_FLAG&quot; # [57] &quot;SEARCH_BASIS_ADMISSION_FLAG&quot; # [58] &quot;SEARCH_BASIS_CONSENT_FLAG&quot; # [59] &quot;SEARCH_BASIS_HARD_OBJECT_FLAG&quot; # [60] &quot;SEARCH_BASIS_INCIDENTAL_TO_ARREST_FLAG&quot; # [61] &quot;SEARCH_BASIS_OTHER_FLAG&quot; # [62] &quot;SEARCH_BASIS_OUTLINE_FLAG&quot; # [63] &quot;DEMEANOR_CODE&quot; # [64] &quot;DEMEANOR_OF_PERSON_STOPPED&quot; # [65] &quot;SUSPECT_REPORTED_AGE&quot; # [66] &quot;SUSPECT_SEX&quot; # [67] &quot;SUSPECT_RACE_DESCRIPTION&quot; # [68] &quot;SUSPECT_HEIGHT&quot; # [69] &quot;SUSPECT_WEIGHT&quot; # [70] &quot;SUSPECT_BODY_BUILD_TYPE&quot; # [71] &quot;SUSPECT_EYE_COLOR&quot; # [72] &quot;SUSPECT_HAIR_COLOR&quot; # [73] &quot;SUSPECT_OTHER_DESCRIPTION&quot; # [74] &quot;STOP_LOCATION_PRECINCT&quot; # [75] &quot;STOP_LOCATION_SECTOR_CODE&quot; # [76] &quot;STOP_LOCATION_APARTMENT&quot; # [77] &quot;STOP_LOCATION_FULL_ADDRESS&quot; # [78] &quot;STOP_LOCATION_STREET_NAME&quot; # [79] &quot;STOP_LOCATION_X&quot; # [80] &quot;STOP_LOCATION_Y&quot; # [81] &quot;STOP_LOCATION_ZIP_CODE&quot; # [82] &quot;STOP_LOCATION_PATROL_BORO_NAME&quot; # [83] &quot;STOP_LOCATION_BORO_NAME&quot; nrow(sqf) # [1] 13459 Each row of data is a stop and frisk, and there were 13,459 in 2019. That number may seem low to you, especially if youve read articles about how frequent stop and frisks happens in New York City. It is correct - at least correct in terms of reported stops. Stop and frisks in New York City peaked in 2011 with nearly 700,000 conducted, and then fell sharply after that to fewer than 23,000 from 2015 through 2019 (the last year available at the time of this writing). Looking at the results of names(), we can see that this is a rich data set with lots of information about each stop (though many columns also have missing data, so it is not as rich as it initially appears). Each stop has, for example, the date and time of the stop, whether an arrest was made and for what, physical characteristics (race, sex, age, height, weight, clothing) of the person stopped, and the location of the stop. One important variable thats missing is a unique identifier for either the officer or the person stopped so we can see how often they are in the data. The ISSUING_OFFICER_COMMAND_CODE variable may be a unique ID for the officer, but its not clear that it is - there are different officer ranks for the same command code so this appears to me to be different officers. 13.1 Reshaping a single column Relative to most data sets in criminology, this is an enormous amount of information. So I encourage you to explore this data and practice your R skills. As this chapter focuses on reshaping, we wont do much exploring of the data. This data is in long format as each row is a different stop, and the columns are information about that specific stop. We cant convert this to wide format as there are no repeated entries in the data; each row is of a unique person stopped (at least as far as we can tell. In reality, there are likely to be many people stopped multiple times in the data). We could use another variable such as stopped persons race or gender and reshape it using that, but thatll lead to many thousands of columns so is not a great idea. Instead, well first aggregate the data and then reshape that aggregated data. Well aggregate the data by month and by day of week and see how many people of each race were stopped at each month-day-of-week. In addition to tidyr, well use functions from dplyr to aggregate our data. Since weve already installed that package we just need to use library() and dont need to use install.packages() again. Weve already used the group_by() function in aggregating, and now well introduce a new function: count(). In our earlier aggregation code (introduced in Section 11.3), we used the function summarize() and summed up columns that had a numeric variable (e.g. the number of murders). count() works similarly by sums up categorical variables, which in our case is the race of people stopped. Using count() we enter in the categorical column in the parentheses, and itll make a new column called n, which is the sum of each category. Since were aggregating the data lets assign the result of our aggregating to a new object called sqf_agg. library(tidyr) library(dplyr) sqf_agg &lt;- sqf %&gt;% group_by( MONTH2, DAY2 ) %&gt;% count(SUSPECT_RACE_DESCRIPTION) Now lets look at the head() of the result. head(sqf_agg) # # A tibble: 6 x 4 # # Groups: MONTH2, DAY2 [1] # MONTH2 DAY2 SUSPECT_RACE_DESCRIPTION n # &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; # 1 April Friday (null) 1 # 2 April Friday ASIAN / PACIFIC ISLANDER 1 # 3 April Friday BLACK 104 # 4 April Friday BLACK HISPANIC 17 # 5 April Friday WHITE 16 # 6 April Friday WHITE HISPANIC 31 Each row is now a month-day-of-week-race combination, and we have two additional columns: first the person stoppeds race and then the n column, which says how many people of that race were stopped in that month-day-of-week. The first race is (null), which means we dont know the race. We could keep these values as an unknown race, but for simplicity well just remove them using filter(). To make sure we only have values we expect, we can use the unique() function to check that we have only months and week-days we expect, and that our race names are consistent. This is important because even though you know, for example, that there are only seven days in a week, there may be more in our data due to typos or erroneous data entry. So it is always good to check. unique(sqf_agg$MONTH2) # [1] &quot;April&quot; &quot;August&quot; &quot;December&quot; &quot;February&quot; &quot;January&quot; &quot;July&quot; &quot;June&quot; &quot;March&quot; &quot;May&quot; # [10] &quot;November&quot; &quot;October&quot; &quot;September&quot; unique(sqf_agg$DAY2) # [1] &quot;Friday&quot; &quot;Monday&quot; &quot;Saturday&quot; &quot;Sunday&quot; &quot;Thursday&quot; &quot;Tuesday&quot; &quot;Wednesday&quot; unique(sqf_agg$SUSPECT_RACE_DESCRIPTION) # [1] &quot;(null)&quot; &quot;ASIAN / PACIFIC ISLANDER&quot; &quot;BLACK&quot; &quot;BLACK HISPANIC&quot; # [5] &quot;WHITE&quot; &quot;WHITE HISPANIC&quot; &quot;AMERICAN INDIAN/ALASKAN N&quot; The months and days of the week look good. For an actual data analysis we may want to combine the Hispanic values to a single Hispanic value instead of splitting it between BLACK HISPANIC and WHITE HISPANIC, but for this lesson well leave it as it is. Now, we want to reshape the data from its current long format to a wide format. We do this using the pivot_wider() function from the tidyr package. In the function we need to input two values, the name of the column, which identifies what each value in the row means, and the column that has that value. In our case this is the SUSPECT_RACE_DESCRIPTION column and the n column. We dont need to put the column names in quotes. These function parameters are called names_from and values_from so well also include that in the pivot_wider() function. And finally before we run pivot_wider(), well first use filter() to remove any row where the race is (null). Well assign the result of this code to an object called sqf_agg_wide and use head() to look at this result. sqf_agg_wide &lt;- sqf_agg %&gt;% filter(SUSPECT_RACE_DESCRIPTION != &quot;(null)&quot;) %&gt;% pivot_wider( names_from = SUSPECT_RACE_DESCRIPTION, values_from = n ) head(sqf_agg_wide) # # A tibble: 6 x 8 # # Groups: MONTH2, DAY2 [6] # MONTH2 DAY2 `ASIAN / PACIFIC ISLANDER` BLACK `BLACK HISPANIC` WHITE `WHITE HISPANIC` AMERICAN INDIAN/ALASKA~1 # &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; # 1 April Friday 1 104 17 16 31 NA # 2 April Monday 1 92 10 32 29 NA # 3 April Saturday 3 115 24 24 44 NA # 4 April Sunday 2 96 12 15 38 NA # 5 April Thursday 2 122 10 18 38 NA # 6 April Tuesday 7 137 14 20 49 NA # # ... with abbreviated variable name 1: `AMERICAN INDIAN/ALASKAN N` Now instead of having one row be a month-day-of-week-race combination, each row is a month-day-of-week pair, and we have one column for every race in our data. Each of these race columns tell us how many people of that race were stopped in that month-day-of-week. This allows for really easy comparison of things like racial differences in stops for each month-day-of-week as we just look at different columns in the same row. We have now successfully done our first reshaping, moving this data from long to wide format! Now we want to reshape it again to go from wide to long. Our race columns names just took the values from the race column, which means that we have column names all in capital letters and with spaces and slashes in them. We could technically use this as it is, but its a bit trickier to use any name with punctuation in it, and is against R column name convention, so well quickly fix this. To do so well use the make_clean_names() function from the janitor package that automatically makes all character inputs to the function lowercase and replaces all punctuation with an underscore. First, we need to install the package with install.packages(). install.packages(&quot;janitor&quot;) To use this function on the column names well make the input of the function names(sqf_agg_wide), which returns the names of the sqf_agg_wide data, and assign the result back into names(sqf_agg_wide). It might look weird to put a function inside of a function, but R is completely fine with it. Running names(sqf_agg_wide) at the end will print out the column names so we can check that it worked. library(janitor) # # Attaching package: &#39;janitor&#39; # The following objects are masked from &#39;package:stats&#39;: # # chisq.test, fisher.test names(sqf_agg_wide) &lt;- make_clean_names(names(sqf_agg_wide)) names(sqf_agg_wide) # [1] &quot;month2&quot; &quot;day2&quot; &quot;asian_pacific_islander&quot; &quot;black&quot; # [5] &quot;black_hispanic&quot; &quot;white&quot; &quot;white_hispanic&quot; &quot;american_indian_alaskan_n&quot; Now each column name is lowercased and has only underscores instead of spaces and slashes. To reshape this wide data to long format, well use the tidyr function pivot_longer(). There are three inputs here: cols, which takes a vector of column names which have our value variables; names_to, which is what itll call the newly created categorical variable; and values_to, which is what itll call the newly created values column. For cols we want to include each of our race columns, and these column names must be in quotes and in a vector as there are multiple columns. For names_to we can call it whatever we want, but here well call it race as the variable is about the race of the person who was stopped. And for values_to well call it number_of_people_stopped though we can call it whatever we like. sqf_agg_long &lt;- sqf_agg_wide %&gt;% pivot_longer( cols = c( &quot;asian_pacific_islander&quot;, &quot;black&quot;, &quot;black_hispanic&quot;, &quot;white&quot;, &quot;white_hispanic&quot;, &quot;american_indian_alaskan_n&quot; ), names_to = &quot;race&quot;, values_to = &quot;number_of_people_stopped&quot; ) head(sqf_agg_long) # # A tibble: 6 x 4 # # Groups: month2, day2 [1] # month2 day2 race number_of_people_stopped # &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; # 1 April Friday asian_pacific_islander 1 # 2 April Friday black 104 # 3 April Friday black_hispanic 17 # 4 April Friday white 16 # 5 April Friday white_hispanic 31 # 6 April Friday american_indian_alaskan_n NA In some cases youll have many columns that you want to include while reshaping, which makes writing them all out by hand time consuming. If all of the columns are sequential you can use a trick in this function by writing first_column:last_column where the : will make it include each column (in order) from the first one you input to the last one. This is doing the same thing as 1:3, which returns 1, 2, and 3, but for columns instead of numbers. This doesnt work in most cases but does work for many tidyverse packages. There are also a large number of functions from the dplyr package that are for selecting columns, and are very helpful for doing things like this. The functions are numerous and have changed relatively frequently in the past so I wont cover them in this book, but if youre interested you can look at them on this page of dplyrs website. sqf_agg_long &lt;- sqf_agg_wide %&gt;% pivot_longer( cols = asian_pacific_islander:american_indian_alaskan_n, names_to = &quot;race&quot;, values_to = &quot;number_of_people_stopped&quot; ) head(sqf_agg_long) # # A tibble: 6 x 4 # # Groups: month2, day2 [1] # month2 day2 race number_of_people_stopped # &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; # 1 April Friday asian_pacific_islander 1 # 2 April Friday black 104 # 3 April Friday black_hispanic 17 # 4 April Friday white 16 # 5 April Friday white_hispanic 31 # 6 April Friday american_indian_alaskan_n NA 13.2 Reshaping multiple columns So far weve just been reshaping using a single column. This is the simplest method, but in some cases well need to reshape using multiple columns. As an example, lets make a new column called n2 in our sqf_agg data set, which just adds 10 to the value in our n column. sqf_agg$n2 &lt;- sqf_agg$n + 10 Since these columns both relate to the race column (called SUSPECT_RACE_DESCRIPTION) we can reuse the pivot_wider() code from before, but now the values_from parameter takes a vector of column names instead of a single column name. Here we want to include both the n and the n2 column. Since its a dplyr function its not necessary to put the column names in quotes. We also want to keep our filter() function from before which removes (null) races and then add a head() function at the end so it prints out the first six rows of our resulting data set. sqf_agg_wide &lt;- sqf_agg %&gt;% filter(SUSPECT_RACE_DESCRIPTION != &quot;(null)&quot;) %&gt;% pivot_wider( names_from = SUSPECT_RACE_DESCRIPTION, values_from = c(n, n2) ) names(sqf_agg_wide) &lt;- make_clean_names(names(sqf_agg_wide)) head(sqf_agg_wide) # # A tibble: 6 x 14 # # Groups: month2, day2 [6] # month2 day2 n_asian~1 n_black n_bla~2 n_white n_whi~3 n_ame~4 n2_as~5 n2_bl~6 n2_bl~7 n2_wh~8 n2_wh~9 n2_am~* # &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 April Friday 1 104 17 16 31 NA 11 114 27 26 41 NA # 2 April Monday 1 92 10 32 29 NA 11 102 20 42 39 NA # 3 April Saturday 3 115 24 24 44 NA 13 125 34 34 54 NA # 4 April Sunday 2 96 12 15 38 NA 12 106 22 25 48 NA # 5 April Thursday 2 122 10 18 38 NA 12 132 20 28 48 NA # 6 April Tuesday 7 137 14 20 49 NA 17 147 24 30 59 NA # # ... with abbreviated variable names 1: n_asian_pacific_islander, 2: n_black_hispanic, 3: n_white_hispanic, # # 4: n_american_indian_alaskan_n, 5: n2_asian_pacific_islander, 6: n2_black, 7: n2_black_hispanic, 8: n2_white, # # 9: n2_white_hispanic, *: n2_american_indian_alaskan_n We now have the same wide data set as before, but now there are twice as many race columns. And the pivot_wider() function renamed the columns so we can tell the n columns from the n2 columns. The easiest way to reshape this data from wide to long is to again use the pivot_longer() function but now use it twice: first to reshape the n columns and then to reshape the n2 columns. Well use the exact same code as before, but change the column names to suit their new names. sqf_agg_long &lt;- sqf_agg_wide %&gt;% pivot_longer( cols = c( &quot;n_asian_pacific_islander&quot;, &quot;n_black&quot;, &quot;n_black_hispanic&quot;, &quot;n_white&quot;, &quot;n_white_hispanic&quot;, &quot;n_american_indian_alaskan_n&quot; ), names_to = &quot;race&quot;, values_to = &quot;number_of_people_stopped&quot; ) %&gt;% pivot_longer( cols = c( &quot;n2_asian_pacific_islander&quot;, &quot;n2_black&quot;, &quot;n2_black_hispanic&quot;, &quot;n2_white&quot;, &quot;n2_white_hispanic&quot;, &quot;n2_american_indian_alaskan_n&quot; ), names_to = &quot;race2&quot;, values_to = &quot;number_of_people_stopped2&quot; ) head(sqf_agg_long) # # A tibble: 6 x 6 # # Groups: month2, day2 [1] # month2 day2 race number_of_people_stopped race2 number_of_people_s~1 # &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; # 1 April Friday n_asian_pacific_islander 1 n2_asian_pacific_islander 11 # 2 April Friday n_asian_pacific_islander 1 n2_black 114 # 3 April Friday n_asian_pacific_islander 1 n2_black_hispanic 27 # 4 April Friday n_asian_pacific_islander 1 n2_white 26 # 5 April Friday n_asian_pacific_islander 1 n2_white_hispanic 41 # 6 April Friday n_asian_pacific_islander 1 n2_american_indian_alaskan_n NA # # ... with abbreviated variable name 1: number_of_people_stopped2 This now gives us two race columns - race and race2 - which are ordered differently so we need to make sure to either reorder the data to be the same ordering or to keep that in mind when comparing the number_of_people_stopped and number_of_people_stopped2 columns as they frequently refer to different races. "],["graphing-intro.html", "14 Graphing with ggplot2 14.1 What does the data look like? 14.2 Graphing data 14.3 Time-series plots 14.4 Scatter plots 14.5 Color blindness", " 14 Graphing with ggplot2 For this chapter youll need the following file, which is available for download here: apparent_per_capita_alcohol_consumption.rda. Weve made some simple graphs earlier; in this lesson we will use the package ggplot2 to make simple and elegant-looking graphs. The gg part of ggplot2 stands for grammar of graphics, which is the idea that most graphs can be made using the same few pieces. Well get into those pieces during this lesson. For a useful cheat sheet for this package see here. install.packages(&quot;ggplot2&quot;) library(ggplot2) When working with new data, its often useful to quickly graph the data to try to understand what youre working with. It is also useful when understanding how much to trust the data. The data we will work on is data about alcohol consumption in US states from 1977-2017 from the National Institutes of Health. It contains the per capita alcohol consumption for each state for every year. Their method to determine per capita consumption is amount of alcohol sold / number of people aged 14+ living in the state. More details on the data are available here. Now we need to load the data. load(&quot;data/apparent_per_capita_alcohol_consumption.rda&quot;) The name of the data is quite long so for convenience lets copy it to a new object with a better name, alcohol. alcohol &lt;- apparent_per_capita_alcohol_consumption The original data has every state, region, and the US as a whole. For this lesson were using data subsetted to just include states. For now lets just look at Pennsylvania. penn_alcohol &lt;- alcohol[alcohol$state == &quot;pennsylvania&quot;, ] 14.1 What does the data look like? Before graphing, its helpful to see what the data includes. An important thing to check is what variables are available and what the units are for these variables. head(penn_alcohol) # state year ethanol_beer_gallons_per_capita ethanol_wine_gallons_per_capita # 1559 pennsylvania 2017 1.29 0.33 # 1560 pennsylvania 2016 1.31 0.33 # 1561 pennsylvania 2015 1.31 0.32 # 1562 pennsylvania 2014 1.32 0.32 # 1563 pennsylvania 2013 1.34 0.31 # 1564 pennsylvania 2012 1.36 0.31 # ethanol_spirit_gallons_per_capita ethanol_all_drinks_gallons_per_capita number_of_beers # 1559 0.71 2.34 305.7778 # 1560 0.72 2.36 310.5185 # 1561 0.70 2.33 310.5185 # 1562 0.70 2.34 312.8889 # 1563 0.68 2.33 317.6296 # 1564 0.67 2.34 322.3704 # number_of_glasses_wine number_of_shots_liquor number_of_drinks_total # 1559 65.48837 147.4128 499.2000 # 1560 65.48837 149.4891 503.4667 # 1561 63.50388 145.3366 497.0667 # 1562 63.50388 145.3366 499.2000 # 1563 61.51938 141.1841 497.0667 # 1564 61.51938 139.1079 499.2000 So each row of the data is a single year of data for Pennsylvania. It includes alcohol consumption for wine, liquor, beer, and total drinks - both as gallons of ethanol (a hard unit to interpret) and more traditional measures such as glasses of wine or number of beers. The original data only included the gallons of ethanol data, which I converted to the more understandable units. If you encounter data with odd units, it is a good idea to convert it to something easier to understand - especially if you intend to show someone else the data or results. 14.2 Graphing data To make a plot using ggplot() (please note that the function does not have a 2 at the end of it, only the package name does), all you need to do is specify the data set and the variables you want to plot. From there you add on pieces of the graph using the + symbol (which operates like a dplyr pipe) and then specify what you want added. For ggplot() we need to specify four things: The data set The x-axis variable The y-axis variable The type of graph - e.g. line, point, etc. Some useful types of graphs are: geom_point() - A point graph, can be used for scatter plots geom_line() - A line graph geom_bar() - A barplot geom_smooth() - Adds a regression line to the graph 14.3 Time-series plots Lets start with a time-series of beer consumption in Pennsylvania. In time-series plots the x-axis is always the time variable while the y-axis is the variable whose trend over time is what were interested in. When you see a graph showing, for example, crime rates over time, this is the type of graph youre looking at. The code below starts by writing our data set name. Then says what our x- and y-axis variables are called. The x- and y-axis variables are within parentheses of the function called aes(). aes() stands for aesthetic, and whats included inside here describes how the graph will look. Its not intuitive to remember, but you need to include it. Like in dplyr functions, you do not need to put the column names in quotes or repeat which data set you are using. ggplot(penn_alcohol, aes( x = year, y = number_of_beers )) Note that on the x-axis it prints out every single year and makes it completely unreadable. That is because the year column is a character type, so R thinks each year is its own category. It prints every single year because it thinks we want every category shown. To fix this, we can make the column numeric, and ggplot() will be smarter about printing fewer years. penn_alcohol$year &lt;- as.numeric(penn_alcohol$year) ggplot(penn_alcohol, aes( x = year, y = number_of_beers )) When we run it, we get our graph. It includes the variable names for each axis and shows the range of data through the tick marks. What is missing is the actual data. For that we need to specify what type of graph it is. We literally add it with the + followed by the type of graph we want. Make sure that the + is at the end of a line, not the start of one. Starting a line with the + will not work. Lets start with point and line graphs. ggplot(penn_alcohol, aes( x = year, y = number_of_beers )) + geom_point() ggplot(penn_alcohol, aes( x = year, y = number_of_beers )) + geom_line() We can also combine different types of graphs. ggplot(penn_alcohol, aes( x = year, y = number_of_beers )) + geom_point() + geom_line() It looks like theres a huge change in beer consumption over time. But look at where they y-axis starts. It starts around 280 so really that change is only ~60 beers. Thats because when graphs dont start at 0, it can make small changes appear big. We can fix this by forcing the y-axis to begin at 0. We can add expand_limits(y = 0) to the graph to say that the value 0 must always appear on the y-axis, even if no data is close to that value. ggplot(penn_alcohol, aes( x = year, y = number_of_beers )) + geom_point() + geom_line() + expand_limits(y = 0) Now that graph shows what looks like nearly no change even though that is also not true. Which graph is best? Its hard to say. Inside the types of graphs we can change how it is displayed. As with using plot(), we can specify the color and size of our lines or points. ggplot(penn_alcohol, aes( x = year, y = number_of_beers )) + geom_line(color = &quot;forestgreen&quot;, size = 1.3) Some other useful features are changing the axis labels and the graph title. Unlike in plot() we do not include it in the () of ggplot() but use their own functions to add them to the graph. The input to each of these functions is a string for what we want it to say. xlab() - x-axis label ylab() - y-axis label ggtitle() - graph title ggplot(penn_alcohol, aes( x = year, y = number_of_beers )) + geom_line(color = &quot;forestgreen&quot;, size = 1.3) + xlab(&quot;Year&quot;) + ylab(&quot;Number of Beers&quot;) + ggtitle(&quot;PA Annual Beer Consumption Per Capita (1977-2017)&quot;) Many time-series plots show multiple variables over the same time period (e.g. murder and robbery over time). There are ways to change the data itself to make creating graphs like this easier, but lets stick with the data we currently have and just change ggplot(). Start with a normal line graph, this time looking at wine. ggplot(penn_alcohol, aes( x = year, y = number_of_glasses_wine )) + geom_line() Then include a second geom_line() with its own aes() for the second variable. Since we are using the penn_alcohol data set for both lines we do not need to include it in the second geom_line() as it assumes that the data is the same if we dont specify otherwise. If we used a different data set for the second line, we would need to specify which data set it is inside of geom_line() and before aes(). ggplot(penn_alcohol, aes( x = year, y = number_of_glasses_wine )) + geom_line() + geom_line(aes( x = year, y = number_of_shots_liquor )) A problem with this is that both lines are the same color. We need to set a color for each line and do so within aes(). Instead of providing a color name, we need to provide the name the color will have in the legend. Do so for both lines. ggplot(penn_alcohol, aes( x = year, y = number_of_glasses_wine, color = &quot;Glasses of Wine&quot; )) + geom_line() + geom_line(aes( x = year, y = number_of_shots_liquor, color = &quot;Shots of Liquor&quot; )) We can change the legend title by using the function labs() and changing the value color to what we want the legend title to be. ggplot(penn_alcohol, aes( x = year, y = number_of_glasses_wine, color = &quot;Glasses of Wine&quot; )) + geom_line() + geom_line(aes( x = year, y = number_of_shots_liquor, color = &quot;Shots of Liquor&quot; )) + labs(color = &quot;Alcohol Type&quot;) Finally, a useful option to move the legend from the side to the bottom is setting the theme() function to move the legend.position to bottom. This will allow the graph to be wider. ggplot(penn_alcohol, aes( x = year, y = number_of_glasses_wine, color = &quot;Glasses of Wine&quot; )) + geom_line() + geom_line(aes( x = year, y = number_of_shots_liquor, color = &quot;Shots of Liquor&quot; )) + labs(color = &quot;Alcohol Type&quot;) + theme(legend.position = &quot;bottom&quot;) 14.4 Scatter plots Making a scatter plot simply requires changing the x-axis from year to another numerical variable and using geom_point(). Since our data has one row for every year for Pennsylvania, we can make a scatterplot comparing different drinks in each year. For this example, well compare liquor to beer sales. ggplot(penn_alcohol, aes( x = number_of_shots_liquor, y = number_of_beers )) + geom_point() This graph shows us that when liquor consumption increases, beer consumption also tends to increase. While scatterplots can help show the relationship between variables, we lose the information of how consumption changes over time. 14.5 Color blindness Please keep in mind that some people are color blind so graphs (or maps, which we will learn about soon) will be hard to read for these people if we choose bad colors. A helpful site for choosing colors for graphs and maps is Color Brewer. This site lets you select which type of colors you want (sequential and diverging, such as shades in a hotspot map, and qualitative, such as for data like what we used in this lesson). In the Only show: section you can set it to colorblind safe to restrict it to colors that allow people with color blindness to read your graph. To the right of this section it shows the HEX codes for each color. A HEX code is just a code that a computer can read and know exactly which color it is. Lets use an example of a color-blind friendly color from the qualitative section of ColorBrewer. We have three options on this page (we can change how many colors we want but it defaults to showing 3): green (HEX = #1b9e77), orange (HEX = #d95f02), and purple (HEX = #7570b3). Well use the orange and purple colors. To manually set colors in ggplot() we use scale_color_manual(values = c()) and include a vector of color names or HEX codes inside the c(). Doing that using the orange and purple HEX codes will change our graph colors to these two colors. ggplot(penn_alcohol, aes( x = year, y = number_of_glasses_wine, color = &quot;Glasses of Wine&quot; )) + geom_line() + geom_line(aes( x = year, y = number_of_shots_liquor, color = &quot;Shots of Liquor&quot; )) + labs(color = &quot;Alcohol Type&quot;) + theme(legend.position = &quot;bottom&quot;) + scale_color_manual(values = c(&quot;#7570b3&quot;, &quot;#d95f02&quot;)) "],["ois-graphs.html", "15 More graphing with ggplot2 15.1 Exploring data 15.2 Graphing a single numeric variable 15.3 Graphing a categorical variable 15.4 Graphing data over time 15.5 Pretty graphs", " 15 More graphing with ggplot2 For this chapter youll need the following file, which is available for download here: fatal-police-shootings-data.csv. In this lesson we will continue to explore graphing using ggplot2. The data we will use is microdata on officer-involved shootings that resulted in a death in the United States since January 1st, 2015. This data has been compiled and released by The Washington Post so it will be a useful exercise in exploring data from non-government sources. This data is useful for our purposes as it has a number of variables related to the person who was shot, allowing us to practice making many types of graphs. Each row of data is a different person who was shot and killed by the police, and each column gives us information about the individual or the shooting, such as their age, whether they carried any weapon, and the shooting location. To explore the data on The Washington Posts website, see here. To examine their methodology, see here. The data initially comes as a .csv file so well use the read_csv() function from the readr package. library(readr) shootings &lt;- read_csv(&quot;data/fatal-police-shootings-data.csv&quot;) Since read_csv() reads files into a tibble object, well turn it into a data.frame so head() shows every single column. shootings &lt;- as.data.frame(shootings) 15.1 Exploring data Now that we have the data read in, lets look at it. ncol(shootings) # [1] 14 nrow(shootings) # [1] 4371 The data has 14 variables and covers 4,371 shootings. Lets check out some of the variables, first using head() then using summary() and table(). head(shootings) # id name date manner_of_death armed age gender race city state # 1 3 Tim Elliot 2015-01-02 shot gun 53 M A Shelton WA # 2 4 Lewis Lee Lembke 2015-01-02 shot gun 47 M W Aloha OR # 3 5 John Paul Quintero 2015-01-03 shot and Tasered unarmed 23 M H Wichita KS # 4 8 Matthew Hoffman 2015-01-04 shot toy weapon 32 M W San Francisco CA # 5 9 Michael Rodriguez 2015-01-04 shot nail gun 39 M H Evans CO # 6 11 Kenneth Joe Brown 2015-01-04 shot gun 18 M W Guthrie OK # signs_of_mental_illness threat_level flee body_camera # 1 TRUE attack Not fleeing FALSE # 2 FALSE attack Not fleeing FALSE # 3 FALSE other Not fleeing FALSE # 4 TRUE attack Not fleeing FALSE # 5 FALSE attack Not fleeing FALSE # 6 FALSE attack Not fleeing FALSE Each row is a single shooting, and it includes variables such as the victims name, the date of the shooting, demographic information about that person, the city and state where the shooting occurred, and some information about the incident. It is clear from these first 6 rows that most variables are categorical so we cant use summary() on them. Lets use summary() on the date and age columns and then use table() for the rest. summary(shootings$date) # Min. 1st Qu. Median Mean 3rd Qu. Max. # &quot;2015-01-02&quot; &quot;2016-02-07&quot; &quot;2017-03-16&quot; &quot;2017-03-18&quot; &quot;2018-04-11&quot; &quot;2019-06-25&quot; summary(shootings$age) # Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s # 6.00 27.00 35.00 36.84 45.00 91.00 182 From this we can see that the data is from early January through mid-2019.18 From the age column we can see that the average age is about 37 with most people around that range. Now we can use table() to see how often each value appears in each variable. We dont want to do this for city or name as there would be too many values, but it will work for the other columns. Lets start with the manner_of_death column. table(shootings$manner_of_death) # # shot shot and Tasered # 4146 225 To turn these counts into percentages we can divide the results by the number of rows in our data and multiply by 100. table(shootings$manner_of_death) / nrow(shootings) * 100 # # shot shot and Tasered # 94.852437 5.147563 Now it is clear to see that in about 95% of shootings, officers used a gun and in 5% of shootings they also used a Taser. As this is data on officer shooting deaths, this is unsurprising. Lets take a look at whether the victim was armed. table(shootings$armed) / nrow(shootings) * 100 # # air conditioner ax barstool # 0.02287806 0.48043926 0.02287806 # baseball bat baseball bat and bottle baseball bat and fireplace poker # 0.27453672 0.02287806 0.02287806 # baton bayonet BB gun # 0.09151224 0.02287806 0.06863418 # bean-bag gun beer bottle blunt object # 0.02287806 0.06863418 0.11439030 # bow and arrow box cutter brick # 0.02287806 0.22878060 0.04575612 # carjack chain chain saw # 0.02287806 0.04575612 0.04575612 # chainsaw chair claimed to be armed # 0.02287806 0.04575612 0.02287806 # contractor&#39;s level cordless drill crossbow # 0.02287806 0.02287806 0.20590254 # crowbar fireworks flagpole # 0.06863418 0.02287806 0.02287806 # flashlight garden tool glass shard # 0.02287806 0.02287806 0.06863418 # gun gun and car gun and knife # 55.43353924 0.11439030 0.34317090 # gun and sword gun and vehicle guns and explosives # 0.02287806 0.04575612 0.06863418 # hammer hand torch hatchet # 0.22878060 0.02287806 0.18302448 # hatchet and gun incendiary device knife # 0.04575612 0.04575612 14.96225120 # lawn mower blade machete machete and gun # 0.04575612 0.86936628 0.02287806 # meat cleaver metal hand tool metal object # 0.06863418 0.02287806 0.09151224 # metal pipe metal pole metal rake # 0.25165866 0.04575612 0.02287806 # metal stick motorcycle nail gun # 0.06863418 0.02287806 0.02287806 # oar pellet gun pen # 0.02287806 0.02287806 0.02287806 # pepper spray pick-axe piece of wood # 0.02287806 0.06863418 0.06863418 # pipe pitchfork pole # 0.13726836 0.04575612 0.04575612 # pole and knife rock samurai sword # 0.04575612 0.09151224 0.02287806 # scissors screwdriver sharp object # 0.06863418 0.18302448 0.11439030 # shovel spear stapler # 0.06863418 0.02287806 0.02287806 # straight edge razor sword Taser # 0.06863418 0.34317090 0.41180508 # tire iron toy weapon unarmed # 0.02287806 3.54609929 6.36010066 # undetermined unknown weapon vehicle # 4.30107527 1.25829330 1.57858614 # vehicle and gun walking stick wrench # 0.02287806 0.02287806 0.02287806 This is fairly hard to interpret as it is sorted alphabetically when wed prefer it to be sorted by most common weapon. It also doesnt round the numbers so there are many numbers past the decimal point shown. Lets solve these two issues using sort() and round(). We could just wrap our initial code inside each of these functions but to avoid making too complicated code, we save the results in a vector called temp and incrementally use sort() and round() on that. Well set the parameter decreasing to TRUE in the sort() function so that it is in descending order of how common each value is. And well round to two decimal places by setting the parameter digits to 2 in round(). temp &lt;- table(shootings$armed) / nrow(shootings) * 100 temp &lt;- sort(temp, decreasing = TRUE) temp &lt;- round(temp, digits = 2) temp # # gun knife unarmed # 55.43 14.96 6.36 # undetermined toy weapon vehicle # 4.30 3.55 1.58 # unknown weapon machete ax # 1.26 0.87 0.48 # Taser gun and knife sword # 0.41 0.34 0.34 # baseball bat metal pipe box cutter # 0.27 0.25 0.23 # hammer crossbow hatchet # 0.23 0.21 0.18 # screwdriver pipe blunt object # 0.18 0.14 0.11 # gun and car sharp object baton # 0.11 0.11 0.09 # metal object rock BB gun # 0.09 0.09 0.07 # beer bottle crowbar glass shard # 0.07 0.07 0.07 # guns and explosives meat cleaver metal stick # 0.07 0.07 0.07 # pick-axe piece of wood scissors # 0.07 0.07 0.07 # shovel straight edge razor brick # 0.07 0.07 0.05 # chain chain saw chair # 0.05 0.05 0.05 # gun and vehicle hatchet and gun incendiary device # 0.05 0.05 0.05 # lawn mower blade metal pole pitchfork # 0.05 0.05 0.05 # pole pole and knife air conditioner # 0.05 0.05 0.02 # barstool baseball bat and bottle baseball bat and fireplace poker # 0.02 0.02 0.02 # bayonet bean-bag gun bow and arrow # 0.02 0.02 0.02 # carjack chainsaw claimed to be armed # 0.02 0.02 0.02 # contractor&#39;s level cordless drill fireworks # 0.02 0.02 0.02 # flagpole flashlight garden tool # 0.02 0.02 0.02 # gun and sword hand torch machete and gun # 0.02 0.02 0.02 # metal hand tool metal rake motorcycle # 0.02 0.02 0.02 # nail gun oar pellet gun # 0.02 0.02 0.02 # pen pepper spray samurai sword # 0.02 0.02 0.02 # spear stapler tire iron # 0.02 0.02 0.02 # vehicle and gun walking stick wrench # 0.02 0.02 0.02 Now it is a little easier to interpret. In over half of the cases the victim was carrying a gun. A knife was involved 15% of the time. And 6% of the time they were unarmed. In 4% of cases there is no data on any weapon. That leaves about 20% of cases where one of the many rare weapons were used, including some that overlap with one of the more common categories. Think about how youd graph this data. There are 85 unique values in this column though fewer than ten of them are common enough to appear more than 1% of the time. Should we graph all of them? No, that would overwhelm any graph. For a useful graph we would need to combine many of these into a single category - possibly called other weapons. And how do we deal with values where they could meet multiple larger categories? There is not always a clear answer for these types of questions. It depends on what data youre interested in, the goal of the graph, the target audience, and personal preference. Lets keep exploring the data by looking at gender and race. table(shootings$gender) / nrow(shootings) * 100 # # F M # 4.667124 95.218485 Nearly all of the shootings are of a man. Given that we saw most shootings involved a person with a weapon and that most violent crimes are committed by men, this shouldnt be too surprising. temp &lt;- table(shootings$race) / nrow(shootings) * 100 temp &lt;- sort(temp) temp &lt;- round(temp, digits = 2) temp # # O N A H B W # 0.87 1.46 1.62 16.45 22.90 44.89 White people are the largest race group that is killed by police, followed by Black people and Hispanic people. In fact, there are about twice as many White people killed than Black people killed, and about 2.5 times as many White people killed than Hispanic people killed. Does this mean that the oft-repeated claim that Black people are killed at disproportionate rates is wrong? No. This data simply shows the number of people killed; it doesnt give any indication on rates of death per group. Youd need to merge it with Census data to get population to determine a rate per race group. And even that would be insufficient since people are, for example, stopped by police at different rates. This data provides a lot of information on people killed by the police, but even so it is insufficient to answer many of the questions on that topic. Its important to understand the data not only to be able to answer questions about it, but to know what questions you cant answer - and youll find when using criminology data that there are a lot of questions that you cant answer.19 One annoying thing with the gender and race variables is that they dont spell out the name. Instead of Female, for example, it has F. For our graphs we want to spell out the words so it is clear to viewers. Well fix this issue, and the issue of having many weapon categories, as we graph each variable. 15.2 Graphing a single numeric variable Weve spent some time looking at the data so now were ready to make the graphs. We need to load the ggplot2 package if we havent done so already this session (i.e. since you last closed RStudio). library(ggplot2) As a reminder, the benefit of using ggplot() is that we can start with a simple plot and build our way up to more complicated graphs. Well start here by building some graphs to depict a numeric variable - in this case the age column. We start every ggplot() the same, by inserting the data set first and then put our x and y variables inside of the aes() parameter. In this case were only going to be plotting an x variable so we dont need to write anything for y. ggplot(shootings, aes(x = age)) Running the above code returns a blank graph since we havent told ggplot() what type of graph we want yet. Below are a few different types of ways to display a single numeric variable. Theyre essentially all variations of each other and show the data at different levels of precision. Its hard to say which is best - youll need to use your best judgment and consider your audience. 15.2.1 Histogram The histogram is a very common type of graph for a single numeric variable. Histograms group a numeric variable into categories and then plot them, with the heights of each bar indicating how common the group is. We can make a histogram by adding geom_histogram() to the ggplot(). The results will print out the text Warning: Removed 182 rows containing non-finite values (stat_bin). which just means that 182 rows in our data have NA for age, and are excluded from the graph. ggplot(shootings, aes(x = age)) + geom_histogram() # `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. # Warning: Removed 182 rows containing non-finite values (stat_bin). The x-axis is ages with each bar being a group of certain ages, and the y-axis is how many people are in each group. The grouping is done automatically and we can alter it by changing the bin parameter in geom_histogram(). By default this parameter is set to 30, but we can make each group smaller (have fewer ages per group) by increasing it from 30 or make each group larger by decreasing it. ggplot(shootings, aes(x = age)) + geom_histogram(bins = 15) # Warning: Removed 182 rows containing non-finite values (stat_bin). ggplot(shootings, aes(x = age)) + geom_histogram(bins = 45) # Warning: Removed 182 rows containing non-finite values (stat_bin). Note that while the overall trend (of most deaths being around age 25) doesnt change when we alter bin, the data gets more or less precise. Having fewer bins means fewer, but larger, bars which can obscure trends that more, smaller, bars would show. But having too many bars may make you focus on minor variations that could occur randomly and take away attention from the overall trend. I prefer to err on the side of more precise graphs (more, smaller bars) but be careful over-interpreting data from small groups. These graphs show the y-axis as the number of people in each bar. If we want to show proportions instead, we can add in a parameter for y in the aes() of the geom_histogram(). We add in y = (..count..)/sum(..count..)), which automatically converts the counts to proportions. The (..count..)/sum(..count..)) stuff is just taking each group and dividing it from the sum of all groups. You could, of course, do this yourself before making the graph, but its an easy helper. If you do this, make sure to relabel the y-axis so you dont accidentally call the proportions a count. ggplot(shootings, aes(x = age)) + geom_histogram(aes(y = (..count..) / sum(..count..))) # `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. # Warning: Removed 182 rows containing non-finite values (stat_bin). 15.2.2 Density plot Density plots are essentially smoothed versions of histograms. Theyre especially useful for numeric variables, which are not integers (integers are whole numbers). Theyre also useful when you want to be more precise than a histogram as they are - to simplify - histograms where each bar is very narrow. Note that the y-axis of a density plot is automatically labeled density and has very small numbers. Interpreting the y-axis is fairly hard to explain to someone not familiar with statistics so Id caution against using this graph unless your audience is already familiar with it. To interpret these kinds of graphs, I recommend looking for trends rather than trying to identify specific points. For example, in the below graph we can see that shootings rise rapidly starting around age 10, peak at around age 30 (if we were presenting this graph to other people wed probably want more ages shown on the x-axis), and then steadily decline until about age 80 where its nearly flat. ggplot(shootings, aes(x = age)) + geom_density() # Warning: Removed 182 rows containing non-finite values (stat_density). 15.2.3 Count graph A count graph is essentially a histogram with a bar for every value in the numeric variable - like a less-smooth density plot. Note that this wont work well if you have too many unique values so Id strongly recommend rounding the data to the nearest whole number first if you dont already have an integer. Our age variable is already rounded so we dont need to do that. To make a count graph, we add stat_count() to the ggplot(). ggplot(shootings, aes(x = age)) + stat_count() # Warning: Removed 182 rows containing non-finite values (stat_count). Now we have a single bar for every age in the data. Like the histogram, the y-axis shows the number of people that are that age. And like the histogram, we can change this from number of people to proportion of people using the exact same code. ggplot(shootings, aes(x = age)) + stat_count(aes(y = (..count..) / sum(..count..))) # Warning: Removed 182 rows containing non-finite values (stat_count). 15.3 Graphing a categorical variable 15.3.1 Bar graph To make this barplot well set the x-axis variable to our race column and add geom_bar() to the end. ggplot(shootings, aes(x = race)) + geom_bar() This gives us a barplot in alphabetical order. In most cases we want the data sorted by frequency, so we can easily see which value is the most common, second most common, etc. There are a few ways to do this, but well do this by turning the race variable into a factor and ordering it by frequency. We can do that using the factor() function. The first input will be the race variable, and then we will need to set the levels parameter to a vector of values sorted by frequency. An easy way to know how often values are in a column is to use the table() function on that column, such as below. table(shootings$race) # # A B H N O W # 71 1001 719 64 38 1962 Its still alphabetical so lets wrap that in a sort() function. sort(table(shootings$race)) # # O N A H B W # 38 64 71 719 1001 1962 Its sorted from smallest to largest. We usually want to graph from largest to smallest so lets set the parameter decreasing in sort() to TRUE. sort(table(shootings$race), decreasing = TRUE) # # W B H A N O # 1962 1001 719 71 64 38 Now, we only need the names of each value, not how often they occur. So we can against wrap this whole thing in names() to get just the names. names(sort(table(shootings$race), decreasing = TRUE)) # [1] &quot;W&quot; &quot;B&quot; &quot;H&quot; &quot;A&quot; &quot;N&quot; &quot;O&quot; If we tie it all together, we can make the race column into a factor variable. shootings$race &lt;- factor(shootings$race, levels = names(sort(table(shootings$race), decreasing = TRUE )) ) Now lets try that barplot again. ggplot(shootings, aes(x = race)) + geom_bar() It works! Note that all the values that are missing in our data are still reported in the barplot under a column called NA. This is not sorted properly since there are more NA values than three of the other values, but NA is still at the far right of the graph. We can change this if we want to make all the NA values an actual character type and call it something like Unknown. But this way it does draw attention to how many values are missing from this column. Like most things in graphing, this is a personal choice as to what to do. For bar graphs it is often useful to flip the graph so each value is a row in the graph rather than a column. This also makes it much easier to read the value name. If the value names are long, itll shrink the graph to accommodate the name. This is usually a sign that you should try to shorten the name to avoid reducing the size of the graph. ggplot(shootings, aes(x = race)) + geom_bar() + coord_flip() Since its flipped, now its sorted from smallest to largest. So well need to change the factor() code to fix that by making the decreasing parameter in sort() FALSE. shootings$race &lt;- factor(shootings$race, levels = names(sort(table(shootings$race), decreasing = FALSE )) ) ggplot(shootings, aes(x = race)) + geom_bar() + coord_flip() The NA value is now at the top, which looks fairly bad. Lets change all NA values to the string Unknown. And while were at it, lets change all the abbreviated race values to actual names. We can get all the NA values by using is.na(shootings$race) and using a conditional statement to get all rows that meet that condition, then assign them the value Unknown. Instead of trying to subset a factor variable to change the values, we should convert it back to a character type first using as.character(), and then convert it to a factor again once were done. shootings$race &lt;- as.character(shootings$race) shootings$race[is.na(shootings$race)] &lt;- &quot;Unknown&quot; Now we can use conditional statements to change all the race letters to names. Its not clear what race O and N are so I checked The Washington Posts GitHub which explains what they mean. Instead of is.na() well use shootings$race == \"\", where we put the letter inside of the quotes. shootings$race[shootings$race == &quot;O&quot;] &lt;- &quot;Other&quot; shootings$race[shootings$race == &quot;N&quot;] &lt;- &quot;Native American&quot; shootings$race[shootings$race == &quot;A&quot;] &lt;- &quot;Asian&quot; shootings$race[shootings$race == &quot;H&quot;] &lt;- &quot;Hispanic&quot; shootings$race[shootings$race == &quot;B&quot;] &lt;- &quot;Black&quot; shootings$race[shootings$race == &quot;W&quot;] &lt;- &quot;White&quot; Now lets see how our graph looks. Well need to rerun the factor() code since now all of the values are changed. shootings$race &lt;- factor(shootings$race, levels = names(sort(table(shootings$race), decreasing = FALSE )) ) ggplot(shootings, aes(x = race)) + geom_bar() + coord_flip() As earlier, we can show proportion instead of count by adding y = (..count..)/sum(..count..) to the aes() in geom_bar(). ggplot(shootings, aes(x = race)) + geom_bar(aes(y = (..count..) / sum(..count..))) + coord_flip() 15.4 Graphing data over time We went over time-series graphs in Chapter 14, but its such an important topic well cover it again. A lot of criminology research is seeing if a policy had an effect, which means we generally want to compare an outcome over time (and compare the treated group to a similar untreated group). To graph that we look at an outcome - in this case numbers of killings - over time. In our case we arent evaluating any policy, just seeing if the number of police killings change over time. Well need to make a variable to indicate that the row is for one shooting. We can call this dummy and assign it a value of 1. Then we can make the ggplot() and set this dummy column to the y-axis value and set our date variable date to the x-axis (the time variable is always on the x-axis). Then well set the type of plot to geom_line() so we have a line graph showing killings over time. shootings$dummy &lt;- 1 ggplot(shootings, aes( x = date, y = dummy )) + geom_line() This graph is clearly wrong. Why? Well, our y-axis variable is always 1 so theres no variation to plot. Every single value, even if there are more than one shooting per day, is on the 1 line on the y-axis. And the fact that we have multiple killings per day is an issue because we only want a single line in our graph. Well need to aggregate our data to some time period (e.g. day, month, year) so that we have one row per time-period and know how many people were killed in that period. Well start with yearly data and then move to monthly data. Since were going to be dealing with dates, lets use the lubridate() package that is well-suited for this task. install.packages(&quot;lubridate&quot;) library(lubridate) # # Attaching package: &#39;lubridate&#39; # The following object is masked from &#39;package:cowplot&#39;: # # stamp # The following objects are masked from &#39;package:base&#39;: # # date, intersect, setdiff, union Well use two functions to create variables that tell us the month and the year of each date in our data. Well use these new variables to aggregate our data to that time unit. First, the floor_date() function is a very useful tool that essentially rounds a date. Here we have the exact date the killing happened on, and we want to determine what month that date is from. So well use the parameter unit in floor_date() and tell the function we want to know the month (for a full set of options please see the documentation for floor_date() by entering ?floor_date in the console). So we can do floor_date(shootings$date, unit = \"month\") to get the month - specifically, it returns the date that is the first of the month for that month - in which the killing happened. Even simpler, to get the year, we simply use year() and put our date variable in the parentheses. Well call the new variables month_year and year, respectively. shootings$month_year &lt;- floor_date(shootings$date, unit = &quot;month&quot;) shootings$year &lt;- year(shootings$date) head(shootings$month_year) # [1] &quot;2015-01-01&quot; &quot;2015-01-01&quot; &quot;2015-01-01&quot; &quot;2015-01-01&quot; &quot;2015-01-01&quot; &quot;2015-01-01&quot; head(shootings$year) # [1] 2015 2015 2015 2015 2015 2015 Since the data is already sorted by date, all the values printed from head() are the same. But you can look at the data using View() to confirm that the code worked properly. We can now aggregate the data by the month_year variable and assign the result to a new data set well call monthly_shootings. Well use the group_by() and summarize() functions from dplyr that were introduced in Chapter 11 to do this. And well use the pipe method of writing dplyr code that was discussed in Section 11.4. Since the dummy column has a value of 1 for each shooting, well sum up this column to get the number of shootings each month/year. library(dplyr) monthly_shootings &lt;- shootings %&gt;% group_by(month_year) %&gt;% summarize(dummy = sum(dummy)) head(monthly_shootings) # # A tibble: 6 x 2 # month_year dummy # &lt;date&gt; &lt;dbl&gt; # 1 2015-01-01 76 # 2 2015-02-01 77 # 3 2015-03-01 92 # 4 2015-04-01 84 # 5 2015-05-01 71 # 6 2015-06-01 65 Since we now have a variable that shows the number of people killed each month, we can graph this new data set. Well use the same process as earlier, but our data set is now monthly_shootings instead of shootings and the x-axis variable is month_year instead of date. ggplot(monthly_shootings, aes( x = month_year, y = dummy )) + geom_line() The process is the same for yearly data. yearly_shootings &lt;- shootings %&gt;% group_by(year) %&gt;% summarize(dummy = sum(dummy)) ggplot(yearly_shootings, aes( x = year, y = dummy )) + geom_line() Note the steep drop-off at the end of each graph. Is that due to fewer shooting occurring more recently? No, its simply an artifact of the graph comparing whole months (years) to parts of a month (year) since we havent finished this month (year) yet. 15.5 Pretty graphs Whats next for these graphs? Youll likely want to add labels for the axes and the title. We went over how to do this in Section 14.3 so please refer to that for more info. Also, check out ggplot2s website to see more on this very versatile package. As Ive said all chapter, a lot of this is going to be personal taste so please spend some time exploring the package and changing the appearance of the graph to learn what looks right to you. 15.5.1 Themes In addition to making changes to the graphs appearance yourself, you can use a theme that someone else made. A theme is just a collection of changes to the graphs appearance that someone put in a function for others to use. Each theme is different and is fairly opinionated, so you should only use one that you think looks best for your graph. To use a theme, simply add the function for that theme to your ggplot using the + as normal. ggplot2 comes with a series of themes that you can look at here. Here, well be looking at themes from the ggthemes package, which is a great source of different themes to modify the appearance of your graph. Check out this website to see a depiction of all of the possible themes. If you dont have the ggthemes package installed, do so using install.packages(\"ggthemes\"). install.packages(&quot;ggthemes&quot;) Lets do a few examples using the graph made above. First, well need to load the ggthemes library. library(ggthemes) # # Attaching package: &#39;ggthemes&#39; # The following object is masked from &#39;package:cowplot&#39;: # # theme_map ggplot(yearly_shootings, aes( x = year, y = dummy )) + geom_line() + theme_fivethirtyeight() ggplot(yearly_shootings, aes( x = year, y = dummy )) + geom_line() + theme_tufte() ggplot(yearly_shootings, aes( x = year, y = dummy )) + geom_line() + theme_few() ggplot(yearly_shootings, aes( x = year, y = dummy )) + geom_line() + theme_excel() The Washington Post is continuing to collect this data so if you look on their site youll find more up-to-date data. It is especially important to not overreach when trying to answer a question when the data cant do it well. Often, no answer is better than a wrong one - especially in a field with serious consequences like criminology. This isnt to say that you should never try to answer questions since no data is perfect and you may be wrong. You should try to develop a deep understanding of the data and be confident that you can actually answer those questions with confidence. "],["hotspot-maps.html", "16 Hotspot maps 16.1 A simple map 16.2 What really are maps? 16.3 Making a hotspot map", " 16 Hotspot maps For this chapter youll need the following file, which is available for download here: san_francisco_suicide_2003_2017.csv. Hotspot maps are used to find where events or places (e.g. crimes, marijuana dispensaries, liquors stores) are especially prevalent. These maps are frequently used by police departments, particularly in determining where to do hotspot policing (which focuses patrols on high-crime areas). However, there are significant flaws with these kinds of maps. As well see during this lesson, minor changes to how we make the maps can cause significant differences in interpretation. For example, determining the size of the clusters that make up the hotspots can make it seem like there are much larger or smaller areas with hotspots than there actually are. These clusters are also drawn fairly arbitrarily, without considering context such as neighborhoods (in Chapter 17 well make maps that try to account for these types of areas). This makes it more difficult to interpret because even though maps give us the context of location, it can combine different areas in an arbitrary way. Hotspot maps also often turn into population maps, where the dots indicate where people live rather than where the risk of something is. For example, a street with several apartment buildings will likely have more crimes (and thus have more dots on a hotspot map) than a street with only single-family homes. Maybe this is because the apartment street really is more crime-ridden than the single-family home street, but it could simply be that places with more people have more events (e.g. crimes, suicides, etc.) even if they actually have a lower rate of these events than less populated places. So not knowing the context of an area can make hotspot maps very misleading. Well explore these issues in more detail throughout the lesson but keep in mind these risks as you make your own hotspot maps. Here, we will make hotspot maps using data on suicides in San Francisco between 2003 and 2017. First, we need to read the data, which is called san_francisco_suicide_2003_2017.csv. We can name the object we make suicide. library(readr) suicide &lt;- read_csv(&quot;data/san_francisco_suicide_2003_2017.csv&quot;) suicide &lt;- as.data.frame(suicide) This data contains information on each crime reported in San Francisco including the type of crime (in our case always suicide), a more detailed crime category, and a number of date and location variables. Please note that suicide is not actually a crime, even though it is included in the San Francisco Police Departments crime data. As shown in Chapter 12, which also used San Francisco crime data, there are a number of other non-crimes included such as Fire Report, Traffic Collision, and Non-Criminal. This is a fairly common occurrence in crime data where it also includes non-crimes that the police generally respond to so it is important to carefully examine your data to see what is included. Simply summing up the rows as a measure of crime will generally overcount crimes. The columns X and Y are our longitude and latitude columns, which we will use to map the data. head(suicide) # IncidntNum Category Descript DayOfWeek Date Time PdDistrict Resolution # 1 180318931 SUICIDE ATTEMPTED SUICIDE BY STRANGULATION Monday 04/30/2018 06:30:00 TARAVAL NONE # 2 180315501 SUICIDE ATTEMPTED SUICIDE BY JUMPING Saturday 04/28/2018 17:54:00 NORTHERN NONE # 3 180295674 SUICIDE SUICIDE BY LACERATION Saturday 04/21/2018 12:20:00 RICHMOND NONE # 4 180263659 SUICIDE SUICIDE Tuesday 04/10/2018 05:13:00 CENTRAL NONE # 5 180235523 SUICIDE ATTEMPTED SUICIDE BY INGESTION Friday 03/30/2018 09:15:00 TARAVAL NONE # 6 180236515 SUICIDE SUICIDE BY ASPHYXIATION Thursday 03/29/2018 17:30:00 RICHMOND NONE # Address X Y Location PdId year # 1 0 Block of BRUCE AV -122.4517 37.72218 POINT (-122.45168059935614 37.72218061554315) 1.803189e+13 2018 # 2 700 Block of HAYES ST -122.4288 37.77620 POINT (-122.42876060987851 37.77620120112792) 1.803155e+13 2018 # 3 3700 Block of CLAY ST -122.4546 37.78818 POINT (-122.45462091999406 37.7881754224736) 1.802957e+13 2018 # 4 0 Block of DRUMM ST -122.3964 37.79414 POINT (-122.39642194376758 37.79414474237039) 1.802637e+13 2018 # 5 0 Block of FAIRFIELD WY -122.4632 37.72679 POINT (-122.46324153155875 37.72679184368551) 1.802355e+13 2018 # 6 300 Block of 29TH AV -122.4893 37.78274 POINT (-122.48929119750689 37.782735835121265) 1.802365e+13 2018 16.1 A simple map To make these maps we will use the package ggmap. install.packages(&quot;ggmap&quot;) library(ggmap) # Google&#39;s Terms of Service: https://cloud.google.com/maps-platform/terms/. # Please cite ggmap if you use it! See citation(&quot;ggmap&quot;) for details. # # Attaching package: &#39;ggmap&#39; # The following object is masked from &#39;package:cowplot&#39;: # # theme_nothing # The following object is masked from &#39;package:tidygeocoder&#39;: # # geocode Well start by making the background to our map, showing San Francisco. We do so by using the get_map() function from ggmap, which gets a map background from a number of sources. Well set the source to stamen since Google no longer allows us to get a map without creating an account. The first parameter in get_map() is simply coordinates for San Franciscos bounding box to ensure we get a map of the right spot. A bounding box is four coordinates that connect to make a rectangle, used for determining where in the world to show. An easy way to find the four coordinates for a bounding box is to go to the site Bounding Box. This site has a map of the world and a box on the screen. Move the box to the area you want the map of. You may need to resize the box to cover the area you want. Then in the section that says Copy &amp; Paste, change the dropdown box to CSV. In the section to the right of this are the four numbers that make up the bounding box. You can copy those numbers into get_map() sf_map &lt;- ggmap(get_map(c(-122.530392, 37.698887, -122.351177, 37.812996), source = &quot;stamen&quot; )) sf_map Since we saved the map output into sf_map we can reuse this map background for all the maps were making in this lesson. This saves us time as we dont have to wait to download the map every time. Lets plot the suicides from our data set. Just as with a scatterplot we use the geom_point() function from the ggplot2 package and set our longitude and latitude variables on the x- and y-axis, respectively. When we load ggmap it also automatically loads ggplot2 as that package is necessary for ggmap to work, so we dont need to do library(ggplot2) ourselves. sf_map + geom_point(aes(x = X, y = Y), data = suicide ) # Warning: Removed 1 rows containing missing values (geom_point). If we wanted to color the dots, we can use color = and then select a color. Lets try it with forestgreen. sf_map + geom_point(aes(x = X, y = Y), data = suicide, color = &quot;forestgreen&quot; ) # Warning: Removed 1 rows containing missing values (geom_point). As with other graphs we can change the size of the dot using size =. sf_map + geom_point(aes(x = X, y = Y), data = suicide, color = &quot;forestgreen&quot;, size = 0.5 ) # Warning: Removed 1 rows containing missing values (geom_point). sf_map + geom_point(aes(x = X, y = Y), data = suicide, color = &quot;forestgreen&quot;, size = 2 ) # Warning: Removed 1 rows containing missing values (geom_point). For maps like this - with one point per event - it is hard to tell if any events happen on the same, or nearly the same, location as each point is solid green. We want to make the dots semi-transparent so if multiple suicides happen at the same place that dot will be shaded darker than if only one suicide happened there. To do so we use the parameter alpha = which takes an input between 0 and 1 (inclusive). The lower the value the more transparent it is. sf_map + geom_point(aes(x = X, y = Y), data = suicide, color = &quot;forestgreen&quot;, size = 2, alpha = 0.5 ) # Warning: Removed 1 rows containing missing values (geom_point). This map is useful because it allows us to easily see where each suicide in San Francisco happened between 2003 and 2017. There are some limitations though. For example, this shows all suicides in a single map, meaning that any time trends are lost. 16.2 What really are maps? Lets pause for a moment to think about what a map really is. I made the following simple scatterplot of our data with one dot per suicide (minus the one without coordinates). Compare this to the previous map and youll see that they are the same except the map has a useful background while the plot has a blank background. That is all static maps are (in Chapter 18 well learn about interactive maps), scatterplots of coordinates overlayed on a map background. Basically, they are scatterplots with context. And this context is useful; we can interpret the map to see that there are lots of suicides in the northeast part of San Francisco but not so many elsewhere, for example. The exact same pattern is present in the scatterplot but without the ability to tell where a dot is. plot(suicide$X, suicide$Y, col = &quot;forestgreen&quot;) 16.3 Making a hotspot map Now we can start making hotspot maps, which help to show areas with clusters of events. Well do this using hexagonal bins, which are an efficient way of showing clusters of events on a map. Our syntax will be similar to the map above, but now we want to use the function stat_binhex() rather than geom_point(). It starts the same as before with aes(x = X, y = Y) (or whatever the longitude and latitude columns are called in your data), as well as data = suicide outside of the aes() parameter. There are two new things we need to make the hotspot map. First, we add the parameter bins = number_of_bins where number_of_bins is a number we select. bins essentially says how large or small we want each cluster of events to be. A smaller value for bins says we want more events clustered together, making larger bins. A larger value for bins has each bin be smaller on the map and capture fewer events. This will become clearer with examples. The second thing is to add the function coord_cartesian(), which just tells ggplot() we are going to do some spatial analysis in the making of the bins. We dont need to add any parameters in this. To use stat_binhex(), well also need to make sure that the package hexbin is installed. stat_binhex() will call the necessary function from hexbin internally so we dont need to run library(hexbin). install.packages(&quot;hexbin&quot;) Lets start with 60 bins and then try some other number of bins to see how it changes the map. sf_map + stat_binhex(aes(x = X, y = Y), bins = 60, data = suicide ) + coord_cartesian() # Warning: Removed 1 rows containing non-finite values (stat_binhex). From this map we can see that most areas in the city had no suicides and that the areas with the most suicides are in downtown San Francisco. What happens when we drop the number of bins to 30? sf_map + stat_binhex(aes(x = X, y = Y), bins = 30, data = suicide ) + coord_cartesian() # Warning: Removed 1 rows containing non-finite values (stat_binhex). Each bin is much larger and covers nearly all of San Francisco. Be careful with maps like these! This map is so broad that it appears that suicides are ubiquitous across the city. We know from the map showing each suicide as a dot that there are fewer than 1,300 suicides; thus this is not true. Maps like this make it easy to mislead the reader, including yourself! What about looking at 100 bins? sf_map + stat_binhex(aes(x = X, y = Y), bins = 100, data = suicide ) + coord_cartesian() # Warning: Removed 1 rows containing non-finite values (stat_binhex). Now each bin is very small and a much smaller area in San Francisco has had a suicide. So what is the right number of bins to use? There is no correct universal answer - you must decide what the goal is with the data you are using. This opens up serious issues for manipulation - intentional or not - of the data as the map is so easily changeable without ever changing the data itself. 16.3.1 Colors To change the bin colors we can use the parameter scale_fill_gradient(). This accepts a color for low, which is when the events are rare, and high for the bins with frequent events. Well use colors from ColorBrewer, selecting the yellow-reddish theme (3-class YlOrRd) from the Multi-hue section of the sequential data part of the page. sf_map + stat_binhex(aes(x = X, y = Y), bins = 60, data = suicide ) + coord_cartesian() + scale_fill_gradient( low = &quot;#ffeda0&quot;, high = &quot;#f03b20&quot; ) # Warning: Removed 1 rows containing non-finite values (stat_binhex). By default it labels the legend as count. Since we know these are counts of suicides lets relabel that as such. sf_map + stat_binhex(aes(x = X, y = Y), bins = 60, data = suicide ) + coord_cartesian() + scale_fill_gradient(&quot;Suicides&quot;, low = &quot;#ffeda0&quot;, high = &quot;#f03b20&quot; ) # Warning: Removed 1 rows containing non-finite values (stat_binhex). "],["choropleth-maps.html", "17 Choropleth maps 17.1 Spatial joins 17.2 Making choropleth maps", " 17 Choropleth maps For this chapter youll need the following files, which are available for download here: san_francisco_suicide_2003_2017.csv, san_francisco_neighborhoods.dbf, san_francisco_neighborhoods.prj, san_francisco_neighborhoods.shp, san_francisco_neighborhoods.shx. In Chapter 16 we made hotspot maps to show which areas in San Francisco had the most suicides. We made the maps in a number of ways and consistently found that suicides were most prevalent in northeast San Francisco. In this chapter we will make choropleth maps, which are shaded maps where each unit is some known geographic area, such as a state or neighborhood. Think of election maps where states are colored blue when a Democratic candidate wins that state and red when a Republican candidate wins. These are choropleth maps - each state is colored to indicate something. In this chapter we will continue to work on the suicide data and make choropleth maps shaded by the number of suicides in each neighborhood (we will define this later in the chapter) in the city. Since we will be working more on the suicide data from San Francisco, lets read it in now. library(readr) suicide &lt;- read_csv(&quot;data/san_francisco_suicide_2003_2017.csv&quot;) suicide &lt;- as.data.frame(suicide) The package that we will use to handle geographic data and do most of the work in this chapter is sf. sf is a sophisticated package and does far more than what we will cover in this chapter. For more information about the packages features please see the website for it here. install.packages(&quot;sf&quot;) library(sf) For this chapter we will need to read in a shapefile that depicts the boundaries of each neighborhood in San Francisco. A shapefile is similar to a data.frame but has information on how to draw a geographic boundary such as a state. The way sf reads in the shapefiles is through the st_read() function. Our input inside the () is a string with the name of the .shp file we want to read in (since we are telling R to read a file on the computer rather than an object that exists, it needs to be in quotes). This shapefile contains neighborhoods in San Francisco so well call the object sf_neighborhoods. I downloaded this data from San Franciscos Open Data site here, selecting the Shapefile format in the Export tab. If you do so yourself itll give you a zip file with multiple files in there. This is normal with shapefiles, you will have multiple files and only read in the file with the .shp extension to R. We still do need all of the files, and st_read() is using them even if not explicitly called. So make sure every file downloaded is in the same working directory as the .shp file. The files from this site had hard-to-read file names, so I relabeled them all as san_francisco_neighborhoods though that doesnt matter once its read into R. sf_neighborhoods &lt;- st_read(&quot;data/san_francisco_neighborhoods.shp&quot;, quiet = TRUE ) As usual when dealing with a new data set, lets look at the first 6 rows. head(sf_neighborhoods) # Simple feature collection with 6 features and 1 field # Geometry type: MULTIPOLYGON # Dimension: XY # Bounding box: xmin: -122.4543 ymin: 37.70822 xmax: -122.357 ymax: 37.80602 # Geodetic CRS: WGS84(DD) # nhood geometry # 1 Bayview Hunters Point MULTIPOLYGON (((-122.3816 3... # 2 Bernal Heights MULTIPOLYGON (((-122.4036 3... # 3 Castro/Upper Market MULTIPOLYGON (((-122.4266 3... # 4 Chinatown MULTIPOLYGON (((-122.4062 3... # 5 Excelsior MULTIPOLYGON (((-122.424 37... # 6 Financial District/South Beach MULTIPOLYGON (((-122.3875 3... The last column is important. In shapefiles, the geometry column is the one with the instructions to make the map. This data has a single row for each neighborhood in the city. So the geometry column in each row has a list of coordinates, which, if connected in order, make up that neighborhood. Since the geometry column contains the instructions to map, we can plot() it to show a map of the data. plot(sf_neighborhoods$geometry) Here we have a map of San Francisco broken up into neighborhoods. Is this a perfect representation of the neighborhoods in San Francisco? No. It is simply the citys attempt to create definitions of neighborhoods. Indeed, youre likely to find that areas at the border of neighborhoods are more similar to each other than they are to areas at the opposite side of their designated neighborhood. You can read a bit about how San Francisco determined the neighborhood boundaries here, but know that this, like all geographic areas that someone has designated, has some degree of inaccuracy and arbitrariness in it. Like many things in criminology, this is just another limitation we will have to keep in mind. In the head() results there was a section about something called epsg and proj4string. Lets talk about that specifically since they are important for working with spatial data. An issue with working with geographic data is that the Earth is not flat. Since the Earth is spherical, there will always be some distortion when trying to plot the data on a flat surface such as a map. To account for this, we need to transform the longitude and latitude values we have to work properly on a map. We do so by projecting our data onto the areas of the Earth we want. This is a complex field with lots of work done on it (both abstractly and for R specifically) so this chapter will be an extremely brief overview of the topic and oversimplify some aspects of it. If we look at the output of st_crs(sf_neighborhoods) we can see that the EPSG is set to 4326 and the proj4string (which tells us the current map projection) is +proj=longlat +datum=WGS84 +no_defs. This CRS, WGS84, is a standard CRS and is the one used whenever you use a GPS to find a location. To find the CRS for certain parts of the world see here. If you search that site for California, youll see that California is broken into 6 zones. The site isnt that helpful on which zones are which, but some Googling can often find state or region maps with the zones depicted there. We want California zone 3, which has the EPSG code 2227. Well use this code to project this data properly. If we want to get the proj4string for 2227 we can run st_crs(2227). Im not running it here because it will print out a large amount of text, but you should run it on your own computer. Note the text in text in this output includes US survey foot. This means that the units are in feet. Some projections have units in meters so be mindful of this when doing some analysis, such as seeing if a point is within X feet of a certain area. Lets convert our sf_neighborhoods data to coordinate reference system 2227 using st_transform(). sf_neighborhoods &lt;- st_transform(sf_neighborhoods, crs = 2227) st_crs(sf_neighborhoods) 17.1 Spatial joins What we want to do with these neighborhoods is to find out which neighborhood each suicide occurred in and sum up the number of suicides per neighborhood. Once we do that, we can make a map at the neighborhood level and be able to measure suicides per neighborhood. A spatial join is very similar to regular joins where we merge two data sets based on common variables (such as state name or unique ID code of a person). In this case it merges based on some shared geographic feature such as if two lines intersect or (as we will do so here) if a point is within some geographic area. Right now our suicide data is in a data.frame with some info on each suicide and the longitude and latitude of the suicide in separate columns. We want to turn this data.frame into a spatial object to allow us to find which neighborhood each suicide happened in. We can convert it into a spatial object using the st_as_sf() function from sf. Our input is first our data, suicide. Then in the coords parameter we put a vector of the column names so the function knows which columns the longitude and latitude columns are so it can convert those columns to a geometry column like we saw in sf_neighborhoods earlier. Well set the CRS to be the WGS84 standard we saw earlier, but we will change it to match the CRS that the neighborhood data has. suicide &lt;- st_as_sf(suicide, coords = c(&quot;X&quot;, &quot;Y&quot;), crs = &quot;+proj=longlat +ellps=WGS84 +no_defs&quot; ) We want our suicides data in the same projection as the neighborhoods data so we need to use st_transform() to change the projection. Since we want the CRS to be the same as in sf_neighborhoods, we can set it using st_crs(sf_neighborhoods) to use the right CRS. suicide &lt;- st_transform(suicide, crs = st_crs(sf_neighborhoods) ) Now we can take a look at head() to see if it was projected. head(suicide) # Simple feature collection with 6 features and 12 fields # Geometry type: POINT # Dimension: XY # Bounding box: xmin: -122.4893 ymin: 37.72218 xmax: -122.3964 ymax: 37.79414 # Geodetic CRS: WGS84(DD) # IncidntNum Category Descript DayOfWeek Date Time PdDistrict Resolution # 1 180318931 SUICIDE ATTEMPTED SUICIDE BY STRANGULATION Monday 04/30/2018 06:30:00 TARAVAL NONE # 2 180315501 SUICIDE ATTEMPTED SUICIDE BY JUMPING Saturday 04/28/2018 17:54:00 NORTHERN NONE # 3 180295674 SUICIDE SUICIDE BY LACERATION Saturday 04/21/2018 12:20:00 RICHMOND NONE # 4 180263659 SUICIDE SUICIDE Tuesday 04/10/2018 05:13:00 CENTRAL NONE # 5 180235523 SUICIDE ATTEMPTED SUICIDE BY INGESTION Friday 03/30/2018 09:15:00 TARAVAL NONE # 6 180236515 SUICIDE SUICIDE BY ASPHYXIATION Thursday 03/29/2018 17:30:00 RICHMOND NONE # Address Location PdId year # 1 0 Block of BRUCE AV POINT (-122.45168059935614 37.72218061554315) 1.803189e+13 2018 # 2 700 Block of HAYES ST POINT (-122.42876060987851 37.77620120112792) 1.803155e+13 2018 # 3 3700 Block of CLAY ST POINT (-122.45462091999406 37.7881754224736) 1.802957e+13 2018 # 4 0 Block of DRUMM ST POINT (-122.39642194376758 37.79414474237039) 1.802637e+13 2018 # 5 0 Block of FAIRFIELD WY POINT (-122.46324153155875 37.72679184368551) 1.802355e+13 2018 # 6 300 Block of 29TH AV POINT (-122.48929119750689 37.782735835121265) 1.802365e+13 2018 # geometry # 1 POINT (-122.4517 37.72218) # 2 POINT (-122.4288 37.7762) # 3 POINT (-122.4546 37.78818) # 4 POINT (-122.3964 37.79414) # 5 POINT (-122.4632 37.72679) # 6 POINT (-122.4893 37.78274) We can see it is now a simple feature collection with the correct projection. And we can see there is a new column called geometry just like in sf_neighborhoods. The type of data in geometry is POINT since our data is just a single location instead of a polygon like in the neighborhoods data. Since we have both the neighborhoods and the suicides data lets make a quick map to see the data. plot(sf_neighborhoods$geometry) plot(suicide$geometry, add = TRUE, col = &quot;red&quot;) Our next step is to combine these two data sets to figure out how many suicides occurred in each neighborhood. This will be a multi-step process so lets plan it out before beginning. Our suicide data is one row for each suicide; our neighborhood data is one row for each neighborhood. Since our goal is to map at the neighborhood-level we need to get the neighborhood where each suicide occurred then aggregate up to the neighborhood-level to get a count of the suicides-per-neighborhood. Then we need to combine that with the original neighborhood data, and we can map it. Find which neighborhood each suicide happened in Aggregate suicide data until we get one row per neighborhood and a column showing the number of suicides in that neighborhood Combine with the neighborhood data Make a map Well start by finding the neighborhood where each suicide occurred using the function st_join(), which is a function in sf. This does a spatial join and finds the polygon (neighborhood in our case) where each point is located in. Since we will be aggregating the data, lets call the output of this function suicide_agg. The order in the () is important! For our aggregation we want the output to be at the suicide-level so we start with the suicide data. In the next step well see why this matters. suicide_agg &lt;- st_join(suicide, sf_neighborhoods) Lets look at the first 6 rows. head(suicide_agg) # Simple feature collection with 6 features and 13 fields # Geometry type: POINT # Dimension: XY # Bounding box: xmin: -122.4893 ymin: 37.72218 xmax: -122.3964 ymax: 37.79414 # Geodetic CRS: WGS84(DD) # IncidntNum Category Descript DayOfWeek Date Time PdDistrict Resolution # 1 180318931 SUICIDE ATTEMPTED SUICIDE BY STRANGULATION Monday 04/30/2018 06:30:00 TARAVAL NONE # 2 180315501 SUICIDE ATTEMPTED SUICIDE BY JUMPING Saturday 04/28/2018 17:54:00 NORTHERN NONE # 3 180295674 SUICIDE SUICIDE BY LACERATION Saturday 04/21/2018 12:20:00 RICHMOND NONE # 4 180263659 SUICIDE SUICIDE Tuesday 04/10/2018 05:13:00 CENTRAL NONE # 5 180235523 SUICIDE ATTEMPTED SUICIDE BY INGESTION Friday 03/30/2018 09:15:00 TARAVAL NONE # 6 180236515 SUICIDE SUICIDE BY ASPHYXIATION Thursday 03/29/2018 17:30:00 RICHMOND NONE # Address Location PdId year # 1 0 Block of BRUCE AV POINT (-122.45168059935614 37.72218061554315) 1.803189e+13 2018 # 2 700 Block of HAYES ST POINT (-122.42876060987851 37.77620120112792) 1.803155e+13 2018 # 3 3700 Block of CLAY ST POINT (-122.45462091999406 37.7881754224736) 1.802957e+13 2018 # 4 0 Block of DRUMM ST POINT (-122.39642194376758 37.79414474237039) 1.802637e+13 2018 # 5 0 Block of FAIRFIELD WY POINT (-122.46324153155875 37.72679184368551) 1.802355e+13 2018 # 6 300 Block of 29TH AV POINT (-122.48929119750689 37.782735835121265) 1.802365e+13 2018 # nhood geometry # 1 Oceanview/Merced/Ingleside POINT (-122.4517 37.72218) # 2 Hayes Valley POINT (-122.4288 37.7762) # 3 Presidio Heights POINT (-122.4546 37.78818) # 4 Financial District/South Beach POINT (-122.3964 37.79414) # 5 West of Twin Peaks POINT (-122.4632 37.72679) # 6 Outer Richmond POINT (-122.4893 37.78274) There is now the nhood column from the neighborhoods data, which says which neighborhood the suicide happened in. Now we can aggregate up to the neighborhood-level using group_by() and summarize() functions from the dplyr package. We actually dont have a variable with the number of suicides so we need to make that. We can simply call it number_suicides and give it the value of 1 since each row is only one suicide. suicide_agg$number_suicides &lt;- 1 Now we can aggregate the data and assign the results back into suicide_agg. library(dplyr) suicide_agg &lt;- suicide_agg %&gt;% group_by(nhood) %&gt;% summarize(number_suicides = sum(number_suicides)) Lets check a summary of the number_suicides variable we made. summary(suicide_agg$number_suicides) # Min. 1st Qu. Median Mean 3rd Qu. Max. # 1.00 13.50 23.50 32.30 37.25 141.00 The minimum is one suicide per neighborhood, 32 on average, and 141 in the neighborhood with the most suicides. So what do we make of this data? Well, there are some data issues that cause problems in these results. Lets think about the minimum value. Did every single neighborhood in the city have at least one suicide? No. Take a look at the number of rows in this data, keeping in mind there should be one row per neighborhood. nrow(suicide_agg) # [1] 40 And lets compare it to the sf_neighborhoods data. nrow(sf_neighborhoods) # [1] 41 The suicides data is missing 2 neighborhoods (one of the 40 values is missing and is NA, not a real neighborhood). That is because if no suicides occurred there, there would never be a matching row in the data so that neighborhood wouldnt appear in the suicide data. Thats not going to be a major issue here but is something to keep in mind if this were a real research project. The data is ready to merge with the sf_neighborhoods data. Well introduce a new function that makes merging data simple. This function also comes from the dplyr package. The function we will use is left_join(), which takes two parameters, the two data sets to join together. left_join(data1, data2) This function joins these data and keeps all of the rows from the left data and every column from both data sets. It combines the data based on any matching columns (matching meaning same column name) in both data sets. Since in our data sets, the column nhood exists in both, it will merge the data based on that column. There are two other functions that are similar but differ based on which rows they keep. left_join() - All rows from Left data and all columns from Left and Right data right_join() - All rows from Right data and all columns from Left and Right data full_join() - All rows and all columns from Left and Right data We could alternatively use the merge() function, which is built into R, but that function is slower than the dplyr functions and requires us to manually set the matching columns. We want to keep all rows in sf_neighborhoods (keep all neighborhoods) so we can use left_join(sf_neighborhoods, suicide_agg). Lets assign the results to a new data set called sf_neighborhoods_suicide. We dont need the spatial data for suicide_agg anymore, and it will cause problems with our join if we keep it, so lets delete the geometry column from that data. We can do this by assigning the column the value of NULL. suicide_agg$geometry &lt;- NULL Now we can do our join. sf_neighborhoods_suicide &lt;- left_join(sf_neighborhoods, suicide_agg) # Joining, by = &quot;nhood&quot; If we look at summary() again for number_suicides we can see that there are now two rows with NAs. These are the neighborhoods where there were no suicides so they werent present in the suicide_agg data. summary(sf_neighborhoods_suicide$number_suicides) # Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s # 1.00 15.00 24.00 33.08 38.50 141.00 2 We need to convert these values to 0. We will use the is.na() function to conditionally find all rows with an NA value in the number_suicides column and use square bracket notation to change the value to 0. sf_neighborhoods_suicide$number_suicides[ is.na(sf_neighborhoods_suicide$number_suicides) ] &lt;- 0 Checking it again we see that the minimum is now 0 and the mean number of suicides decreases a bit to about 31.5 per neighborhood. summary(sf_neighborhoods_suicide$number_suicides) # Min. 1st Qu. Median Mean 3rd Qu. Max. # 0.00 12.00 23.00 31.46 36.00 141.00 17.2 Making choropleth maps Finally we are ready to make some choropleth maps. For these maps we are going to use ggplot2 again so we need to load it. library(ggplot2) ggplot2s benefit is you can slowly build graphs or maps and improve the graph at every step. Earlier, we used functions such as geom_line() for line graphs and geom_point() for scatter plots. For mapping these polygons we will use geom_sf(), which knows how to handle spatial data. As usual we will start with ggplot(), inputting our data first. Then inside of aes (the aesthetics of the graph/map) we use a new parameter fill. In fill we will put in the number_suicides column, and it will color the polygons (neighborhoods) based on values in that column. Then we can add the geom_sf(). ggplot(sf_neighborhoods_suicide, aes(fill = number_suicides)) + geom_sf() We have now created a choropleth map showing the number of suicides per neighborhood in San Francisco! Based on the legend, neighborhoods that are light blue have the most suicides while neighborhoods that are dark blue have the fewest (or none at all). Normally wed want the opposite, with darker areas signifying a greater amount of whatever the map is showing. We can use scale_fill_gradient() to set the colors to what we want. We input a color for low value and a color for high value, and itll make the map shade by those colors. ggplot( sf_neighborhoods_suicide, aes(fill = number_suicides) ) + geom_sf() + scale_fill_gradient( low = &quot;white&quot;, high = &quot;red&quot; ) This gives a much better map and clearly shows the areas where suicides are most common and where there were no suicides. To make this map easier to read and look better, lets add a title to the map and to the legend. ggplot( sf_neighborhoods_suicide, aes(fill = number_suicides) ) + geom_sf() + scale_fill_gradient( low = &quot;white&quot;, high = &quot;red&quot; ) + labs( fill = &quot;# of suicides&quot;, title = &quot;Suicides in San Francisco, by neighborhood&quot;, subtitle = &quot;2003 - 2017&quot; ) Since the coordinates dont add anything to the map, lets get rid of them. ggplot( sf_neighborhoods_suicide, aes(fill = number_suicides) ) + geom_sf() + scale_fill_gradient( low = &quot;white&quot;, high = &quot;red&quot; ) + labs( fill = &quot;# of suicides&quot;, title = &quot;Suicides in San Francisco, by neighborhood&quot;, subtitle = &quot;2003 - 2017&quot; ) + theme( axis.text.x = element_blank(), axis.text.y = element_blank(), axis.ticks = element_blank() ) So what should we take away from this map? There are more suicides in the downtown area than any other place in the city. Does this mean that people are more likely to kill themselves there than elsewhere? Not necessarily. A major mistake people make when making a choropleth map (or really any type of map) is accidentally making a population map. The darker shaded parts of our map are also where a lot of people live. So if there are more people, it is reasonable that there would be more suicides (or crimes, etc.). What wed really want to do is make a rate per some population (usually per 100k though this assumes equal risk for every person in the city which isnt really correct) to control for population differences. Well use this data in Chapter 18 to make interactive choropleth maps so lets save it. save(sf_neighborhoods_suicide, file = &quot;data/sf_neighborhoods_suicide.rda&quot;) "],["interactive-maps.html", "18 Interactive maps 18.1 Why do interactive graphs matter? 18.2 Making the interactive map 18.3 Adding popup information 18.4 Dealing with too many markers 18.5 Interactive choropleth maps", " 18 Interactive maps For this chapter youll need the following files, which are available for download here: san_francisco_marijuana_geocoded.csv and sf_neighborhoods_suicide.rda. While maps of data are useful, their ability to show incident-level information is quite limited. They tend to show broad trends - where crime happened in a city - rather than provide information about specific crime incidents. While broad trends are important, there are significant drawbacks about being unable to get important information about an incident without having to check the data. An interactive map bridges this gap by showing trends while allowing you to zoom into individual incidents and see information about each incident. For this lesson we will be using data on every marijuana dispensary in San Francisco that has an active dispensary license as of late September 2019. The file is called san_francisco_marijuana_geocoded.csv. When downloaded from Californias Bureau of Cannabis Control (here if youre interested) the data contains the address of each dispensary but does not have coordinates. Without coordinates we are unable to map points, meaning we need to geocode them. Geocoding is the process of taking an address and getting the longitude and latitude of that address for mapping. For this lesson Ive already geocoded the data, and well learn how to do so in Chapter 24. library(readr) marijuana &lt;- read_csv(&quot;data/san_francisco_marijuana_geocoded.csv&quot;) marijuana &lt;- as.data.frame(marijuana) 18.1 Why do interactive graphs matter? 18.1.1 Understanding your data The most important thing to learn from this book is that understanding your data is crucial to good research. Making interactive maps is a very useful way to better understand your data as you can immediately see geographic patterns and quickly look at characteristics of those incidents to understand them. In this lesson we will make a map of each marijuana dispensary in San Francisco that lets you click on the dispensary and see some information about it. If we see a cluster of dispensaries, we can click on each one to see if they are similar - for example, if owned by the same person. Though it is possible to find these patterns just looking at the data, it is easier to be able to see a geographic pattern and immediately look at information about each incident. 18.1.2 Police departments use them Interactive maps are popular in large police departments, such as Philadelphia and New York City. They allow easy understanding of geographic patterns in the data and, importantly, allow such access to people who do not have the technical skills necessary to interact with the data itself. If nothing else, learning interactive maps may help you with a future job. 18.2 Making the interactive map As usual, lets take a look at the top 6 rows of the data. head(marijuana) # License_Number License_Type Business_Owner Business_Structure # 1 C10-0000614-LIC Cannabis - Retailer License Terry Muller Limited Liability Company # 2 C10-0000586-LIC Cannabis - Retailer License Jeremy Goodin Corporation # 3 C10-0000587-LIC Cannabis - Retailer License Justin Jarin Corporation # 4 C10-0000539-LIC Cannabis - Retailer License Ondyn Herschelle Corporation # 5 C10-0000522-LIC Cannabis - Retailer License Ryan Hudson Limited Liability Company # 6 C10-0000523-LIC Cannabis - Retailer License Ryan Hudson Limited Liability Company # Premise_Address Status Issue_Date Expiration_Date Activities # 1 2165 IRVING ST san francisco, CA 94122 Active 9/13/2019 9/12/2020 N/A for this license type # 2 122 10TH ST SAN FRANCISCO, CA 941032605 Active 8/26/2019 8/25/2020 N/A for this license type # 3 843 Howard ST SAN FRANCISCO, CA 94103 Active 8/26/2019 8/25/2020 N/A for this license type # 4 70 SECOND ST SAN FRANCISCO, CA 94105 Active 8/5/2019 8/4/2020 N/A for this license type # 5 527 Howard ST San Francisco, CA 94105 Active 7/29/2019 7/28/2020 N/A for this license type # 6 2414 Lombard ST San Francisco, CA 94123 Active 7/29/2019 7/28/2020 N/A for this license type # Adult-Use/Medicinal lat long # 1 BOTH 37.76318 -122.4811 # 2 BOTH 37.77480 -122.4157 # 3 BOTH 37.78228 -122.4035 # 4 BOTH 37.78823 -122.4004 # 5 BOTH 37.78783 -122.3965 # 6 BOTH 37.79944 -122.4414 This data has information about the type of license, who the owner is, and where the dispensary is (as an address and as coordinates). Well be making a map showing every dispensary in the city and make it so when you click a dot itll make a popup showing information about that dispensary. We will use the package leaflet for our interactive map. leaflet produces maps similar to Google Maps with circles (or any icon we choose) for each value we add to the map. It allows you to zoom in, scroll around, and provides context to each incident that isnt available on a static map. install.packages(&quot;leaflet&quot;) library(leaflet) # Warning: package &#39;leaflet&#39; was built under R version 4.1.3 To make a leaflet map we need to run the function leaflet() and add a tile to the map. We can just use the default tile which doesnt need an input. If youre interested in other tiles, please see this website. We will use a standard tile from Open Street Maps. This tile gives street names and highlights important features such as parks and large stores which provides useful contexts for looking at the data. leaflet() %&gt;% addTiles() When you run the above code it shows a world map (copied several times). Zoom into it, and itll start showing relevant features of wherever youre looking. Note the %&gt;% between the leaflet() function and the addTiles() function. leaflet is one of the packages in R where we can use pipes. To add the points to the graph we use the function addMarkers(), which has two parameters, lng and lat. For both parameters we put the column in which the longitude and latitude are, respectively. leaflet() %&gt;% addTiles() %&gt;% addMarkers( lng = marijuana$long, lat = marijuana$lat ) It now adds an icon indicating where every dispensary in our data is. You can zoom in and scroll around to see more about where the dispensaries are. There are only a few dozen locations in the data so the popups overlapping a bit doesnt affect our map too much. If we had more - such as crime data with millions of offenses - it would make it very hard to read. To change the icons to circles we can change the function addMarkers() to addCircleMarkers(), keeping the rest of the code the same. leaflet() %&gt;% addTiles() %&gt;% addCircleMarkers( lng = marijuana$long, lat = marijuana$lat ) This makes the icon into circles, which take up less space than icons. To adjust the size of our icons we use the radius parameter in addMarkers() or addCircleMarkers(). The larger the radius, the larger the icons. leaflet() %&gt;% addTiles() %&gt;% addCircleMarkers( lng = marijuana$long, lat = marijuana$lat, radius = 5 ) Setting the radius option to 5 shrinks the size of the icon a lot. In your own maps youll have to fiddle with this option to get it to look the way you want. Lets move on to adding information about each icon when clicked upon. 18.3 Adding popup information The parameter popup in the addMarkers() or addCircleMarkers() functions lets you input a character value (if not already a character value it will convert it to one) and that will be shown as a popup when you click on the icon. Lets start simple here by inputting the business owner column in our data and then build it up to a more complicated popup. leaflet() %&gt;% addTiles() %&gt;% addCircleMarkers( lng = marijuana$long, lat = marijuana$lat, radius = 5, popup = marijuana$Business_Owner ) Try clicking around and youll see that the owner of the dispensary you clicked on appears over the dot. If youre reading the print version of this book you wont, of course, be able to click on the map. We usually want to have a title indicating what the value in the popup means. We can do this by using the paste() function to combine text explaining the value with the value itself. Lets add the words Business Owner: before the business owner column. leaflet() %&gt;% addTiles() %&gt;% addCircleMarkers( lng = marijuana$long, lat = marijuana$lat, radius = 5, popup = paste( &quot;Business Owner:&quot;, marijuana$Business_Owner ) ) We dont have too much information in the data, but lets add the address and license number to the popup by adding them to the paste() function were using. leaflet() %&gt;% addTiles() %&gt;% addCircleMarkers( lng = marijuana$long, lat = marijuana$lat, radius = 5, popup = paste( &quot;Business Owner:&quot;, marijuana$Business_Owner, &quot;Address:&quot;, marijuana$Premise_Address, &quot;License:&quot;, marijuana$License_Number ) ) Just adding the location text makes it try to print out everything on one line, which is hard to read. If we add the text &lt;br&gt; where we want a line break, it will make one. &lt;br&gt; is the HTML tag for line-break, which is why it works making a new line in this case. leaflet() %&gt;% addTiles() %&gt;% addCircleMarkers( lng = marijuana$long, lat = marijuana$lat, radius = 5, popup = paste( &quot;Business Owner:&quot;, marijuana$Business_Owner, &quot;&lt;br&gt;&quot;, &quot;Address:&quot;, marijuana$Premise_Address, &quot;&lt;br&gt;&quot;, &quot;License:&quot;, marijuana$License_Number ) ) 18.4 Dealing with too many markers In our case with only 33 rows of data, turning the markers to circles solves our visibility issue. In cases with many more rows of data, this doesnt always work. A solution for this is to cluster the data into groups where the dots only show if you zoom in. If we add the code clusterOptions = markerClusterOptions() to our addCircleMarkers() it will cluster for us. leaflet() %&gt;% addTiles() %&gt;% addCircleMarkers( lng = marijuana$long, lat = marijuana$lat, radius = 5, popup = paste( &quot;Business Owner:&quot;, marijuana$Business_Owner, &quot;&lt;br&gt;&quot;, &quot;Address:&quot;, marijuana$Premise_Address, &quot;&lt;br&gt;&quot;, &quot;License:&quot;, marijuana$License_Number ), clusterOptions = markerClusterOptions() ) Locations close to each other are grouped together in fairly arbitrary groupings, and we can see how large each grouping is by moving our cursor over the circle. Click on a circle or zoom in and it will show smaller groupings at lower levels of aggregation. Keep clicking or zooming in, and it will eventually show each location as its own circle. This method is very useful for dealing with huge amounts of data as it avoids overflowing the map with too many icons at one time. A downside, however, is that the clusters are created arbitrarily meaning that important context, such as neighborhood, can be lost. 18.5 Interactive choropleth maps In Chapter 17 we worked on choropleth maps which are maps with shaded regions, such as states colored by which political party won them in an election. Here we will make interactive choropleth maps where you can click on a shaded region and see information about that region. Well make the same map as before - neighborhoods shaded by the number of suicides. Lets load the San Francisco suicides-by-neighborhood data that we made earlier. Well also want to project it to the standard longitude and latitude projection, otherwise our map wont work right. library(sf) # Warning: package &#39;sf&#39; was built under R version 4.1.3 # Linking to GEOS 3.10.2, GDAL 3.4.1, PROJ 7.2.1; sf_use_s2() is TRUE load(&quot;data/sf_neighborhoods_suicide.rda&quot;) sf_neighborhoods_suicide &lt;- st_transform( sf_neighborhoods_suicide, &quot;+proj=longlat +datum=WGS84&quot; ) Well begin the leaflet map similar to before but use the function addPolygons(), and our input here is the geometry column of sf_neighborhoods_suicide. leaflet() %&gt;% addTiles() %&gt;% addPolygons(data = sf_neighborhoods_suicide$geometry) It made a map with thick blue lines indicating each neighborhood. Lets change the appearance of the graph a bit before making a popup or shading the neighborhoods The parameter color in addPolygons() changes the color of the lines - lets change it to black. The lines are also very thick, blurring into each other and making the neighborhoods hard to see. We can change the weight parameter to alter the size of these lines - smaller values are thinner lines. Lets try setting this to 1. leaflet() %&gt;% addTiles() %&gt;% addPolygons( data = sf_neighborhoods_suicide$geometry, color = &quot;black&quot;, weight = 1 ) That looks better and we can clearly distinguish each neighborhood now. As we did earlier, we can add the popup text directly to the function which makes the geographic shapes, in this case addPolygons(). Lets add the nhood column value - the name of that neighborhood - and the number of suicides that occurred in that neighborhood. As before, when we click on a neighborhood a popup appears with the output we specified. leaflet() %&gt;% addTiles() %&gt;% addPolygons( data = sf_neighborhoods_suicide$geometry, col = &quot;black&quot;, weight = 1, popup = paste0( &quot;Neighborhood: &quot;, sf_neighborhoods_suicide$nhood, &quot;&lt;br&gt;&quot;, &quot;Number of Suicides: &quot;, sf_neighborhoods_suicide$number_suicides ) ) For these types of maps we generally want to shade each polygon to indicate how frequently the event occurred in the polygon. Well use the function colorNumeric(), which takes a lot of the work out of the process of coloring in the map. This function takes two inputs, first a color palette, which we can get from the site Color Brewer. Lets use the fourth bar in the Sequential page, which is light orange to red. If you look in the section with each HEX value it says that the palette is 3-class OrRd. The 3-class just means we selected 3 colors, the OrRd is the part we want. That will tell colorNumeric() to make the palette using these colors. The second parameter is the column for our numeric variable, number_suicides. We will save the output of colorNumeric(\"OrRd\", sf_neighborhoods_suicide$number_suicides) as a new object, which well call pal for convenience since it is a palette of colors. Then inside of addPolygons() well set the parameter fillColor to pal(sf_neighborhoods_suicide$number_suicides), running this function on the column. What this really does is determine which color every neighborhood should be based on the value in the number_suicides column. pal &lt;- colorNumeric(&quot;OrRd&quot;, sf_neighborhoods_suicide$number_suicides) leaflet() %&gt;% addTiles() %&gt;% addPolygons( data = sf_neighborhoods_suicide$geometry, col = &quot;black&quot;, weight = 1, popup = paste0( &quot;Neighborhood: &quot;, sf_neighborhoods_suicide$nhood, &quot;&lt;br&gt;&quot;, &quot;Number of Suicides: &quot;, sf_neighborhoods_suicide$number_suicides ), fillColor = pal(sf_neighborhoods_suicide$number_suicides) ) Since the neighborhoods are transparent, it is hard to distinguish which color is shown. We can make each neighborhood a solid color by setting the parameter fillOpacity inside of addPolygons() to 1. leaflet() %&gt;% addTiles() %&gt;% addPolygons( data = sf_neighborhoods_suicide$geometry, col = &quot;black&quot;, weight = 1, popup = paste0( &quot;Neighborhood: &quot;, sf_neighborhoods_suicide$nhood, &quot;&lt;br&gt;&quot;, &quot;Number of Suicides: &quot;, sf_neighborhoods_suicide$number_suicides ), fillColor = pal(sf_neighborhoods_suicide$number_suicides), fillOpacity = 1 ) To add a legend to this we use the function addLegend(), which takes three parameters. pal asks which color palette we are using - we want it to be the exact same as we use to color the neighborhoods, so well use the pal object we made. The values parameter is used for which column our numeric values are from, in our case the number_suicides column so well input that. Finally opacity determines how transparent the legend will be. As each neighborhood is set to not be transparent at all, well also set this to 1 to be consistent. leaflet() %&gt;% addTiles() %&gt;% addPolygons( data = sf_neighborhoods_suicide$geometry, col = &quot;black&quot;, weight = 1, popup = paste0( &quot;Neighborhood: &quot;, sf_neighborhoods_suicide$nhood, &quot;&lt;br&gt;&quot;, &quot;Number of Suicides: &quot;, sf_neighborhoods_suicide$number_suicides ), fillColor = pal(sf_neighborhoods_suicide$number_suicides), fillOpacity = 1 ) %&gt;% addLegend( pal = pal, values = sf_neighborhoods_suicide$number_suicides, opacity = 1 ) Finally, we can add a title to the legend using the title parameter inside of addLegend(). leaflet() %&gt;% addTiles() %&gt;% addPolygons( data = sf_neighborhoods_suicide$geometry, col = &quot;black&quot;, weight = 1, popup = paste0( &quot;Neighborhood: &quot;, sf_neighborhoods_suicide$nhood, &quot;&lt;br&gt;&quot;, &quot;Number of Suicides: &quot;, sf_neighborhoods_suicide$number_suicides ), fillColor = pal(sf_neighborhoods_suicide$number_suicides), fillOpacity = 1 ) %&gt;% addLegend( pal = pal, values = sf_neighborhoods_suicide$number_suicides, opacity = 1, title = &quot;Suicides&quot; ) %&gt;% addProviderTiles(providers$CartoDB.Positron) "],["webscraping-with-rvest.html", "19 Webscraping with rvest 19.1 Scraping one page 19.2 Cleaning the webscraped data", " 19 Webscraping with rvest If I ever stop working in the field of criminology, I would certainly be a baker. So for the next few chapters we are going to work with data on baking. What well learn to do is find a recipe from the website All Recipes and webscrape the ingredients and directions of that recipe.20 For our purposes we will be using the package rvest. This package makes it relatively easy to scrape data from websites, especially when that data is already in a table on the page as our data will be. If you havent done so before, make sure to install rvest. install.packages(&quot;rvest&quot;) And every time you start R, if you want to use rvest you must tell R so by using library(rvest). library(rvest) # # Attaching package: &#39;rvest&#39; # The following object is masked from &#39;package:readr&#39;: # # guess_encoding Here is a screenshot of the recipe for the MMMMM Brownies (an excellent brownies recipe) page. 19.1 Scraping one page In later lessons well learn how to scrape the ingredients of any recipe on the site. For now, well focus on just getting data for our brownies recipe. The first step to scraping a page is to read in that pages information to R using the function read_html() from the rvest package. The input for the () is the URL of the page we want to scrape. In a later lesson, we will manipulate this URL to be able to scrape data from many pages. read_html(&quot;https://www.allrecipes.com/recipe/25080/mmmmm-brownies/&quot;) # {html_document} # &lt;html lang=&quot;en&quot;&gt; # [1] &lt;head&gt;\\n&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=UTF-8&quot;&gt;\\n&lt;meta charset=&quot;utf-8&quot;&gt;\\n&lt;meta n ... # [2] &lt;body class=&quot;template-recipe node- recipePage mdex-test karma-site-container alrcom no-js&quot; data-content-grap ... When running the above code, it returns an XML Document. The rvest package is well suited for interpreting this and turning it into something we already know how to work with. To be able to work on this data, we need to assign the output of read_html() to an object, which well call brownies since that is the recipe we are currently scraping. brownies &lt;- read_html(&quot;https://www.allrecipes.com/recipe/25080/mmmmm-brownies/&quot;) We now need to select only a small part of the page that has the relevant information - in this case, the ingredients and directions. We need to find just which parts of the page to scrape. To do so well use the helper tool SelectorGadget, a Google Chrome extension that lets you click on parts of the page to get the CSS selector code that well use. Install that extension in Chrome and go to the brownie recipe page. When you open SelectorGadget it allows you to click on parts of the page, and it will highlight every similar piece and show the CSS selector code in the box near the bottom. Here we clicked on the first ingredient - 1/2 cup white sugar. Every ingredient is highlighted in yellow as (to oversimplify this explanation) these ingredients are the same type in the page. Note that in the bottom right of the screen, the SelectorGadget bar now has the text .ingredients-item-name. This is the CSS selector code we can use to get all of the ingredients. We will use the function html_nodes() to grab the part of the page (based on the CSS selectors) that we want. The input for this function is first the object made from read_html() (which we called brownies) and then we can paste the CSS selector text - in this case, .ingredients-item-name. Well assign the resulting object to ingredients since we want to use brownies to also get the directions. ingredients &lt;- html_nodes(brownies, &quot;.ingredients-item-name&quot;) Since we are getting data that is a text format, we need to tell rvest that the format of the scraped data is text. We do with using html_text() and our input in the () is the object made in the function html_nodes(). ingredients &lt;- html_text(ingredients) Now lets check what we got. ingredients # [1] &quot;½ cup white sugar &quot; &quot;2 tablespoons butter &quot; # [3] &quot;2 tablespoons water &quot; &quot;1 ½ cups semisweet chocolate chips &quot; # [5] &quot;2 large eggs, beaten &quot; &quot;½ teaspoon vanilla extract &quot; # [7] &quot;&lt;U+2154&gt; cup all-purpose flour &quot; &quot;½ teaspoon salt &quot; # [9] &quot;¼ teaspoon baking soda &quot; We have successfully scraped the ingredients for this brownies recipes. Now lets do the same process to get the directions for baking. In SelectorGadget click clear to unselect the ingredients. Now click one of the lines of directions that starts with the word Step. Itll highlight all three directions as theyre all of the same type.21 Note that if you click on the instructions without starting on one of the Step lines, such as clicking on the actual instructions (e.g. Preheat the oven) lines itself, SelectorGadget will have the node p and say it has found 25 things on that page that match. To fix this you just scroll up to see where the text Best brownies Ive ever had! is also highlighted in yellow and click that to unselect it. Using SelectorGadget is often steps like this where you use trial and error to only select the parts of the page that you want. The CSS selector code this time is .instructions-section-item so we can put that inside of html_nodes(). Lets assign the output as directions. directions &lt;- html_nodes(brownies, &quot;.instructions-section-item&quot;) directions &lt;- html_text(directions) Did it work? directions Yes! You may notice that each direction is one very long string, so long that we have to scroll to the right (in the web version of this book) to read it. If you run the code direction in RStudio, itll automatically put it on multiple lines for easy reading. If you put it on a website or a PDF, itll instead be so long that it may extend off the page. There are many features in RStudio that make it easy to work with data like this. In cases where you are presenting the data outside of RStudio, such as making an R Markdown document, it is important to check that the results look right in every format you are making (e.g. Word, HTML, PDF). 19.2 Cleaning the webscraped data Now we just need to clean up the extra spaces to have nice, clean instructions to make the brownies from the recipe we scraped. We can remove white space at the beginning or end of strings using the trimws() function that is built into R. We just put the vector object inside the parentheses. directions &lt;- trimws(directions) ingredients &lt;- trimws(ingredients) And lets print out both objects to make sure it worked. ingredients directions Now ingredients is as it should be, though note that all of the ingredient amounts - e.g. 2/3 cups - looks fine when in R. But when exporting it to PDF or HTML it shows weird characters like &lt;U+2154&gt;. This is because the conversion from R to PDF or HTML isnt working right. Im keeping this unfixed as a demonstration of how things can look right in R but look wrong when moving it elsewhere. So when working on something that you export out of R (including from R to PDF/HTML or even R to Excel), you should make sure to check that no issue occurred during the conversion. directions has a bunch of space between the step number and the instructions. Lets use gsub() to remove the multiple spaces and replace it with a single space. Well search for anything with two or more spaces and replace that with a single space. directions &lt;- gsub(&quot; {2,}&quot;, &quot; &quot;, directions) And one final check to make sure it worked. directions In Chapter 20 well learn to make a function to scrape any recipe from this site using just the URL and to print the ingredients and directions to the console. The recipe was submitted by the user cicada77. To be slightly more specific, when the site is made it has to put all of the pieces of the site together, such as links, photos, the section on ingredients, the section on directions, the section on reviews. So in this case we selected a text type in the section on directions and SelectorGadget then selected all text types inside of that section. "],["functions.html", "20 Functions 20.1 A simple function 20.2 Adding parameters 20.3 Making a function to scrape recipes", " 20 Functions So far, we have been writing code to handle specific situations, such as subsetting a single data set, often using other peoples functions. In cases where you want to reuse the code, it is unwise to simply copy and paste the code and make minor changes to handle the new data. Instead we want something that is able to take multiple values and perform the same action (subset, aggregate, make a plot, webscrape, etc.) on those values. Weve used lots of other peoples function throughout this book, and in this chapter well learn how to create our own. Think of a function like a stapler - you put the paper in, push down, and it staples the paper together. It doesnt matter what papers you are using; it always staples them together. If you needed to buy a new stapler every time you needed to staple something (i.e. copy and pasting code) youd quickly have way too many staplers (and waste a bunch of money). An important benefit is that you can use this function again and again to help solve other problems. Lets imagine you need to clean crime data from 10 different cities. Most cities crime data is very similar so writing the code for one gets you most of the way there for the other 9 cities. Your code will probably work for the other cities, with only minor changes necessary (for example, column names are probably different across all agencies). However, copy and pasting code quickly becomes a terrible solution - functions work much better. If you did copy and paste 10 times and you found a bug, then youd have to fix the bug 10 times. With a function, you would change the code once. 20.1 A simple function Well start with a simple function that takes a number and returns that number plus the number 2. add_2 &lt;- function(number) { number &lt;- number + 2 return(number) } The syntax (how we write it) of a function is function_name &lt;- function(parameters) { code return(output) } There are five essential parts of a function function_name - This is just the name we give to the function. It can be anything, but as when making other objects, call it something that is easy to remember what it does. parameters - Here is where we say what goes into the function. In most cases you will want to put some data in and expect something new out. For example, for the function mean() you put in a vector of numbers in the () section, and it returns the mean of those numbers. Here is also where you can put any options to affect how the code is run. code - This is the code you write to do the thing you want the function to do. In the above example our code is number &lt;- number + 2. For any number inputted, our code adds 2 to it and assigns it back into the object number. return - This is something new in this book, here you use the return() function and inside the () you put the object you want to be outputted. In our example we have number inside the return() as thats what we want to come out of the function. It is not always necessary to end your function with return() but is highly recommended to do so to make sure youre outputting what it is you want to output. The final piece is the structure of your function. After the function_name (whatever it is you call it) you always need the text &lt;- function() where the parameters (if any) are in the (). After the closing parentheses put a {, and at the very end of the function, after the return(), close those squiggly brackets with a }. The &lt;- function() tells R that you are making a function rather than some other type of object. And the { and } tell R that all the code in between are part of that function. Our function here adds 2 to any number we input. add_2(2) # [1] 4 add_2(5) # [1] 7 20.2 Adding parameters Lets add a single parameter, which multiplies the result by 5 if selected. add_2 &lt;- function(number, times_5 = FALSE) { number &lt;- number + 2 return(number) } Now we have added a parameter called time_5 to the () part of the function and set it the be FALSE by default. Right now it doesnt do anything so we need to add code to say what happens if it is TRUE (remember in R true and false must always be all capital letters and not in quotes). add_2 &lt;- function(number, times_5 = FALSE) { number &lt;- number + 2 if (times_5 == TRUE) { number &lt;- number * 5 } return(number) } Now our code says if the parameter times_5 is TRUE, then do the thing in the squiggly brackets {} below. Note that we use the same squiggly brackets as when making the entire function. That just tells R that the code in those brackets belong together. Lets try out our function. add_2(2) # [1] 4 It returns 4, as expected. Since the parameter times_5 is defaulted to FALSE, we dont need to specify that parameter if we want it to stay FALSE. When we dont tell the function that we want it to be TRUE, the code in our if statement doesnt run. When we set times_5 to TRUE, it runs that code. add_2(2, times_5 = TRUE) # [1] 20 20.3 Making a function to scrape recipes In Section 19.1 we wrote some code to scrape data from the website All Recipes for a recipe. We are going to turn that code into a function here. The benefit is that our input to the function will be a URL, and then it will print out the ingredients and directions for that recipe. If we want multiple recipes (and for webscraping you usually will want to scrape multiple pages), we just change the URL we input without changing the code at all. We used the rvest package so we need to tell R we want to use it again. library(rvest) Lets start by writing a shell of the function - everything but the code. We can call it scrape_recipes (though any name would work), add in the &lt;- function() and put URL in the () as our input for the function is the URL of the page with the recipe we want. For this function we wont return any object, we will just print things to the console, so we dont need the return() value. Dont forget the { after the end of the function() and } at the very end of the function. scrape_recipes &lt;- function(URL) { } Now we need to add the code that takes the URL, scrapes the website, and assigns the ingredients part of the page to an object called ingredients and the directions part to an object called directions. Since we have the code from an earlier lesson, we can copy and paste that code into the function and make a small change to get a working function. scrape_recipes &lt;- function(URL) { brownies &lt;- read_html(&quot;https://www.allrecipes.com/recipe/25080/mmmmm-brownies/&quot;) ingredients &lt;- html_nodes(brownies, &quot;.ingredients-item-name&quot;) ingredients &lt;- html_text(ingredients) directions &lt;- html_nodes(brownies, &quot;.instructions-section-item&quot;) directions &lt;- html_text(directions) directions &lt;- trimws(directions) } The part inside the () of read_html() is the URL of the page we want to scrape. This is the part of the function that will change based on our input. We want whatever input is in the URL parameter to be the URL we scrape. So lets change the URL of the brownies recipe we scraped previously to simply say URL (without quotes). scrape_recipes &lt;- function(URL) { brownies &lt;- read_html(URL) ingredients &lt;- html_nodes(brownies, &quot;.ingredients-item-name&quot;) ingredients &lt;- html_text(ingredients) directions &lt;- html_nodes(brownies, &quot;.instructions-section-item&quot;) directions &lt;- html_text(directions) directions &lt;- trimws(directions) } To make this function print something to the console, we need to specifically tell it to do so in the code. We do this using the print() function. Lets first print the ingredients and then the directions. Well add that to the final lines of the function. scrape_recipes &lt;- function(URL) { brownies &lt;- read_html(URL) ingredients &lt;- html_nodes(brownies, &quot;.ingredients-item-name&quot;) ingredients &lt;- html_text(ingredients) directions &lt;- html_nodes(brownies, &quot;.instructions-section-item&quot;) directions &lt;- html_text(directions) directions &lt;- trimws(directions) print(ingredients) print(directions) } Now we can try it for a new recipe, this one for The Best Lemon Bars at this link. scrape_recipes(&quot;https://www.allrecipes.com/recipe/10294/the-best-lemon-bars/&quot;) In the next lesson well use for loops to scrape multiple recipes very quickly. "],["for-loops.html", "21 For loops 21.1 Basic for loops 21.2 Scraping multiple recipes", " 21 For loops We will often want to perform the same task on a number of different items, such as cleaning every column in a data set. One effective way to do this is through for loops. Earlier in this book we learned how to scrape the recipe website All Recipes. We did so for a single recipe. If we wanted to get a feasts worth of recipes, typing out each recipe would be slow, even with the function we made in Section 20.3. In this chapter we will use a for loop to scrape multiple recipes very quickly. 21.1 Basic for loops Well start with a simple example of a for loop, making R print the numbers 1-10. for (i in 1:10) { print(i) } # [1] 1 # [1] 2 # [1] 3 # [1] 4 # [1] 5 # [1] 6 # [1] 7 # [1] 8 # [1] 9 # [1] 10 The basic concept of a for loop is that you have some code that you need to run many times with slight changes to a value or values in the code - somewhat like a function. And like a function, all the code you want to use goes in between the { and } squiggly brackets. And you loop through all the values you specify - meaning that the code runs once for each of those values. Lets look closer at the (i in 1:10). The i is simply a placeholder object, which takes the value 1 through 10 each iteration of the loop. An iteration is the formal term for each time the loop runs. In our loop it will run 10 times as we have 10 numbers (1-10). The first time it runs the i gets the value of 1, the second time it runs i gets the value of 2, and so on. Its not necessary to call it i, but it is the convention in programming to do so. It takes the value of whatever follows the in, which can range from a vector of strings or numbers to lists of data.frames (though we wont do anything that complicated in this chapter). Especially when youre an early learner of R, it could help to call the i something informative to you about what value it has. Lets go through a few examples with different names for i and different values it is looping through. for (a_number in 1:10) { print(a_number) } # [1] 1 # [1] 2 # [1] 3 # [1] 4 # [1] 5 # [1] 6 # [1] 7 # [1] 8 # [1] 9 # [1] 10 animals &lt;- c(&quot;cat&quot;, &quot;dog&quot;, &quot;gorilla&quot;, &quot;buffalo&quot;, &quot;lion&quot;, &quot;snake&quot;) for (animal in animals) { print(animal) } # [1] &quot;cat&quot; # [1] &quot;dog&quot; # [1] &quot;gorilla&quot; # [1] &quot;buffalo&quot; # [1] &quot;lion&quot; # [1] &quot;snake&quot; Now lets make our code a bit more complicated, adding the number 2 every loop. for (a_number in 1:10) { print(a_number + 2) } # [1] 3 # [1] 4 # [1] 5 # [1] 6 # [1] 7 # [1] 8 # [1] 9 # [1] 10 # [1] 11 # [1] 12 Were keeping the results inside of print() since for loops do not print the results by default. Lets try combining this with some subsetting using square bracket notation []. We will look through every value in numbers, a vector we will make with the values 1:10, and replace each value with its value plus 2. The object were looping through is numbers. But were actually looping through every index it has, hence the 1:length(numbers). That is saying, i takes the value of each index in numbers, which is useful when we want to change that element. length(numbers) finds how long the vector numbers is (if this was a data.frame we could use nrow()) to find how many elements it has. In the code we take the value at each index numbers[i] and add 2 to it. numbers &lt;- 1:10 for (i in 1:length(numbers)) { numbers[i] &lt;- numbers[i] + 2 } numbers # [1] 3 4 5 6 7 8 9 10 11 12 We can also include functions we made in for loops. Heres a function we made last chapter which adds 2 to each inputted number. add_2 &lt;- function(number) { number &lt;- number + 2 return(number) } Lets put that in the loop. for (i in 1:length(numbers)) { numbers[i] &lt;- add_2(numbers[i]) } numbers # [1] 5 6 7 8 9 10 11 12 13 14 21.2 Scraping multiple recipes Below is the function copied from Section 20.3 which takes a single URL and scraped the site All Recipes for that recipe. It printed the ingredients and directions to cook that recipe to the Console. If we wanted to get that info for multiple recipes, we would need to run the function multiple times. Here we will use a for loop to do this. Since were using the read_html() function from rvest, we need to tell R we want to use that package. library(rvest) scrape_recipes &lt;- function(URL) { brownies &lt;- read_html(URL) ingredients &lt;- html_nodes(brownies, &quot;.ingredients-item-name&quot;) ingredients &lt;- html_text(ingredients) directions &lt;- html_nodes(brownies, &quot;.instructions-section-item&quot;) directions &lt;- html_text(directions) directions &lt;- trimws(directions) print(ingredients) print(directions) } With any for loop you need to figure out what is going to be changing, in this case it is the URL. And since we want multiple recipes, we will make a vector with the URLs of all the recipes we want. Here I am making a vector called recipe_urls with the URLs of a few recipes that I like on the site. The way I got the URLs was to go to each recipes page and copy and paste the URL. Is this the right approach? Shouldnt we do everything in R? Not always. In situations like this where we know that there are a small number of links we want, it is reasonable to do it by hand. Remember that R is a tool to help you. While keeping everything you do in R is good for reproducibility, it is not always reasonable and may take too much time or effort given the constraints - usually limited time - of your project. recipe_urls &lt;- c( &quot;https://www.allrecipes.com/recipe/25080/mmmmm-brownies/&quot;, &quot;https://www.allrecipes.com/recipe/27188/crepes/&quot;, &quot;https://www.allrecipes.com/recipe/22180/waffles-i/&quot; ) Now we can write the for loop to go through every single URL in recipe_urls and use the function scrape_recipes on that URL. for (recipe_url in recipe_urls) { scrape_recipes(recipe_url) } "],["scrape-table.html", "22 Scraping tables from PDFs 22.1 Scraping the first table 22.2 Making a function", " 22 Scraping tables from PDFs For this chapter youll need the following file, which is available for download here: usbp_stats_fy2017_sector_profile.pdf. Government agencies in particular like to release their data in long PDFs which often have the data we want in a table on one of the pages. To use this data we need to scrape it from the PDF into R. In the majority of cases when you want data from a PDF it will be in a table. Essentially the data will be an Excel file inside of a PDF. This format is not altogether different from what weve done before. Lets first take a look at the data we will be scraping. The first step in any PDF scraping should be to look at the PDF and try to think about the best way to approach this particular problem. While all PDF scraping follows a general format, you cannot necessarily reuse your old code as each situation is likely slightly different. Our data is from the US Customs and Border Protection (CBP) and contains a wealth of information about apprehensions and contraband seizures in border sectors. We will be using the Sector Profile 2017 PDF which has information in four tables, three of which well scrape and then combine together. The data was downloaded from the US Customs and Border Protection Stats and Summaries page here. If youre interested in using more of their data, some of it has been cleaned and made available here. The file we want to use is called usbp_stats_fy2017_sector_profile.pdf and has four tables in the PDF. Lets take a look at them one at a time, understanding what variables are available, and what units each row is in. Then well start scraping the tables. The first table is Sector Profile - Fiscal Year 2017 (Oct. 1st through Sept. 30th). Before we even look down more at the table, the title is important. It is for fiscal year 2017, not calendar year 2017, which is more common in the data we usually use. This is important if we ever want to merge this data with other data sets. If possible, we would have to get data that is monthly so we can just use October 2016 through September 2017 to match up properly. Now if we look more at the table, we can see that each row is a section of the US border. There are three main sections - Coastal, Northern, and Southwest, with subsections of each also included. The bottom row is the sum of all these sections and gives us nationwide data. Many government data sets will be like this form with sections and subsections in the same table. Watch out when doing mathematical operations! Just summing any of these columns will give you triple the true value due to the presence of nationwide, sectional, and subsectional data. There are 9 columns in the data other than the border section identifier. We have total apprehensions, apprehensions for people who are not Mexican citizens, marijuana and cocaine seizures (in pounds), the number of accepted prosecutions (presumably of those apprehended), and the number of CBP agents assaulted. The last two columns have the number of people rescued by CBP and the number of people who died (it is unclear from this data alone if this is solely people in custody or deaths during crossing the border). These two columns are also special as they only have data for the Southwest border. The second table has a similar format with each row being a section or subsection. The columns now have the number of juveniles apprehended, subdivided by if they were accompanied by an adult or not, and the number of adults apprehended. The last column is total apprehensions which is also in the first table. The third table follows the same format, and the new columns are number of apprehensions by gender. Finally, the fourth table is a bit different in its format. The rows are now variables, and the columns are the locations. In this table it doesnt include subsections, only border sections and the nationwide total. The data it has available are partially a repeat of the first table but with more drug types and the addition of the number of drug seizures and some firearm seizure information. As this table is formatted differently from the others, we wont scrape it in this lesson - but you can use the skills youll learn to do so yourself. 22.1 Scraping the first table Weve now seen all three of the tables that we want to scrape so we can begin the process of actually scraping them. Note that each table is very similar, meaning that we can reuse some code to scrape as well as to clean the data. That means that we will want to write some functions to make our work easier and avoid copy and pasting code. We will start by using the pdf_text() function from the pdftools package to read the PDFs into R. install.packages(&quot;pdftools&quot;) library(pdftools) # Warning: package &#39;pdftools&#39; was built under R version 4.1.3 # Using poppler version 22.04.0 We can assign the output of the pdf_text() function to the object border_patrol, and well use it for each table. The input to pdf_text() is the name of the PDF we want to scrape. border_patrol &lt;- pdf_text(&quot;data/usbp_stats_fy2017_sector_profile.pdf&quot;) We can take a look at the head() of the result using head(border_patrol). head(border_patrol) If you look closely in this huge amount of text output, you can see that it is a vector with each table being an element in the vector. We can see this further by checking the length() of border_patrol, which tells us how many elements are in a vector. length(border_patrol) # [1] 4 It is four elements long, one for each table. Looking at just the first element in border_patrol gives us all the values in the first table plus a few sentences at the end detailing some features of the table. At the end of each line (where in the PDF it should end but doesnt in our data yet) there is a \\n indicating that there should be a new line. We want to use strsplit() to split at the \\n. border_patrol[1] The strsplit() function breaks up a string into pieces based on a value inside of the string. Lets use the word criminology as an example. If we want to split it by the letter n wed have two results, crimi and ology as these are the pieces of the word after breaking up criminology at letter n. strsplit(&quot;criminology&quot;, split = &quot;n&quot;) # [[1]] # [1] &quot;crimi&quot; &quot;ology&quot; Note that it deletes whatever value is used to break up the string. Lets assign a new object with the value in the first element of border_patrol, calling it sector_profile as thats the name of that table, and then using strsplit() on it to split it every \\n. In effect this makes each line of the table an element in a vector that well create rather than having the entire table be a single long string as it is now. strsplit() returns a list so we will also want to keep just the first element of that list using double square bracket [[]] notation. sector_profile &lt;- border_patrol[1] sector_profile &lt;- strsplit(sector_profile, &quot;\\n&quot;) sector_profile &lt;- sector_profile[[1]] Now we can look at the first six rows of this data. head(sector_profile) Notice that there is a lot of empty white space at the beginning of the rows. We want to get rid of that to make our next steps easier. We can use trimws() and put the entire sector_profile data in the (), and itll remove any white space that is at the beginning or end of the string. sector_profile &lt;- trimws(sector_profile) We have more rows than we want so lets look at the entire data and try to figure out how to keep just the necessary rows. sector_profile Based on the PDF, we want every row from Miami to Nationwide Total. But here we have several rows with the title of the table and the column names, and at the end we have the sentences with some details that we dont need. To keep only the rows that we want, we can combine grep() and subsetting to find the rows from Miami to Nationwide Total and keep only those rows. We will use grep() to find which row has the text Miami and which has the text Nationwide Total and keep all rows between them (including those matched rows as well). Since each only appears once in the table we dont need to worry about handling duplicate results. grep(&quot;Miami&quot;, sector_profile) # [1] 10 grep(&quot;Nationwide Total&quot;, sector_profile) # [1] 35 Well use square bracket notation to keep all rows between those two values (including each value). Since the data is a vector, not a data.frame, we dont need a comma. sector_profile &lt;- sector_profile[grep(&quot;Miami&quot;, sector_profile): grep(&quot;Nationwide Total&quot;, sector_profile)] Note that were getting rid of the rows that had the column names. Its easier to make the names ourselves than to deal with that mess. The data now has only the rows we want but still doesnt have any columns, its currently just a vector of strings. We want to make it into a data.frame to be able to work on it like we usually do. head(sector_profile) When looking at this data it is clear that where the division between columns is supposed to be is a bunch of white space in each string. Take the first row for example, it says Miami then after lots of white spaces 111 than again with 2,280 and so on for the rest of the row. Well use this pattern of columns differentiated by white space to make sector_profile into a data.frame. We will use the function str_split_fixed() from the stringr package. This function is very similar to strsplit() except you can tell it how many columns to expect. install.packages(&quot;stringr&quot;) library(stringr) The syntax of str_split_fixed() is similar to strsplit() except the new parameter of the number of splits to expect. The _fixed part of str_split_fixed() is that it expects the same number of splits (which in our case become columns) for every element in the vector that we input. Looking at the PDF shows us that there are 10 columns so thats the number well use. Our split will be  {2,}. That is, a space that occurs two or more times. Since there are sectors with spaces in their name, we cant have only one space, we need at least two. If you look carefully at the rows with sectorsCoastal Border Sectors Total and Northern Border Sectors Total, the final two columns actually do not have two spaces between them because of the amount of asterisks they have. Normally wed want to fix this using gsub(), but those values will turn to NA anyway so we wont bother in this case. sector_profile &lt;- str_split_fixed(sector_profile, &quot; {2,}&quot;, 10) If we check the head() we can see that we have the proper columns now, but this still isnt a data.frame and has no column names. head(sector_profile) # [,1] [,2] [,3] [,4] # [1,] &quot;Miami&quot; &quot;111&quot; &quot;2,280&quot; &quot;1,646&quot; # [2,] &quot;New Orleans&quot; &quot;63&quot; &quot;920&quot; &quot;528&quot; # [3,] &quot;Ramey&quot; &quot;38&quot; &quot;388&quot; &quot;387&quot; # [4,] &quot;Coastal Border Sectors Total&quot; &quot;212&quot; &quot;3,588&quot; &quot;2,561&quot; # [5,] &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; # [6,] &quot;Blaine&quot; &quot;296&quot; &quot;288&quot; &quot;237&quot; # [,5] [,6] [,7] [,8] [,9] [,10] # [1,] &quot;2,253&quot; &quot;231&quot; &quot;292&quot; &quot;1&quot; &quot;N/A&quot; &quot;N/A&quot; # [2,] &quot;21&quot; &quot;6&quot; &quot;10&quot; &quot;0&quot; &quot;N/A&quot; &quot;N/A&quot; # [3,] &quot;3&quot; &quot;2,932&quot; &quot;89&quot; &quot;0&quot; &quot;N/A&quot; &quot;N/A&quot; # [4,] &quot;2,277&quot; &quot;3,169&quot; &quot;391&quot; &quot;1&quot; &quot;N/A ****&quot; &quot;N/A ****&quot; # [5,] &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; # [6,] &quot;0&quot; &quot;0&quot; &quot;9&quot; &quot;0&quot; &quot;N/A&quot; &quot;N/A&quot; We can make it a data.frame just by putting it in data.frame(). And we can assign the columns names using a vector of strings we can make. Well use the same column names as in the PDF but in lowercase and replacing spaces and parentheses with underscores. sector_profile &lt;- data.frame(sector_profile) names(sector_profile) &lt;- c( &quot;sector&quot;, &quot;agent_staffing&quot;, &quot;apprehensions&quot;, &quot;other_than_mexican_apprehensions&quot;, &quot;marijuana_pounds&quot;, &quot;cocaine_pounds&quot;, &quot;accepted_prosecutions&quot;, &quot;assaults&quot;, &quot;rescues&quot;, &quot;deaths&quot; ) We have now taken a table from a PDF and successfully scraped it to a data.frame in R. Now we can work on it as we would any other data set that weve used previously. head(sector_profile) # sector agent_staffing apprehensions # 1 Miami 111 2,280 # 2 New Orleans 63 920 # 3 Ramey 38 388 # 4 Coastal Border Sectors Total 212 3,588 # 5 # 6 Blaine 296 288 # other_than_mexican_apprehensions marijuana_pounds # 1 1,646 2,253 # 2 528 21 # 3 387 3 # 4 2,561 2,277 # 5 # 6 237 0 # cocaine_pounds accepted_prosecutions assaults rescues # 1 231 292 1 N/A # 2 6 10 0 N/A # 3 2,932 89 0 N/A # 4 3,169 391 1 N/A **** # 5 # 6 0 9 0 N/A # deaths # 1 N/A # 2 N/A # 3 N/A # 4 N/A **** # 5 # 6 N/A To really be able to use this data well want to clean the columns to turn the values to numeric type, but we can leave that until later. For now lets write a function that replicates much of this work for the next tables. 22.2 Making a function As weve done before, we want to take the code we wrote for the specific case of the first table in this PDF and turn it into a function for the general case of other tables in the PDF. Lets copy the code we used previously before we convert it to a function. sector_profile &lt;- border_patrol[1] sector_profile &lt;- trimws(sector_profile) sector_profile &lt;- strsplit(sector_profile, &quot;\\r\\n&quot;) sector_profile &lt;- sector_profile[[1]] sector_profile &lt;- sector_profile[grep( &quot;Miami&quot;, sector_profile ): grep( &quot;Nationwide Total&quot;, sector_profile )] sector_profile &lt;- str_split_fixed(sector_profile, &quot; {2,}&quot;, 10) sector_profile &lt;- data.frame(sector_profile) names(sector_profile) &lt;- c( &quot;sector&quot;, &quot;agent_staffing&quot;, &quot;total_apprehensions&quot;, &quot;other_than_mexican_apprehensions&quot;, &quot;marijuana_pounds&quot;, &quot;cocaine_pounds&quot;, &quot;accepted_prosecutions&quot;, &quot;assaults&quot;, &quot;rescues&quot;, &quot;deaths&quot; ) Since each table is so similar our function will only need a few changes in the above code to work for all three tables. The object border_patrol has all four of the tables in the data, so we need to say which of these tables we want - we can call the parameter table_number. Then each table has a different number of columns so we need to change the str_split_fixed() function to take a variable with the number of columns we input, a value well call number_columns. We rename each column to its proper name so we need to input a vector - which well call column_names - with the names for each column. Finally, we want to have a parameter where we enter in the data, which holds all of the tables, our object border_patrol, we can call this list_of_tables as it is fairly descriptive. We do this as it is bad form (and potentially dangerous) to have a function that relies on an object that isnt explicitly put in the function. It we change our border_patrol object (such as by scraping a different file but calling that object border_patrol) and the function doesnt have that as an input, it will work differently than we expect. Since we called the object we scraped sector_profile for the first table, lets change that to data as not all tables are called Sector Profile. scrape_pdf &lt;- function(list_of_tables, table_number, number_columns, column_names) { data &lt;- list_of_tables[table_number] data &lt;- trimws(data) data &lt;- strsplit(data, &quot;\\n&quot;) data &lt;- data[[1]] data &lt;- data[grep(&quot;Miami&quot;, data): grep(&quot;Nationwide Total&quot;, data)] data &lt;- str_split_fixed(data, &quot; {2,}&quot;, number_columns) data &lt;- data.frame(data) names(data) &lt;- column_names return(data) } Now lets run this function for each of the three tables we want to scrape, changing the functions parameters to work for each table. To see what parameter values you need to input, look at the PDF itself or the screenshots in this lesson. table_1 &lt;- scrape_pdf( list_of_tables = border_patrol, table_number = 1, number_columns = 10, column_names = c( &quot;sector&quot;, &quot;agent_staffing&quot;, &quot;total_apprehensions&quot;, &quot;other_than_mexican_apprehensions&quot;, &quot;marijuana_pounds&quot;, &quot;cocaine_pounds&quot;, &quot;accepted_prosecutions&quot;, &quot;assaults&quot;, &quot;rescues&quot;, &quot;deaths&quot; ) ) table_2 &lt;- scrape_pdf( list_of_tables = border_patrol, table_number = 2, number_columns = 6, column_names = c( &quot;sector&quot;, &quot;accompanied_juveniles&quot;, &quot;unaccompanied_juveniles&quot;, &quot;total_juveniles&quot;, &quot;total_adults&quot;, &quot;total_apprehensions&quot; ) ) table_3 &lt;- scrape_pdf( list_of_tables = border_patrol, table_number = 3, number_columns = 4, column_names = c( &quot;sector&quot;, &quot;female&quot;, &quot;male&quot;, &quot;total_apprehensions&quot; ) ) We can use the function left_join() from the dplyr package to combine the three tables into a single object. In the first table there are some asterisks after the final two row names in the Sector column. For our match to work properly we need to delete them, which we can do using gsub(). table_1$sector &lt;- gsub(&quot;\\\\*&quot;, &quot;&quot;, table_1$sector) Now we can run left_join(). left_join() will automatically join based on shared column names in the two data sets we are joining. In our case this is sector and total_apprehensions. All we need to input into left_join() is the name of the data sets we want to join together. left_join() can only combine two data sets at a time so well first join table_1 and table_2 and then join table_3 with the result of the first join, which well call final_data. library(dplyr) # Warning: package &#39;dplyr&#39; was built under R version 4.1.3 # # Attaching package: &#39;dplyr&#39; # The following objects are masked from &#39;package:stats&#39;: # # filter, lag # The following objects are masked from &#39;package:base&#39;: # # intersect, setdiff, setequal, union final_data &lt;- left_join(table_1, table_2) # Joining, by = c(&quot;sector&quot;, &quot;total_apprehensions&quot;) final_data &lt;- left_join(final_data, table_3) # Joining, by = c(&quot;sector&quot;, &quot;total_apprehensions&quot;) Lets take a look at the head() of this combined data. head(final_data) # sector agent_staffing # 1 Miami 111 # 2 New Orleans 63 # 3 Ramey 38 # 4 Coastal Border Sectors Total 212 # 5 # 6 Blaine 296 # total_apprehensions other_than_mexican_apprehensions # 1 2,280 1,646 # 2 920 528 # 3 388 387 # 4 3,588 2,561 # 5 # 6 288 237 # marijuana_pounds cocaine_pounds accepted_prosecutions # 1 2,253 231 292 # 2 21 6 10 # 3 3 2,932 89 # 4 2,277 3,169 391 # 5 # 6 0 0 9 # assaults rescues deaths accompanied_juveniles # 1 1 N/A N/A 19 # 2 0 N/A N/A 1 # 3 0 N/A N/A 7 # 4 1 N/A **** N/A **** 27 # 5 &lt;NA&gt; # 6 0 N/A N/A 29 # unaccompanied_juveniles total_juveniles total_adults # 1 42 61 2,219 # 2 22 23 897 # 3 1 8 380 # 4 65 92 3,496 # 5 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; # 6 7 36 252 # female male # 1 219 2,061 # 2 92 828 # 3 65 323 # 4 376 3,212 # 5 &lt;NA&gt; &lt;NA&gt; # 6 97 191 In one data set we now have information from three separate tables in a PDF. We have now scraped three different tables from a PDF and turned them into a single data set, turning the PDF into actually usable (and useful) data! "],["scrape-table2.html", "23 More scraping tables from PDFs 23.1 Texas jail data 23.2 Pregnant women incarcerated 23.3 Making PDF-scraped data available to others", " 23 More scraping tables from PDFs For this chapter youll need the following files, which are available for download here: AbbreRptCurrent.pdf and PregnantFemaleReportingCurrent.pdf. In Chapter 22 we used the package pdftools to scrape tables on arrests/seizures from the US Customs and Border Protection that were only available in a PDF. Given the importance of PDF scraping, in this chapter well continue working on scraping tables from PDFs. Here, we will use the package tabulizer, which has a number of features making it especially useful for grabbing tables from PDFs. One issue, which we saw in Chapter 22, is that the table may not be the only thing on the page - the page could also have a title, page number, etc. When using pdftools we use regular expressions and subsetting to remove all the extra lines. Using tabulizer we can simply say (through a handy function) that we only want a part of the page, so we only scrape the table itself. For more info about the tabulizer package please see its site here. 23.1 Texas jail data For this chapter well scrape data from the Texas Commission on Jail Standards - Abbreviated Population Report. This is a report that shows monthly data on people incarcerated in jails for counties in Texas. This PDF is 9 pages long because of how many counties there are in Texas. Lets take a look at what the first page looks like. If you look at the PDF yourself youll see that every page follows the format of the 1st page, which greatly simplifies our scrape. The data is in county-month units, which means that each row of data has info for a single county in a single month. We know that because the first column is County, and each row is a single county (this is not true in every case. For example, on page 3 there are the rows Fannin 1(P) and Fannin 2(P), possibly indicating that there are two jails in that county. It is unclear from this PDF what the (P) means.). For knowing that the data is monthly, the title of this document says for 06/01/2020 indicating that it is for that date, though this doesnt by itself mean the data is monthly - it could be daily based only on this data. To know for sure that it is monthly data wed have to go to the original source on the Texas Commission on Jail Standards website here. On this page it says that Monthly population reports are available for review below, which tells us that the data is monthly. Its important to know the unit so you can understand the data properly - primarily so you know what kinds of questions you can answer. If someone asks whether yearly trends on jail incarceration change in Texas, you can answer that with this data. If they ask whether more people are in jail on a Tuesday than on a Friday, you cant. Just to understand what units our data is in we had to look at both the PDF itself and the site it came from. This kind of multi-step process is tedious but often necessary to truly understand your data. And even now we have questions - what does the (P) thats in some rows mean? For this wed have to email or call the people who handle the data and ask directly. This is often the easiest way to answer your question, though different organizations have varying speeds in responding - if ever. Now lets look at what columns are available. It looks like each column is the number of people incarcerated in the jail, broken down into categories of people. For example, the first two columns after County are Pretrial Felons and Conv. Felons so those are probably how many people are incarcerated who are awaiting trial for a felony and those already convicted of a felony. The other columns seem to follow this same format until the last few ones, which describe the jails capacity (i.e. how many people they can hold), what percent of capacity they are at, and specifically how many open beds they have. Now that weve familiarized ourselves with the data, lets begin scraping this data using tabulizer. If you dont have this package installed, youll need to install it using install.packages(\"tabulizer\"). Then well need to run library(tabulizer). install.packages(&quot;tabulizer&quot;) library(tabulizer) The main function that well be using from the tabulizer package is extract_tables(). In the parentheses we need to put the name of our PDF (in quotes). This function basically looks at a PDF page, figures out which part of the page is a table, and then scrapes just that table. As well see, its not always perfect at figuring out what part of the page is a table so we can also tell it exactly where to look. You can look at all of the features of extract_tables() by running help(extract_tables). data &lt;- extract_tables(file = &quot;data/AbbreRptCurrent.pdf&quot;) Normally wed now look at the head() of our data object, but if we did that it would print out a very large amount of information. Instead, well check how long our object is using length() which tells us how many elements a vector or list has. Well also check what type of data it is since different types of data (e.g. vector, data.frame) operate differently. length(data) # [1] 18 is(data) # [1] &quot;list&quot; &quot;vector&quot; We learn that it is a list of length of 18, or has 18 elements in it. Why is this? We have 9 pages so it is reasonable that we would have 9 lists since we have one table per page, but we shouldnt have 18 tables. Back in Section 3.3.3 I said that lists are one of the data types that we dont have to worry about as we dont use them much in this book. Thats still true. Our data object is a list, and we want to convert this to a data.frame as quickly as possible. The important thing to know when interacting with a list is that subsetting here uses two pairs of square brackets [[]] instead of one pair of square brackets for a normal vector. Lets look again at just the first table in our object, subsetting using [[1]]. data[[1]] # [,1] [,2] [,3] [,4] # [1,] &quot;&quot; &quot;&quot; &quot;&quot; &quot;Conv. Felons&quot; # [2,] &quot;&quot; &quot;&quot; &quot;&quot; &quot;Sentenced to&quot; # [3,] &quot;&quot; &quot;&quot; &quot;&quot; &quot;County Jail&quot; # [4,] &quot;&quot; &quot;Pretrial&quot; &quot;Conv.&quot; &quot;&quot; # [5,] &quot;&quot; &quot;&quot; &quot;&quot; &quot;time&quot; # [6,] &quot;County&quot; &quot;Felons&quot; &quot;Felons&quot; &quot;&quot; # [,5] [,6] [,7] [,8] [,9] # [1,] &quot;&quot; &quot;Parole&quot; &quot;&quot; &quot;&quot; &quot;&quot; # [2,] &quot;&quot; &quot;Violators&quot; &quot;&quot; &quot;&quot; &quot;&quot; # [3,] &quot;&quot; &quot;with a New&quot; &quot;&quot; &quot;&quot; &quot;&quot; # [4,] &quot;Parole&quot; &quot;&quot; &quot;Pretrial&quot; &quot;Conv.&quot; &quot;Bench&quot; # [5,] &quot;&quot; &quot;Charge&quot; &quot;&quot; &quot;&quot; &quot;&quot; # [6,] &quot;Violators&quot; &quot;&quot; &quot;Misd.&quot; &quot;Misd.&quot; &quot;Warrants&quot; # [,10] [,11] [,12] [,13] # [1,] &quot;&quot; &quot;&quot; &quot;Conv. SJF&quot; &quot;Conv.&quot; # [2,] &quot;&quot; &quot;&quot; &quot;Sentenced&quot; &quot;SJF&quot; # [3,] &quot;&quot; &quot;&quot; &quot;to Co. Jail&quot; &quot;Sentenced&quot; # [4,] &quot;&quot; &quot;Pretrial&quot; &quot;&quot; &quot;&quot; # [5,] &quot;&quot; &quot;&quot; &quot;Time&quot; &quot;to State Jail&quot; # [6,] &quot;Federal&quot; &quot;SJF&quot; &quot;&quot; &quot;&quot; # [,14] [,15] [,16] [,17] [,18] # [1,] &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; # [2,] &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; # [3,] &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; # [4,] &quot;Total&quot; &quot;Total&quot; &quot;Total&quot; &quot;Total&quot; &quot;Total&quot; # [5,] &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; # [6,] &quot;Others&quot; &quot;Local&quot; &quot;Contract&quot; &quot;Population&quot; &quot;Capacity&quot; # [,19] [,20] # [1,] &quot;&quot; &quot;&quot; # [2,] &quot;&quot; &quot;&quot; # [3,] &quot;&quot; &quot;&quot; # [4,] &quot;% of&quot; &quot;Available&quot; # [5,] &quot;&quot; &quot;&quot; # [6,] &quot;Capacity&quot; &quot;Beds&quot; The results from data[[1]] provide some answers. It has the right number of columns but only 6 rows! This is our first table so it should be the entire table we can see on page 1. Instead, it appears to be just the column names, with 6 rows because some column names are on multiple rows. Heres the issue, we can read the table and easily see that the column names may be on multiple rows but belong together, and that they are part of the table. tabulizer cant see this obvious fact. It must rely on a series of rules to indicate what is part of a table and what isnt. For example, having white space between columns and thin black lines around rows tells it where each row and column is. Our issue is that the column names appear to just be text until there is a thick black line and (in tabulizer's mind) the table begins, so it keeps the column name part separate from the rest of the table. Now lets look closer at the second element in our data object and see if it is correct for the table on page 1 of our PDF. head(data[[2]]) # [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] # [1,] &quot;Anderson&quot; &quot;81&quot; &quot;13&quot; &quot;3&quot; &quot;1&quot; &quot;5&quot; &quot;12&quot; &quot;1&quot; &quot;0&quot; # [2,] &quot;Andrews&quot; &quot;23&quot; &quot;11&quot; &quot;0&quot; &quot;2&quot; &quot;4&quot; &quot;11&quot; &quot;0&quot; &quot;0&quot; # [3,] &quot;Angelina&quot; &quot;79&quot; &quot;35&quot; &quot;4&quot; &quot;6&quot; &quot;0&quot; &quot;14&quot; &quot;0&quot; &quot;3&quot; # [4,] &quot;Aransas&quot; &quot;23&quot; &quot;10&quot; &quot;0&quot; &quot;2&quot; &quot;6&quot; &quot;7&quot; &quot;0&quot; &quot;6&quot; # [5,] &quot;Archer&quot; &quot;12&quot; &quot;3&quot; &quot;0&quot; &quot;0&quot; &quot;1&quot; &quot;3&quot; &quot;1&quot; &quot;1&quot; # [6,] &quot;Armstrong&quot; &quot;1&quot; &quot;1&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; # [,10] [,11] [,12] [,13] [,14] [,15] [,16] [,17] [,18] # [1,] &quot;0&quot; &quot;21&quot; &quot;0&quot; &quot;1&quot; &quot;0&quot; &quot;138&quot; &quot;0&quot; &quot;138&quot; &quot;300&quot; # [2,] &quot;0&quot; &quot;5&quot; &quot;0&quot; &quot;6&quot; &quot;0&quot; &quot;35&quot; &quot;0&quot; &quot;35&quot; &quot;50&quot; # [3,] &quot;0&quot; &quot;23&quot; &quot;0&quot; &quot;3&quot; &quot;1&quot; &quot;168&quot; &quot;0&quot; &quot;168&quot; &quot;279&quot; # [4,] &quot;73&quot; &quot;2&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;56&quot; &quot;73&quot; &quot;129&quot; &quot;212&quot; # [5,] &quot;2&quot; &quot;5&quot; &quot;0&quot; &quot;0&quot; &quot;1&quot; &quot;26&quot; &quot;9&quot; &quot;35&quot; &quot;48&quot; # [6,] &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;2&quot; &quot;0&quot; &quot;2&quot; &quot;8&quot; # [,19] [,20] # [1,] &quot;46.00&quot; &quot;132&quot; # [2,] &quot;70.00&quot; &quot;10&quot; # [3,] &quot;60.22&quot; &quot;83&quot; # [4,] &quot;60.85&quot; &quot;62&quot; # [5,] &quot;72.92&quot; &quot;0&quot; # [6,] &quot;25.00&quot; &quot;0&quot; tail(data[[2]]) # [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] # [24,] &quot;Brooks&quot; &quot;15&quot; &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; # [25,] &quot;Brooks (P)&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; # [26,] &quot;Brown&quot; &quot;70&quot; &quot;20&quot; &quot;0&quot; &quot;7&quot; &quot;20&quot; &quot;9&quot; &quot;0&quot; &quot;2&quot; # [27,] &quot;Burleson&quot; &quot;19&quot; &quot;2&quot; &quot;0&quot; &quot;2&quot; &quot;0&quot; &quot;3&quot; &quot;0&quot; &quot;0&quot; # [28,] &quot;Burnet&quot; &quot;57&quot; &quot;23&quot; &quot;1&quot; &quot;5&quot; &quot;9&quot; &quot;3&quot; &quot;0&quot; &quot;1&quot; # [29,] &quot;Caldwell&quot; &quot;89&quot; &quot;4&quot; &quot;0&quot; &quot;3&quot; &quot;2&quot; &quot;26&quot; &quot;1&quot; &quot;2&quot; # [,10] [,11] [,12] [,13] [,14] [,15] [,16] [,17] [,18] # [24,] &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;18&quot; &quot;0&quot; &quot;18&quot; &quot;36&quot; # [25,] &quot;164&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;408&quot; &quot;408&quot; &quot;652&quot; # [26,] &quot;0&quot; &quot;0&quot; &quot;3&quot; &quot;0&quot; &quot;2&quot; &quot;133&quot; &quot;7&quot; &quot;140&quot; &quot;196&quot; # [27,] &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;1&quot; &quot;27&quot; &quot;0&quot; &quot;27&quot; &quot;96&quot; # [28,] &quot;0&quot; &quot;10&quot; &quot;1&quot; &quot;0&quot; &quot;0&quot; &quot;110&quot; &quot;158&quot; &quot;268&quot; &quot;595&quot; # [29,] &quot;19&quot; &quot;13&quot; &quot;0&quot; &quot;3&quot; &quot;0&quot; &quot;143&quot; &quot;21&quot; &quot;164&quot; &quot;301&quot; # [,19] [,20] # [24,] &quot;50.00&quot; &quot;14&quot; # [25,] &quot;62.58&quot; &quot;179&quot; # [26,] &quot;71.43&quot; &quot;36&quot; # [27,] &quot;28.13&quot; &quot;59&quot; # [28,] &quot;45.04&quot; &quot;268&quot; # [29,] &quot;54.49&quot; &quot;107&quot; Were looking just at the head() and tail() to get the first and last six rows as otherwise wed print out all 29 rows in that table. When you are exploring your own data, youll probably want to be more thorough and ensure that rows around the middle are also correct - but this is a good first pass. If you look at the output we just printed out and compare it to the PDF, youll see that the scrape was successful. Every row is where it should be and the columns are correct - unlike when using pdftools(), we have the results already in proper columns. One thing to note is that this data isnt in a data.frame format, its in a matrix. Matrices are the default output of extract_tables() though you can set it to output a data.frame by setting the parameter output = \"data.frame\". In our case we actually wouldnt want that due to the issue of the column names. As shown below, outputting to a data.frame will automatically take the first row of data and convert that to column names. So now we have our first county as the column names, which is not correct. Note too that the function added X before the column names that are numbers. Thats because column names cannot start with a number so the function tries to fix it by adding the X to the start. data &lt;- extract_tables( file = &quot;data/AbbreRptCurrent.pdf&quot;, output = &quot;data.frame&quot; ) head(data[[2]]) # Anderson X81 X13 X3 X1 X5 X12 X1.1 X0 X0.1 X21 X0.2 X1.2 # 1 Andrews 23 11 0 2 4 11 0 0 0 5 0 6 # 2 Angelina 79 35 4 6 0 14 0 3 0 23 0 3 # 3 Aransas 23 10 0 2 6 7 0 6 73 2 0 0 # 4 Archer 12 3 0 0 1 3 1 1 2 5 0 0 # 5 Armstrong 1 1 0 0 0 0 0 0 0 0 0 0 # 6 Atascosa 54 2 0 8 4 21 0 5 0 29 0 4 # X0.3 X138 X0.4 X138.1 X300 X46.00 X132 # 1 0 35 0 35 50 70.00 10 # 2 1 168 0 168 279 60.22 83 # 3 0 56 73 129 212 60.85 62 # 4 1 26 9 35 48 72.92 0 # 5 0 2 0 2 8 25.00 0 # 6 0 127 29 156 250 62.40 69 Lets rerun the extract_tables() function, this time keeping it as outputting a matrix. data &lt;- extract_tables(file = &quot;data/AbbreRptCurrent.pdf&quot;) Since the column names are the same on each page, we can set the names manually. There are 20 columns so this will be a lot of writing, but its simpler and quicker than trying to do it programmatically. Since each table will have the same column names, well want to create a vector with the column names to use for every table. Following normal naming conventions, well make everything lowercase, and the only punctuation well use is an underscore. column_names &lt;- c( &quot;county&quot;, &quot;pretrial_felons&quot;, &quot;conv_felons&quot;, &quot;conv_felons_sentence_to_county_jail_time&quot;, &quot;parole_violators&quot;, &quot;parole_violators_with_a_new_charge&quot;, &quot;pretrial_misd&quot;, &quot;conv_misd&quot;, &quot;bench_warrants&quot;, &quot;federal&quot;, &quot;pretrial_sjf&quot;, &quot;conv_sjf_sentenced_to_co_jail_time&quot;, &quot;conv_sjf_sentence_to_state_jail&quot;, &quot;total_others&quot;, &quot;total_local&quot;, &quot;total_contract&quot;, &quot;total_population&quot;, &quot;total_capacity&quot;, &quot;percent_of_capacity&quot;, &quot;available_beds&quot; ) We can combine the results from this vector with that of the second table to have a complete table from page 1 of our PDF. We do this first by making the second element from our data object into a data.frame. Then we use names() and assign the column names to that of the vector of names we just made. Since this is the table from page 1 of the PDF, well call the object page1_table. Well look just at the head() of our page1_table object. page1_table &lt;- data[[2]] page1_table &lt;- data.frame(page1_table) names(page1_table) &lt;- column_names head(page1_table) # county pretrial_felons conv_felons # 1 Anderson 81 13 # 2 Andrews 23 11 # 3 Angelina 79 35 # 4 Aransas 23 10 # 5 Archer 12 3 # 6 Armstrong 1 1 # conv_felons_sentence_to_county_jail_time parole_violators # 1 3 1 # 2 0 2 # 3 4 6 # 4 0 2 # 5 0 0 # 6 0 0 # parole_violators_with_a_new_charge pretrial_misd # 1 5 12 # 2 4 11 # 3 0 14 # 4 6 7 # 5 1 3 # 6 0 0 # conv_misd bench_warrants federal pretrial_sjf # 1 1 0 0 21 # 2 0 0 0 5 # 3 0 3 0 23 # 4 0 6 73 2 # 5 1 1 2 5 # 6 0 0 0 0 # conv_sjf_sentenced_to_co_jail_time # 1 0 # 2 0 # 3 0 # 4 0 # 5 0 # 6 0 # conv_sjf_sentence_to_state_jail total_others total_local # 1 1 0 138 # 2 6 0 35 # 3 3 1 168 # 4 0 0 56 # 5 0 1 26 # 6 0 0 2 # total_contract total_population total_capacity # 1 0 138 300 # 2 0 35 50 # 3 0 168 279 # 4 73 129 212 # 5 9 35 48 # 6 0 2 8 # percent_of_capacity available_beds # 1 46.00 132 # 2 70.00 10 # 3 60.22 83 # 4 60.85 62 # 5 72.92 0 # 6 25.00 0 Looking at the results, weve done this correctly. The values are right and the column names are correct. Weve done it for one page but now must add the remaining pages. Well do this through a for loop. We want to take the code we used above and loop through each of the tables we have. Since half of our tables are just the column names and not actual data, we need to skip those elements in our for loop. Luckily, our data follows a pattern where the first element is the column names from page 1, the second is the data from page 1, the third is the column names from page 2, the fourth is the data from page 2, and so on. So we need only every other value from 1 to 18, or every even number. We can get every other value using logical values, as shown in the next section, but since we only have 18 elements well just create the simple vector ourselves: c(2, 4, 6, 8, 10, 12, 14, 16, 18). For our for loop we can copy the code above but lets change the object name from page1_table to temp as each iteration will be of a different page so page1_table doesnt make sense. for (i in c(2, 4, 6, 8, 10, 12, 14, 16, 18)) { temp &lt;- data[[i]] temp &lt;- data.frame(temp) names(temp) &lt;- column_names } Running the above code runs our for loop successfully but doesnt assign the output anywhere. It just runs one iteration, assigns it to temp, and then overwrites temp for the next iteration. What we really want is a single object, which will end up having every single row of data from every page in one data.frame. To do this we make an empty data.frame by saying some object gets data.frame() without anything in the parentheses. And then for every iteration of the loop we add the data that is in temp to this empty data.frame (which will soon fill up with data). By creating an empty data.frame at the start we avoid having to name any of the column names or say how many rows of data there will be. To add data to this data.frame each iteration we will use the function bind_rows() from dplyr which stacks data sets on top of each other. Lets first look at a simple example of this before including it in our for loop. To use bind_rows() we put two (or more) data.frames as the parameters, and it will return a single data set with all rows stacked together. Lets create two data.frames that each have the rows of head(mtcars) as a demonstration. library(dplyr) example1 &lt;- head(mtcars) example2 &lt;- head(mtcars) bind_rows(example1, example2) # mpg cyl disp hp drat wt qsec # Mazda RX4...1 21.0 6 160 110 3.90 2.620 16.46 # Mazda RX4 Wag...2 21.0 6 160 110 3.90 2.875 17.02 # Datsun 710...3 22.8 4 108 93 3.85 2.320 18.61 # Hornet 4 Drive...4 21.4 6 258 110 3.08 3.215 19.44 # Hornet Sportabout...5 18.7 8 360 175 3.15 3.440 17.02 # Valiant...6 18.1 6 225 105 2.76 3.460 20.22 # Mazda RX4...7 21.0 6 160 110 3.90 2.620 16.46 # Mazda RX4 Wag...8 21.0 6 160 110 3.90 2.875 17.02 # Datsun 710...9 22.8 4 108 93 3.85 2.320 18.61 # Hornet 4 Drive...10 21.4 6 258 110 3.08 3.215 19.44 # Hornet Sportabout...11 18.7 8 360 175 3.15 3.440 17.02 # Valiant...12 18.1 6 225 105 2.76 3.460 20.22 # vs am gear carb # Mazda RX4...1 0 1 4 4 # Mazda RX4 Wag...2 0 1 4 4 # Datsun 710...3 1 1 4 1 # Hornet 4 Drive...4 1 0 3 1 # Hornet Sportabout...5 0 0 3 2 # Valiant...6 1 0 3 1 # Mazda RX4...7 0 1 4 4 # Mazda RX4 Wag...8 0 1 4 4 # Datsun 710...9 1 1 4 1 # Hornet 4 Drive...10 1 0 3 1 # Hornet Sportabout...11 0 0 3 2 # Valiant...12 1 0 3 1 The data that is printed out has 12 rows, and in this example the first six and the last six rows are identical. bind_rows() took the second object in the parentheses (example2) and stacked it right below the last row in example1. In this case the columns are already in the same order, but if they werent, bind_rows() is smart enough to arrange the columns in the second object to be the same as the first object. Now we can run our for loop and create a single data set with every row from our 9 pages of data. We start by creating our empty data.frame, and well call that final. At the end of our loop we say that final gets bind_rows(final, temp) meaning that temp is stacked to the bottom of final every time the loop runs. Well end this code chunk by looking at head() and tail() of final to be sure it worked correctly. final &lt;- data.frame() for (i in c(2, 4, 6, 8, 10, 12, 14, 16, 18)) { temp &lt;- data[[i]] temp &lt;- data.frame(temp) names(temp) &lt;- column_names final &lt;- bind_rows(final, temp) } # New names: # * `` -&gt; `...21` head(final) # county pretrial_felons conv_felons # 1 Anderson 81 13 # 2 Andrews 23 11 # 3 Angelina 79 35 # 4 Aransas 23 10 # 5 Archer 12 3 # 6 Armstrong 1 1 # conv_felons_sentence_to_county_jail_time parole_violators # 1 3 1 # 2 0 2 # 3 4 6 # 4 0 2 # 5 0 0 # 6 0 0 # parole_violators_with_a_new_charge pretrial_misd # 1 5 12 # 2 4 11 # 3 0 14 # 4 6 7 # 5 1 3 # 6 0 0 # conv_misd bench_warrants federal pretrial_sjf # 1 1 0 0 21 # 2 0 0 0 5 # 3 0 3 0 23 # 4 0 6 73 2 # 5 1 1 2 5 # 6 0 0 0 0 # conv_sjf_sentenced_to_co_jail_time # 1 0 # 2 0 # 3 0 # 4 0 # 5 0 # 6 0 # conv_sjf_sentence_to_state_jail total_others total_local # 1 1 0 138 # 2 6 0 35 # 3 3 1 168 # 4 0 0 56 # 5 0 1 26 # 6 0 0 2 # total_contract total_population total_capacity # 1 0 138 300 # 2 0 35 50 # 3 0 168 279 # 4 73 129 212 # 5 9 35 48 # 6 0 2 8 # percent_of_capacity available_beds ...21 # 1 46.00 132 &lt;NA&gt; # 2 70.00 10 &lt;NA&gt; # 3 60.22 83 &lt;NA&gt; # 4 60.85 62 &lt;NA&gt; # 5 72.92 0 &lt;NA&gt; # 6 25.00 0 &lt;NA&gt; tail(final) # county pretrial_felons conv_felons # 264 Yoakum 6 # 265 Young 19 # 266 Zapata 15 # 267 Zavala 16 # 268 Zavala (P) 0 # 269 Total 29173 # conv_felons_sentence_to_county_jail_time # 264 1 # 265 5 # 266 1 # 267 0 # 268 0 # 269 5814 # parole_violators parole_violators_with_a_new_charge # 264 0 0 # 265 0 5 # 266 0 0 # 267 0 3 # 268 0 0 # 269 383 2700 # pretrial_misd conv_misd bench_warrants federal # 264 0 1 0 0 # 265 1 3 1 0 # 266 0 5 0 1 # 267 0 4 0 0 # 268 0 0 0 0 # 269 3180 3370 415 816 # pretrial_sjf conv_sjf_sentenced_to_co_jail_time # 264 0 2 # 265 0 6 # 266 58 5 # 267 0 0 # 268 0 0 # 269 4354 4195 # conv_sjf_sentence_to_state_jail total_others # 264 0 0 # 265 1 0 # 266 0 0 # 267 0 0 # 268 0 0 # 269 161 1186 # total_local total_contract total_population # 264 0 9 21 # 265 0 41 2 # 266 0 27 58 # 267 0 22 17 # 268 0 0 0 # 269 2790 53017 6696 # total_capacity percent_of_capacity available_beds ...21 # 264 30 48 62.50 13 # 265 43 144 29.86 87 # 266 85 240 35.42 131 # 267 39 66 59.09 20 # 268 0 0 0 # 269 59713 93991 63.53 24681 If you look closely at the final several rows youll see that there is an extra column, and that the second column (pretrial_felons) is blank for all of these rows. Thats because when scraping the final page tabulizer incorrectly added an empty column between the first and second column, meaning that all columns to the right of the first column shifted once to the right. So all the values in pretrial_felons are actually in conv_felons and so on. The last column now is named 21 since it is the 21st column, and that name was made automatically as our column_names object only has 20 values. This can occasionally happen, even if seemingly identical formatted pages like we have here. To fix something like this, wed want to check every column and delete any that had all values be empty strings. I leave solving this to you. While it may be a challenge, at this point in the book you have the skills to do it. 23.2 Pregnant women incarcerated Well finish this chapter with another example of data from Texas - this time using data on the number of pregnant women booked in Texas county jails. This data has a unique challenge: it has 10 columns, but we want to make it have only 2. In the data (shown following), it starts with a column of county names, then a column of the number of pregnant women booked into that countys jail. Next is another column of county names - instead of continuing onto another page, this data just makes new columns when it runs out of room. Well scrape this PDF using tabulizer() and then work to fix this multiple-column issue. Notice that this data doesnt even have column names, so well have to make them ourselves. This is always a bit risky as maybe next month the table will change, and if we hard-code any column names, well either have code that breaks or - much more dangerous - mislabel the columns without noticing. In cases like this we have no other choice, but if you intend to scrape PDFs that regularly update (such as when a new month of data comes out) be careful about situations like this. Well start scraping this PDF using the standard extract_tables() function without any parameters other than the file name. This is usually a good start since its quick and often works - and if it doesnt, we havent lost much time checking. Since we know extract_tables() will return a list by default, well assign the result of extract_tables() to an object called data and then just pull the first element (the only element if this scrape works properly) from that list. And to see how the scraping went, well look at the head() of the data. data &lt;- extract_tables(file = &quot;data/PregnantFemaleReportingCurrent.pdf&quot;) data &lt;- data[[1]] head(data) # [,1] [,2] [,3] [,4] [,5] [,6] # [1,] &quot;Anderson&quot; &quot;0&quot; &quot;Delta&quot; &quot;0&quot; &quot;Irion&quot; &quot;0&quot; # [2,] &quot;Andrews&quot; &quot;1&quot; &quot;Denton&quot; &quot;3&quot; &quot;Jack&quot; &quot;0&quot; # [3,] &quot;Angelina&quot; &quot;0&quot; &quot;DeWitt&quot; &quot;0&quot; &quot;Jackson&quot; &quot;1&quot; # [4,] &quot;Aransas&quot; &quot;0&quot; &quot;Dickens&quot; &quot;0&quot; &quot;Jasper&quot; &quot;0&quot; # [5,] &quot;Archer&quot; &quot;1&quot; &quot;Dickens (P)&quot; &quot;0&quot; &quot;Jeff Davis&quot; &quot;0&quot; # [6,] &quot;Armstrong&quot; &quot;0&quot; &quot;Dimmit&quot; &quot;0&quot; &quot;Jefferson&quot; &quot;0&quot; # [,7] [,8] [,9] [,10] # [1,] &quot;Motley&quot; &quot;0&quot; &quot;Upton&quot; &quot;0&quot; # [2,] &quot;Nacogdoches&quot; &quot;2&quot; &quot;Uvalde&quot; &quot;0&quot; # [3,] &quot;Navarro&quot; &quot;2&quot; &quot;Val Verde (P)&quot; &quot;1&quot; # [4,] &quot;Newton&quot; &quot;0&quot; &quot;Van Zandt&quot; &quot;0&quot; # [5,] &quot;Newton (P)&quot; &quot;0&quot; &quot;Victoria&quot; &quot;1&quot; # [6,] &quot;Nolan&quot; &quot;2&quot; &quot;Walker&quot; &quot;1&quot; If we check the output from the above code to the PDF, we can see that it worked. Every column in the PDF is in our output, and the values were scraped correctly. This is great! Now we want to make two columns - county and pregnant_females_booked (or whatever youd like to call it) - from these 10. As usual with R, there are a few ways we can do this. Well just do this in two different ways. First, since there are only 10 columns, we can just do it manually. We can use square bracket [] notation to grab specific columns using the column number (since the data is a matrix and not a data.frame we cant use dollar sign notation even if we wanted to). We can see from the PDF that the county columns are columns 1, 3, 5, 7, and 9. So can use a vector of numbers to get that c(1, 3, 5, 7, 9). head(data[, c(1, 3, 5, 7, 9)]) # [,1] [,2] [,3] [,4] # [1,] &quot;Anderson&quot; &quot;Delta&quot; &quot;Irion&quot; &quot;Motley&quot; # [2,] &quot;Andrews&quot; &quot;Denton&quot; &quot;Jack&quot; &quot;Nacogdoches&quot; # [3,] &quot;Angelina&quot; &quot;DeWitt&quot; &quot;Jackson&quot; &quot;Navarro&quot; # [4,] &quot;Aransas&quot; &quot;Dickens&quot; &quot;Jasper&quot; &quot;Newton&quot; # [5,] &quot;Archer&quot; &quot;Dickens (P)&quot; &quot;Jeff Davis&quot; &quot;Newton (P)&quot; # [6,] &quot;Armstrong&quot; &quot;Dimmit&quot; &quot;Jefferson&quot; &quot;Nolan&quot; # [,5] # [1,] &quot;Upton&quot; # [2,] &quot;Uvalde&quot; # [3,] &quot;Val Verde (P)&quot; # [4,] &quot;Van Zandt&quot; # [5,] &quot;Victoria&quot; # [6,] &quot;Walker&quot; Now again for the pregnant_females_booked columns, which are the even numbers. head(data[, c(2, 4, 6, 8, 10)]) # [,1] [,2] [,3] [,4] [,5] # [1,] &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; # [2,] &quot;1&quot; &quot;3&quot; &quot;0&quot; &quot;2&quot; &quot;0&quot; # [3,] &quot;0&quot; &quot;0&quot; &quot;1&quot; &quot;2&quot; &quot;1&quot; # [4,] &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; # [5,] &quot;1&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;1&quot; # [6,] &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;2&quot; &quot;1&quot; These results look right so we can make a data.frame using the data.frame() and having the input be from the above code - removing the head() function since we want every row. Conveniently, data.frame() allows us to name the columns we are making so well name the two columns county and pregnant_females_booked. Well assign the result to an object that well call data and check out the head() and tail() of that data.frame. data &lt;- data.frame( county = c(data[, c(1, 3, 5, 7, 9)]), pregnant_females_booked = c(data[, c(2, 4, 6, 8, 10)]) ) head(data) # county pregnant_females_booked # 1 Anderson 0 # 2 Andrews 1 # 3 Angelina 0 # 4 Aransas 0 # 5 Archer 1 # 6 Armstrong 0 tail(data) # county pregnant_females_booked # 295 # 296 # 297 # 298 # 299 # 300 These results look good! We now have only two columns, and the first six rows (from head()) look right. Why are the last six rows all empty? Look back at the PDF. The final two columns are shorter than the others, so extract_tables() interprets them as empty strings. We can subset those away using a conditional statement to remove any row with an empty string in either column. Since we know that if theres an empty string in one of the columns it will also be there in the other, we only need to run this once. data &lt;- data[data$county != &quot;&quot;, ] head(data) # county pregnant_females_booked # 1 Anderson 0 # 2 Andrews 1 # 3 Angelina 0 # 4 Aransas 0 # 5 Archer 1 # 6 Armstrong 0 tail(data) # county pregnant_females_booked # 260 Wood 0 # 261 Yoakum 0 # 262 Young 0 # 263 Zapata 0 # 264 Zavala 0 # 265 Zavala (P) 0 Now the results from tail() look right. We can now use the second method which will use logical values to only keep odd or even columns (as the columns we want are conveniently all odd or all even columns). First, Im rerunning the code to scrape the PDF since now our data data set is already cleaned from above. data &lt;- extract_tables(file = &quot;data/PregnantFemaleReportingCurrent.pdf&quot;) data &lt;- data[[1]] Well use a toy example now with a vector of numbers from 1 to 10 1:10 which we can call x. x &lt;- 1:10 x # [1] 1 2 3 4 5 6 7 8 9 10 Now say we want every value of x and want to use logical values (also called Booleans) to get it. We need a vector of 10 values since wed need one for every element in x. Specifically, wed be using square bracket [] notation to subset (in this case not really a true subset since wed return all the original values) and write ten TRUEs in the square brackets []. x[c(TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE)] # [1] 1 2 3 4 5 6 7 8 9 10 If youre reading the code carefully, you might have notices that I only wrote nine TRUE values. Since R was expecting 10 values, when I only gave it nine, it started again from the beginning and used the first value in place of the expected tenth value. If we only wrote one TRUE value, R would just repeat that all 10 times. x[TRUE] # [1] 1 2 3 4 5 6 7 8 9 10 What happens when the value isnt always TRUE? Itll recycle it the exact same way. Lets try using now a vector c(TRUE, FALSE). x[c(TRUE, FALSE)] # [1] 1 3 5 7 9 It returns only the odd numbers. Thats because the first value in our vector is TRUE so it returns the first value of x, which is 1. The next value is FALSE so it does not return the second value of x, which is 2. R then recycles our vector and uses the first value in our vector (TRUE) to interpret how to subset the third value of x. Since its TRUE, it returns 3. But now the value for 4 is FALSE so it doesnt return it. The process repeats again until the end of the subset. Since every other value is returned, it returns only the odd numbers. We can use Rs method of recycling a vector that is shorter than it expects to solve our pregnant females booked issue. Indeed we can use this exact c(TRUE, FALSE) vector to select only the odd columns. Reversing it to c(FALSE, TRUE) gives us only the even columns. So well copy over the code that made the data.frame last time and change the c(data[, c(1, 3, 5, 7, 9)] to c(data[, c(TRUE, FALSE)]) and the c(data[, c(2, 4, 6, 8, 10)]) to c(data[, c(FALSE, TRUE)]). Since the issue of empty strings is still there, well reuse the data &lt;- data[data$county != \"\", ] we made above to fix it. data &lt;- data.frame( county = c(data[, c(TRUE, FALSE)]), pregnant_females_booked = c(data[, c(FALSE, TRUE)]) ) data &lt;- data[data$county != &quot;&quot;, ] head(data) # county pregnant_females_booked # 1 Anderson 0 # 2 Andrews 1 # 3 Angelina 0 # 4 Aransas 0 # 5 Archer 1 # 6 Armstrong 0 tail(data) # county pregnant_females_booked # 260 Wood 0 # 261 Yoakum 0 # 262 Young 0 # 263 Zapata 0 # 264 Zavala 0 # 265 Zavala (P) 0 23.3 Making PDF-scraped data available to others Youve now seen two examples of scraping tables from PDFs using the tabulizer() package and a few more examples from the pdftools package in Chapter 22. These chapters should get you started on most PDF scraping, but every PDF is different so dont rely on the functions alone to do all of the work. Youll still likely have to spend some time cleaning up the data afterwards to make it usable. Given the effort youll spend in scraping a PDF - and the relative rarity of this skill in criminology - I recommend that you help others by making your data available to the public. There are several current websites that let you do this, but I recommend openICPSR. openICPSR lets people submit data for free (under a certain limit, 3GB per submission as of mid-2020 though you can ask for a limit increase) and has a number of features to make it easier to store and document the data. This includes a section to describe your data in text form, fill out tags to help people search for the data, and answer (optional) questions on how the data was collected and the geographic and temporal scope of the data. If you decide to update the data, itll keep a link to your older submission so you essentially have versions of the data. When you update the data, I recommend having a section on the submission description describing the changes in each version. As an example of what it looks like when submitting data to openICPSR, below are a few images showing the submission page for one of my submissions that has many versions (and corresponding version notes). "],["geocoding.html", "24 Geocoding 24.1 Geocoding a single address 24.2 Geocoding San Francisco marijuana dispensary locations", " 24 Geocoding For this chapter youll need the following file, which is available for download here: san_francisco_active_marijuana_retailers.csv. Several recent studies have looked at the effect of marijuana dispensaries on crime around the dispensary. For these analyses they find the coordinates of each crime in the city and see if it occurred in a certain distance from the dispensary. Many crime data sets provide the coordinates of where each crime occurred, however sometimes the coordinates are missing - and other data such as marijuana dispensary locations give only the address - meaning that we need a way to find the coordinates of these locations. 24.1 Geocoding a single address In this chapter we will cover how to geocode addresses. Geocoding is the process of taking an address (e.g. 123 Main Street, Somewhere, CA, 12345) and getting the longitude and latitude coordinates of that address. With these coordinates we can then do spatial analyses on the data ranging from simply making a map and showing where each address is to merging these coordinates with some other spatial data (such as seeing which police district the address is in) and seeing how it relates to other variables, such as crime. To do our geocoding, were going to use the package tidygeocoder which greatly simplifies the work of geocoding addresses in R. For more information about this package, please see the packages site here. If youve never used this package before youll need to install it using install.packages(\"tidygeocoder\"). install.packages(&quot;tidygeocoder&quot;) Now we need to tell R that we want to use this package by running library(tidygeocoder). library(tidygeocoder) To geocode our addresses well use the helpfully named geocode() function inside of tidygeocoder. For geocode() we input an address and it returns the coordinates for that address. For our address well use 750 Race St. Philadelphia, PA 19106, which is the address of the Philadelphia Police Department headquarters. geocode(&quot;750 Race St. Philadelphia, PA 19106&quot;) # Error: .tbl is not a dataframe. See ?geocode As shown above, running geocode(\"750 Race St. Philadelphia, PA 19106\") gives us an error that tells us that .tbl is not a dataframe. The issue is that geocode() expects a data.frame (and .tbl is an abbreviation for tibble, which is a kind of data.frame), but we entered only the string with our one address, not a data.frame. For this function to work we need to enter two parameters into geocode(): a data.frame (or something similar such as a tibble) and the name of the column that has the addresses.22 Since we need a data.frame, well make one below. Im calling it address_to_geocode and calling the column with the address address, but you can call both the data.frame and the column whatever name you want. address_to_geocode &lt;- data.frame( address = &quot;750 Race St. Philadelphia, PA 19106&quot; ) Now lets try again. Well enter our data.frame address_to_geocode first and then the name of our column which is address. geocode(address_to_geocode, address) # # A tibble: 1 x 3 # address lat long # &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 750 Race St. Philadelphia, PA 19106 40.0 -75.2 It worked, returning the same data.frame but with two additional columns with the latitude and longitude of that address. You might be wondering why we put address into geocode() without quotes when usually when we talk about a column we need to do so in quotes. The simple answer is that the authors of the tidygeocoder package spent the time allowing users to input the column name either with or without quotes. Trying it again and now having address in quotes gives us the same result. geocode(address_to_geocode, &quot;address&quot;) # # A tibble: 1 x 3 # address lat long # &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 750 Race St. Philadelphia, PA 19106 40.0 -75.2 There are two additional parameters that are important to talk about for this function, especially when you encounter an address that doesnt geocode properly. First, there are actually multiple sources where you can enter an address and get the coordinates for that address. Just think about the big mapping apps or sites, such as Google Maps and Apple Maps. For these sources you can enter in the same address and youll get different results. In most cases youll get extremely similar coordinates, usually off only after a few decimals points, so they are functionally identical. But occasionally youll have some addresses that can be geocoded through some sources but not others. This is because some sources have a more comprehensive list of addresses than others. At the time of this writing the tidygeocoder package can handle geocoding from 13 different sources. For 10 of these, however, you need to setup an API key and some also require paying money (usually after a set number of addresses that itll geocode for free each day). So here Ill just cover the three sources of geocoding that dont require any setup: osm (Open Street Map or OSM is similar to Google Maps), census (the US Census Bureaus geocoder), and arcgis (ArcGIS is a clunky mapping software that nonetheless has an excellent geocoder that R can use). To select which of these to use (osm is the default), you add the parameter method and set that equal to which one you want to use. As osm is the default we actually dont need to set it explicitly, but well do so anyways here as an example of the three geocoding sources we want to use. example &lt;- geocode(address_to_geocode, &quot;address&quot;, method = &quot;osm&quot;) example # # A tibble: 1 x 3 # address lat long # &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 750 Race St. Philadelphia, PA 19106 40.0 -75.2 example &lt;- geocode(address_to_geocode, &quot;address&quot;, method = &quot;census&quot;) example # # A tibble: 1 x 3 # address lat long # &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 750 Race St. Philadelphia, PA 19106 40.0 -75.2 example &lt;- geocode(address_to_geocode, &quot;address&quot;, method = &quot;arcgis&quot;) example # # A tibble: 1 x 3 # address lat long # &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 750 Race St. Philadelphia, PA 19106 40.0 -75.2 By default this function returns a tibble instead of a normal data.frame so it only shows one decimal point by default - though it doesnt actually round the number, merely shorten what it shows us. We can change the output back into a data.frame by using the data.frame() function. If you check each result after converting it to a data.frame youll see that each set of coordinates are very slightly different, though for all purposes are the same location. example &lt;- geocode(address_to_geocode, &quot;address&quot;, method = &quot;arcgis&quot;) example &lt;- data.frame(example) example # address lat long # 1 750 Race St. Philadelphia, PA 19106 39.95488 -75.15205 Given how similar the coordinates are, you really only need to set the source of the geocoder in cases where one geocoder fails to find a match for the address. The second important parameter is full_results, which is by default set to FALSE. When set to TRUE it gives more columns in the returning data.frame than just the longitude and latitude of that address. These columns differ for each geocoder source so well look at all three. Ill convert all of these results to a data.frame so it prints out all of the columns, and doesnt abbreviate results, which is how tibbles function. The example$display_name &lt;- NULL isnt necessary, but I use it to remove a column that prints out an extremely long line for the locations full address and that looks bad in the print version of this book. example &lt;- geocode(address_to_geocode, &quot;address&quot;, method = &quot;osm&quot;, full_results = TRUE ) example &lt;- data.frame(example) example$display_name &lt;- NULL example # address lat long # 1 750 Race St. Philadelphia, PA 19106 39.95506 -75.15217 # place_id # 1 304083147 # licence # 1 Data © OpenStreetMap contributors, ODbL 1.0. https://osm.org/copyright # osm_type osm_id # 1 way 626341043 # boundingbox # 1 39.955010747193, 39.955110747193, -75.152217431613, -75.152117431613 # class type importance # 1 place house -0.62 For OSM as a source we also get information about the address, such as what type of place it is, a bounding box which is a geographic area right around this coordinate, the address for those coordinates in the OSM database, and a bunch of other variables that dont seem very useful for our purposes such as the importance of the address. Its interesting that OSM classifies this address as a house as the headquarters of a major police department is quite a bit bigger than a house, so this is likely an misclassification of the type of address. The most important extra variable here is the address, called the display_name. Sometimes geocoders will be quite a bit off in their geocoding because they match the address you inputted incorrectly to one in their database. For example, if you input 123 Main Street and the geocoder thinks you mean 123 Maine Street you may be quite a bit off in the resulting coordinates. When you only get coordinates returned you wont know that the coordinates are wrong. Even if you know where an address is supposed to be its hard to catch errors like this. If youre geocoding addresses in a single city and one point is in a different city (or completely different part of the world), then its pretty clear that theres an error. But if the coordinates are simply in a wrong part of the city, but near other coordinates, then its very hard to notice a problem. So having an address to check against the one you inputted is a very useful way of validate the geocoding. example &lt;- geocode(address_to_geocode, &quot;address&quot;, method = &quot;census&quot;, full_results = TRUE ) example &lt;- data.frame(example) example # address lat long # 1 750 Race St. Philadelphia, PA 19106 39.95488 -75.1514 # matchedAddress tigerLine.side # 1 750 RACE ST, PHILADELPHIA, PA, 19106 L # tigerLine.tigerLineId addressComponents.zip # 1 131423677 19106 # addressComponents.streetName addressComponents.preType # 1 RACE # addressComponents.city addressComponents.preDirection # 1 PHILADELPHIA # addressComponents.suffixDirection # 1 # addressComponents.fromAddress addressComponents.state # 1 700 PA # addressComponents.suffixType addressComponents.toAddress # 1 ST 798 # addressComponents.suffixQualifier # 1 # addressComponents.preQualifier # 1 The Census results are similar to the OSM results and also have the matched address to compare your inputted address to. Most of the columns are just the address broken into different pieces (street, city, state, etc.) so are mostly repeating the address again in multiple columns. example &lt;- geocode(address_to_geocode, &quot;address&quot;, method = &quot;arcgis&quot;, full_results = TRUE ) example &lt;- data.frame(example) example # address lat long # 1 750 Race St. Philadelphia, PA 19106 39.95488 -75.15205 # arcgis_address score # 1 750 Race St, Philadelphia, Pennsylvania, 19106 100 # location.x location.y extent.xmin extent.ymin extent.xmax # 1 -75.15205 39.95488 -75.15305 39.95388 -75.15105 # extent.ymax # 1 39.95588 For the ArcGIS results we have the matched address again, and then an important variable called score, which is basically a measure of how confident ArcGIS is that it matched the right address. Higher values are more confident, but in my experience anything under 90-95 confidence is an incorrect address. These results also repeat the longitude and latitude columns as location.x and location.y columns, and Im not sure why they do so. 24.2 Geocoding San Francisco marijuana dispensary locations So now that we can use the geocoder() function well, we can geocode every location in our marijuana dispensary data. Lets read in the marijuana dispensary data, which is called san_francisco_active_marijuana_retailers.csv and call the object marijuana. Note the data/ part in front of the name of the .csv file. This is to tell R that the file we want is in the data folder of our working directory. Doing this is essentially a shortcut to changing the working directory directly. For this book I keep all of the data files in a folder called data in my working directory. Unless you also have a folder called data in your working directory which has this file, please delete data/ from the following code. library(readr) marijuana &lt;- read_csv(&quot;data/san_francisco_active_marijuana_retailers.csv&quot;) marijuana &lt;- data.frame(marijuana) Lets look at the top 6 rows. head(marijuana) # License.Number License.Type # 1 C10-0000614-LIC Cannabis - Retailer License # 2 C10-0000586-LIC Cannabis - Retailer License # 3 C10-0000587-LIC Cannabis - Retailer License # 4 C10-0000539-LIC Cannabis - Retailer License # 5 C10-0000522-LIC Cannabis - Retailer License # 6 C10-0000523-LIC Cannabis - Retailer License # Business.Owner Business.Structure # 1 Terry Muller Limited Liability Company # 2 Jeremy Goodin Corporation # 3 Justin Jarin Corporation # 4 Ondyn Herschelle Corporation # 5 Ryan Hudson Limited Liability Company # 6 Ryan Hudson Limited Liability Company # Premise.Address # 1 2165 IRVING ST san francisco, CA 94122 County: SAN FRANCISCO # 2 122 10TH ST SAN FRANCISCO, CA 941032605 County: SAN FRANCISCO # 3 843 Howard ST SAN FRANCISCO, CA 94103 County: SAN FRANCISCO # 4 70 SECOND ST SAN FRANCISCO, CA 94105 County: SAN FRANCISCO # 5 527 Howard ST San Francisco, CA 94105 County: SAN FRANCISCO # 6 2414 Lombard ST San Francisco, CA 94123 County: SAN FRANCISCO # Status Issue.Date Expiration.Date # 1 Active 9/13/2019 9/12/2020 # 2 Active 8/26/2019 8/25/2020 # 3 Active 8/26/2019 8/25/2020 # 4 Active 8/5/2019 8/4/2020 # 5 Active 7/29/2019 7/28/2020 # 6 Active 7/29/2019 7/28/2020 # Activities Adult.Use.Medicinal # 1 N/A for this license type BOTH # 2 N/A for this license type BOTH # 3 N/A for this license type BOTH # 4 N/A for this license type BOTH # 5 N/A for this license type BOTH # 6 N/A for this license type BOTH The column with the address is called Premise Address. Since the address county is always County: SAN FRANCISCO we can just gsub() out that entire string. marijuana$Premise.Address &lt;- gsub( &quot; County: SAN FRANCISCO&quot;, &quot;&quot;, marijuana$Premise.Address ) Now lets make sure we did it right. head(marijuana$Premise.Address) # [1] &quot;2165 IRVING ST san francisco, CA 94122&quot; # [2] &quot;122 10TH ST SAN FRANCISCO, CA 941032605&quot; # [3] &quot;843 Howard ST SAN FRANCISCO, CA 94103&quot; # [4] &quot;70 SECOND ST SAN FRANCISCO, CA 94105&quot; # [5] &quot;527 Howard ST San Francisco, CA 94105&quot; # [6] &quot;2414 Lombard ST San Francisco, CA 94123&quot; To do the geocoding well just tell geocode() our data.frame name and the name of the column with the addresses. Well assign the results back into the marijuana object. As noted earlier, we dont need to put the name of our column in quotes, but I like to do so because it is consistent with some other functions that require it. Running this code may take up to a minute because its geocoding 33 different addresses. marijuana &lt;- geocode(marijuana, &quot;Premise.Address&quot;) Now it appears that we have longitude and latitude for every dispensary. We should check that they all look sensible. summary(marijuana$long) # Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s # -122.5 -122.4 -122.4 -122.4 -122.4 -122.4 10 summary(marijuana$lat) # Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s # 37.71 37.75 37.78 37.77 37.78 37.80 10 The minimum and maximum are very similar to each other for both longitude and latitude so thats a sign that it geocoded correctly. The 10 NA values mean that it didnt find a match for 10 of the addresses. Lets try again and now set method to arcgis. which generally has a very high match rate. Before we do this lets just remove the entire latitude and longitude columns from our data. How the geocode() function works is that if we keep the long and lat columns that are currently in the data from when we just geocoded, when we run it again itll make new columns that have nearly identical names. We usually want as few columns in our data as possible so theres no point having the lat column from the last geocode run with the 10 NAs and another lat (though slightly different, automatically chosen name) column from this time we run geocode(). We could also just geocode the 10 addresses that failed on the first run, but given that well only be geocoding a small number of addresses it wont take much extra time to have ArcGIS run it all. Running this function on just the NA rows requires a bit more work than just rerunning them all. In general, when the choice is between you spending time writing code and letting the computer do more work, let the computer do the work. And in general Id recommend starting with ArcGIS as it is more reliable for geocoding. Well remove the current coordinate columns by setting them each to NULL. marijuana$long &lt;- NULL marijuana$lat &lt;- NULL marijuana &lt;- geocode(marijuana, &quot;Premise.Address&quot;, method = &quot;arcgis&quot; ) And lets do the summary() check again. summary(marijuana$long) # Min. 1st Qu. Median Mean 3rd Qu. Max. # -122.5 -122.4 -122.4 -122.4 -122.4 -122.4 summary(marijuana$lat) # Min. 1st Qu. Median Mean 3rd Qu. Max. # 37.71 37.76 37.77 37.77 37.78 37.80 No more NAs, which means that we successfully geocoded our addresses. Another check is to make a simple scatterplot of the data. Since all of the data is from San Francisco, they should be relatively close to each other. If there are dots far from the rest, that is probably a geocoding issue. plot(marijuana$long, marijuana$lat) Most points are within a very narrow range so it appears that our geocoding worked properly. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
